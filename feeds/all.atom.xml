<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>エンジニアの英語学習法</title><link href="http://englishforhackers.com/" rel="alternate"></link><link href="http://englishforhackers.com/feeds/all.atom.xml" rel="self"></link><id>http://englishforhackers.com/</id><updated>2019-04-12T00:00:00-04:00</updated><entry><title>あなたはなぜ英語ができないのか — プロジェクトベース英語学習法のススメ</title><link href="http://englishforhackers.com/project-based-english-learning.html" rel="alternate"></link><published>2019-04-12T00:00:00-04:00</published><updated>2019-04-12T00:00:00-04:00</updated><author><name>Masato Hagiwara</name></author><id>tag:englishforhackers.com,2019-04-12:/project-based-english-learning.html</id><summary type="html">&lt;p&gt;&lt;span style="display:block;text-align:center"&gt;
&lt;img alt="英語学習" src="images/pbel-language.jpg"&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;目次&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#intro"&gt;はじめに&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#learning-materials"&gt;巷の英語教材のウソ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#5-neighbors"&gt;あなたの周りの人は、英語が話せますか？&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#3-reasons"&gt;あなたが英語ができない理由&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#reason1"&gt;理由1: 日本語と英語が違いすぎる&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#reason2"&gt;理由2: 時間がかかる&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#reason3"&gt;理由3: 必要がない&lt;/a&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#how-to-be-fluent"&gt;「私はこうして英語がペラペラになった」&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#toeic"&gt;TOEIC はどうなの&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#hacking-environment"&gt;環境をハックする&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#methods"&gt;具体的にどうすればよいのか&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#favorite-materials"&gt;自分の好きな教材を使う&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#recommended-materials"&gt;オススメの教材&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#not-recommended-materials"&gt;オススメしない教材・勉強法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#hacking-5-neighbors"&gt;周りの5人をハックする&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#social-commitment"&gt;社会的な制約を使う&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#hobbies-in-english"&gt;英語を使うことでしかできないことを趣味にする&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#principles"&gt;英語学習の原則&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#sleep"&gt;睡眠&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#implicit-learning"&gt;意識的な勉強と、無意識の学習&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#input-output"&gt;インプット・アウトプット&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#shadowing"&gt;シャドーイング&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#finding-a-good-teacher"&gt;良い先生を探す&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#summary"&gt;おわりに&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;突然ですが、皆さん、ピアノは上手ですか？「弾けない」と答えた方、でも、義務教育（小学や中学）で、「音楽」の時間がありましたよね。そこで、何らかの鍵盤楽器を練習した方も多いと思います。なぜ上手ではないんでしょう。&lt;/p&gt;
&lt;p&gt;サッカー、ならどうでしょう。水泳 …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;span style="display:block;text-align:center"&gt;
&lt;img alt="英語学習" src="images/pbel-language.jpg"&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;目次&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#intro"&gt;はじめに&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#learning-materials"&gt;巷の英語教材のウソ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#5-neighbors"&gt;あなたの周りの人は、英語が話せますか？&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#3-reasons"&gt;あなたが英語ができない理由&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#reason1"&gt;理由1: 日本語と英語が違いすぎる&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#reason2"&gt;理由2: 時間がかかる&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#reason3"&gt;理由3: 必要がない&lt;/a&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#how-to-be-fluent"&gt;「私はこうして英語がペラペラになった」&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#toeic"&gt;TOEIC はどうなの&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#hacking-environment"&gt;環境をハックする&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#methods"&gt;具体的にどうすればよいのか&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#favorite-materials"&gt;自分の好きな教材を使う&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#recommended-materials"&gt;オススメの教材&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#not-recommended-materials"&gt;オススメしない教材・勉強法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#hacking-5-neighbors"&gt;周りの5人をハックする&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#social-commitment"&gt;社会的な制約を使う&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#hobbies-in-english"&gt;英語を使うことでしかできないことを趣味にする&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#principles"&gt;英語学習の原則&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#sleep"&gt;睡眠&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#implicit-learning"&gt;意識的な勉強と、無意識の学習&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#input-output"&gt;インプット・アウトプット&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#shadowing"&gt;シャドーイング&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#finding-a-good-teacher"&gt;良い先生を探す&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#summary"&gt;おわりに&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;突然ですが、皆さん、ピアノは上手ですか？「弾けない」と答えた方、でも、義務教育（小学や中学）で、「音楽」の時間がありましたよね。そこで、何らかの鍵盤楽器を練習した方も多いと思います。なぜ上手ではないんでしょう。&lt;/p&gt;
&lt;p&gt;サッカー、ならどうでしょう。水泳、でも良いです。「上手でない」と答えた方、でも、義務教育の体育の時間で、サッカーや水泳をいくらか練習した方も多いと思います。なぜ上手ではないんでしょうか。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;「練習してないから」&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;「必要ないから」&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;「別に好きでもないから」&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;当たり前ですよね。&lt;/p&gt;
&lt;p&gt;このガイドは、この「当たり前」のことを「英語」に対して応用したものです。ピアノやサッカーであれば、「必要無いし、練習していないのだから上手ではないのは当たり前」ということに異論を唱える人は少ないのですが、英語になると「義務教育で小中高と何年も勉強してきたのに話せないのはおかしい」という文句を言う人が居るのは少しおかしな気がします。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;あなたが英語ができないのは、学習方法や教材が悪いからでも、学校教育や先生が悪かったからでもありません。&lt;/strong&gt; もちろん、そういった要素も少しはあるでしょうが、平均的な日本人が英語ができないのは、もっと根本的な複数の要素が絡み合っています。&lt;/p&gt;
&lt;p&gt;その要素を「ハック」することによって、あなたの英語は今よりもずっと上達します。このガイドでは、そのお手伝いができればと思っています。&lt;/p&gt;
&lt;p&gt;&lt;a name="intro"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;はじめに&lt;/h2&gt;
&lt;p&gt;このガイドは、これまで、英語を含む色々な外国語の学習に成功そして挫折してきた私の個人的な経験と、数え切れない人に英語学習のアドバイスをしてきた私の経験に基づいて、英語学習を継続させ、成功させるためのポイントをまとめたものです。メインのメッセージは、&lt;strong&gt;「言語はツールである。必要になる状況を故意に作り出すことが成功の秘訣だ」&lt;/strong&gt; ということに尽きます。&lt;/p&gt;
&lt;p&gt;このガイドのメインの想定読者は、以下のような方です。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;学校で習った基本的な挨拶や文法は覚えていてある程度分かるが、英語力が長期的に伸び悩んでいる。特に、スピーキングが苦手&lt;/li&gt;
&lt;li&gt;学業や仕事、趣味なので英語を学ぶ必要があるのは理解しているが、英語の学習が続かない&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;このような方の英語学習のお役に立てればと思っています。&lt;/p&gt;
&lt;p&gt;逆に、&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;中学英語で習うような初歩的な文法が分からない&lt;/li&gt;
&lt;li&gt;海外出張・赴任するので３ヶ月以内になるべく早く英語力を向上させたい&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;という方は、対象ではありません。１番目に当てはまる方は、中学・高校の英語の簡単な復習ができる良い教材が日本では数多く販売されているので、そういったものから復習することをオススメします。また、２番めの方については、残念ながら本ガイドを読むだけで３ヶ月程度で英語力を劇的に向上させることは難しいかもしれません。ただ、出張・赴任された後でも、きっと、長期的に渡る英語学習の助けにはなるでしょう。&lt;/p&gt;
&lt;p&gt;ちなみに、このガイドを読むこと自体では、英語力は全く向上しません。具体的な教材名や練習法もほとんど出てきません。しかし、後で述べるように、英語の学習は長期戦です。何千時間もかかります。このガイドを読むには、数十分とかかりません。この数十分で、後に何千時間の学習を楽に進められるのなら、その学習の効率が大幅に向上するなら、良い投資だと思います。&lt;/p&gt;
&lt;p&gt;&lt;span style="display:block;text-align:center"&gt;
&lt;img alt="3000時間と vs このガイドを読むのにかかる時間" src="images/pbel-3000hours.png"&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;なお、ここでは「英語学習」に限定して話を進めていますが、他の言語、フランス語、スペイン語、中国語、韓国語などにもそのまま当てはまるアドバイスがほとんどですので、そのような言語を学習されている方にも有用でしょう。&lt;/p&gt;
&lt;p&gt;&lt;a name="learning-materials"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;巷の英語教材のウソ&lt;/h2&gt;
&lt;p&gt;&lt;span style="display:block;text-align:center"&gt;
&lt;img alt="英会話学校" src="images/pbel-classroom.jpg"&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;日本では、英語学習が一大産業を形成しています。書店に行けば英語の学習書が大きなスペースを占め、英会話教室も乱立しており、そういった教材等の広告をいたるところで目にします。英語学習を題材にしたブログ等も多いですよね。&lt;/p&gt;
&lt;p&gt;そういった広告などでよく目にするフレーズが、「日本に居ながら英語が上達する」です。「駅前留学」というフレーズで積極的に宣伝していた英会話学校もありましたね。&lt;/p&gt;
&lt;p&gt;確かに、日本に居ながら英語を上達させることは可能です。でも、こういったフレーズで触れ込みの英会話学校に通ったり、教材を使ったりするだけで、英語が上級レベルまで到達した人を私は知りません。私も、日本に居たときは、こういった教材や英会話学校に何十万円もお金を使いましたが、それ相応の効果があったかについては疑わしいです。&lt;/p&gt;
&lt;p&gt;ここに、英語教材や英会話学校の巧妙な戦略があります。巷にあふれるほとんどの英語教材や英会話学校は、&lt;strong&gt;「買ってもらって終わり」&lt;/strong&gt; です。書籍などは、買ってもらった時点で販売元にお金が入るので、その後、その教材を使った人がどのぐらい英語が上達したか、ということには関心がありません。英会話学校も、入学時に何ヶ月分も、何十万円以上も授業料を前払いするのが普通なので、その後、生徒の英語がどのぐらい向上したかについては関心があまりありません。また、そもそも日本にある英会話学校は、生徒に海外に行かれてしまうと、日本でお金を落としてくれないばかりか、その方がはるかに効率的であることを知られてしまうため、海外に行く必要がないことを強調するのです。&lt;/p&gt;
&lt;p&gt;このように、ほとんどの語学ビジネスと、学習者との利益は一致していないのです。&lt;/p&gt;
&lt;p&gt;英語を学ぶには、短期でも良いので、英語が話されている国に行くのが効率的な方法です。日本では英語学習が一大産業を形成しているため、こんな基本的な事実さえも消費者に歪んで伝わっていることに気が付くべきです。&lt;/p&gt;
&lt;p&gt;&lt;a name="5-neighbors"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;あなたの周りの人は、英語が話せますか？&lt;/h2&gt;
&lt;p&gt;ここで、あなたと最も近い5人の人を思い浮かべてください。家族・友人・会社の上司・同僚・近所の人、色々と居ると思います。&lt;/p&gt;
&lt;p&gt;&lt;span style="display:block;text-align:center"&gt;
&lt;img alt="周りの5人" src="images/pbel-network.jpg"&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;その5人の人は、英語ができますか？ その5人の人の英語力を平均してみてください。おそらく、あなたの英語力とかなり近いのではないでしょうか。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;「人は周りの5人の平均になる」&lt;/strong&gt; ということがよく言われています。これは、生活習慣や年収に至るまで、色んなところで成り立つことが示されています。 &lt;strong&gt; 英語も、例外ではありません。&lt;/strong&gt; 特に英語は、周りの人と直接使うものなので、この現象がよく成り立つのではと思います。周りの人が英語ができないのに、あなただけ英語ができることを期待するのは、少し無理があるような気がしますよね。&lt;/p&gt;
&lt;p&gt;ちなみに、上で、英語力を向上させるためには、短期でもいいので海外に行くのが効率的であるということを書きました。この「周りの5人」の英語力を向上させる一番手っ取り早い方法が、「海外に行く」ことなのです。現地の友人や近所の人が英語を話していたら、自分も英語が上達する可能性はぐっと高まります。&lt;/p&gt;
&lt;p&gt;ここでよくある反論は「ただ、海外に行っても英語ができるようになるとは限らない」というものです。確かに、何年もアメリカに住みながらも、依然として日本人発音のつたない英語を話し続けている方。チャイナタウンなどの中華系コミュニティですべて事足りてしまうため、英語が全然話せないままアメリカに何十年も住んでいる中華系の方々。フィリピンのセブ島に英語留学に行っても、基本的に日本人とばかりつるんで、結局英語力が全然上達しない人。様々な方を見てきました。&lt;/p&gt;
&lt;p&gt;そのようなケースは、すべてこの「周りの5人の平均」で説明がつきます。日本から赴任して海外にやってきた方も、家族はもちろん日本人、日本人の多く住むアパートに住み、仕事上でも日本人との日本語のやりとりがメイン、休日は日本人の友人と付き合う、といったように、周りの5人の英語力が低いままの人がけっこういます。セブ島に留学して日本人とばかり話している人も同じです。&lt;/p&gt;
&lt;p&gt;こうした場合、海外に何年居ようとも、「周りの5人問題」を解決しない限り、英語力の向上は望めないのです。&lt;/p&gt;
&lt;p&gt;&lt;a name="3-reasons"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;あなたが英語ができない理由&lt;/h2&gt;
&lt;p&gt;ここで、典型的な日本人が、なぜ英語ができないか、という理由をまず整理してみましょう。&lt;/p&gt;
&lt;p&gt;&lt;a name="reason1"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;理由1: 日本語と英語が違いすぎる&lt;/h3&gt;
&lt;p&gt;まず、一番大きな理由は、&lt;strong&gt; 日本語と英語が違いすぎるからです。&lt;/strong&gt; 文字も単語も発音も文法も文化も何もかも違います。英語で多くの人がつまづく冠詞・前置詞・複数形など、すべて日本語にありません。語順も、日本語を英語に訳すとまったく逆になることが多いですよね。&lt;/p&gt;
&lt;p&gt;人間が外国語を勉強する際には、この母語の影響というのが非常に大きいことが知られています。日本語と英語が違いすぎるので、そもそも英語を勉強することが非常に大変なのです。&lt;/p&gt;
&lt;p&gt;これは、この逆のパターンを観察してみると分かります。日本にも数多くの外国人が訪問・移住して来ているわけですが、その外国人の中の日本語の習得度を比べてみると、英語などヨーロッパ系の言語を母語とする人に比べ、中国語や韓国語など、東アジア系の言語を母語とする人の方が圧倒的に早いのです。中国語は日本語と非常に似た漢字を使いますし、韓国語の文法は日本語と非常に似ています。両者とも、日本語と似た漢語由来の語彙が非常に多くあります。これらの言語と日本語の「距離」が近いので、東アジア出身の外国人は日本語の習得が早いのです。&lt;/p&gt;
&lt;p&gt;あなたの周りの日本語の上手な外国人を思い浮かべてみてください。中国・韓国など東アジア出身者が多くないですか？　&lt;/p&gt;
&lt;p&gt;この理由は、日本語を母語として生まれた以上、どうしようもないものです。ただ、私は、この記事で述べる3つの理由の中で、この理由が一番些細なものだと思っています。&lt;/p&gt;
&lt;p&gt;&lt;span style="display:block;text-align:center"&gt;
&lt;img alt="スペイン語と英語" src="images/pbel-spanish.jpg"&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;なぜかというと、例えばアメリカにはスペイン語を外国語として学んだことのある人が多いのですが、アメリカ人の平均的なスペイン語力は、スペイン語を学んだことのある人でも、恐ろしく低いというのが個人的な観察です。レストランで注文できれば上出来といったところです。英語とスペイン語は文法や単語などが非常に似ているのにもかかわらず、なぜこういうことが起こるのか。それは、以下に述べるもう２つの理由が大きいからです。&lt;/p&gt;
&lt;p&gt;&lt;a name="reason2"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;理由2: 時間がかかる&lt;/h3&gt;
&lt;p&gt;２つ目の理由は、そもそも外国語が上達するには時間がかかる、というものです。アメリカの外交官などを育成する Foreign Service Institute (FSI) という組織が、&lt;a href="https://www.effectivelanguagelearning.com/language-guide/language-difficulty"&gt;外国語の難易度ランキング&lt;/a&gt;と、その外国語を実用的に使いこなせるために必要な時間を公開しています。&lt;/p&gt;
&lt;p&gt;このデータによると、日本語は、英語話者にとって、アラビア語・中国語・韓国語と並んで、最も難しいカテゴリに分類されています。日本語は、このカテゴリの中でも最も難しい言語として、&lt;strong&gt;2200〜3000時間もの学習が必要だとされています&lt;/strong&gt;。このデータは、英語話者が日本語を学習する場合のものですが、その逆、日本語話者が英語を勉強する場合にも当てはめて良いでしょう。&lt;/p&gt;
&lt;p&gt;一方で、学校教育で英語に触れる時間は1000時間程度だという見積もりがあります。つまり、学習時間が圧倒的に足りないのです。&lt;/p&gt;
&lt;p&gt;&lt;span style="display:block;text-align:center"&gt;
&lt;img alt="日本語の難易度" src="images/pbel-difficulties.png"&gt;
(ソース: &lt;a href="https://www.lingholic.com/hardest-languages-learn/"&gt;What are the hardest languages to learn? Check the ranking&lt;/a&gt;)
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;英語の学習、というと、どういった教材を使うとか、どういった勉強法をするか、という、「効率」の部分につい焦点が当たる傾向があります。しかし、下のグラフで示したように、学習量は、「効率×時間」で表されます。効率と時間、両方の要素が必要です。いかに短期的に効率を追求しても、学習が続かなければ全く意味がありません。巷にあふれる多くの英語教材は、この「時間」の部分、「いかに学習を続けるか」ということにあまり関心が無いように思われます。&lt;/p&gt;
&lt;p&gt;&lt;span style="display:block;text-align:center"&gt;
&lt;img alt="時間×効率＝学習量" src="images/pbel-auc.png"&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;2000時間もの学習量を確保するためには、1日1時間ずつ勉強しても、5年ほどかかります。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;「1日1時間ずつ勉強して、5年」&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;これを少ないと見るか、気の遠くなるような時間と見るかは、人それぞれでしょう。ただ、&lt;strong&gt;巷にあふれる「１ヶ月で英語がペラペラになる」系の教材は、たいていが誇張です。&lt;/strong&gt; 繰り返しますが、巷にあふれるほとんどの教材は、商業的に成功するために、あなたが英語がうまくなる必要がないのです。&lt;/p&gt;
&lt;p&gt;さて、「1日1時間ずつ勉強して、5年」勉強しなければいけないという事実を受け入れたとしましょう。どんなに頑張っても、それだけの時間がかかるのです。逆に見ると、結局それだけの時間がかかるのであれば、楽をしたいというのが人間の性（さが）です。このガイドは、「いかに楽にこの2000時間を乗り越えるか」というガイドでもあります。&lt;/p&gt;
&lt;p&gt;私の個人的な考え方ですが、英語学習は「ダイエット」にとても似ています。健康に、確実に痩せたいと思うのであれば、ある一定の期間、例えば一週間だけ一生懸命に運動して、がんばって断食して終わり、ではありません。このようなやり方では、確実にリバウンドします。長い期間に渡って健康的な生活習慣を継続することが必要になります。これは「生き方」そのものを変える仕組みづくりです。これは、一見、大変に見えるかもしれませんが、実は、「一週間だけ、一生懸命に運動して、断食する」よりもずっと楽なやり方です。なにしろ、最初からずっと続くと分かっているわけですから、いかに楽するか、いかにモチベーションを保つか、いかに続けるか、ということにフォーカスできます。これらが、英語学習に成功するための要素です。&lt;/p&gt;
&lt;p&gt;ちなみに、日本語で「勉強」というと、机に座って本に向かってひたすら頑張っているイメージがあります。&lt;strong&gt;実は、この残りの2000時間は、机に全く座らなくても達成ができます。&lt;/strong&gt; 詳しい方法は、後ほどのセクションで見ていくことにしましょう。&lt;/p&gt;
&lt;p&gt;&lt;a name="reason3"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;理由3: 必要がない&lt;/h3&gt;
&lt;p&gt;日本人が英語ができない最後にして最大の理由は、「必要がない」からです。&lt;strong&gt; 日本に住んで主に日本人と生活している以上、英語ができなくてもあまり困らないのです。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;そもそも、あなたはなぜ英語ができるようになりたいのでしょうか。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;外国人に道を聞かれた時にサッと英語で対応したい&lt;/li&gt;
&lt;li&gt;仕事で外国人を英語で接待したい&lt;/li&gt;
&lt;li&gt;好きな海外ドラマや映画を字幕無しで見たい&lt;/li&gt;
&lt;li&gt;何となくカッコいい&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;といったところが出てくるかもしれません。&lt;/p&gt;
&lt;p&gt;でも、これらって、本当に必要ですか？これらができなかったら、どのぐらい困りますか？&lt;/p&gt;
&lt;p&gt;ここで、あなたの「英語力」と、その英語力が達成できたら「嬉しい度」の関係をグラフにしてみましょう。これを「モチベーション・グラフ」と呼ぶことにします。&lt;/p&gt;
&lt;p&gt;&lt;span style="display:block;text-align:center"&gt;
&lt;img alt="モチベーション・グラフ1" src="images/pbel-motivation1.png"&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;これが、典型的な日本人のモチベーション・グラフです。「英語ができない＝嬉しさゼロ」からスタートして、英語力がだいぶ向上した右の段階で、少し上がっています。つまり、典型的な日本人にとって、英語力と嬉しさの関係は、英語ができなくても困らないが、できると少し嬉しい、という程度なのです。&lt;/p&gt;
&lt;p&gt;これが、たとえばあなたが海外移住・留学・赴任するとどう変わるかを見てみましょう。短期の旅行でも構いません。海外に行くと、現地の言葉が分からないと、衣・食・住の全てにおいて困る場面が出てくるでしょう。日本企業の支社や、現地企業に勤務している場合、英語が分からないと業績の問題でクビになるかもしれません。留学している場合は、英語が分からないと落第になるかもしれません。&lt;/p&gt;
&lt;p&gt;このような人のモチベーション・グラフは、以下のようになります：&lt;/p&gt;
&lt;p&gt;&lt;span style="display:block;text-align:center"&gt;
&lt;img alt="モチベーション・グラフ2" src="images/pbel-motivation2.png"&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;つまり、英語ができないと、すごく困るわけです。嬉しさマイナスです。英語がある程度できてやっと現地で生活できるので、その段階でやっと「嬉しさゼロ」になります。一方で、英語ができると、現地に馴染めたり、日本ではできないような体験、例えば、シリコンバレーで報酬の高い職に就くなどの可能性が広がるため、「嬉しさ」は一気に上ることになります。&lt;/p&gt;
&lt;p&gt;英語の学習を長期的に成功させるためには、この &lt;strong&gt;モチベーション・グラフをハックする&lt;/strong&gt; ことが必要不可欠です。人間は、「無くても困らないが、いつか役に立つかもしれない」ようなものに長い時間をかけて勉強できるモチベーションが出るほど、うまくできてはないのです。&lt;/p&gt;
&lt;p&gt;&lt;a name="how-to-be-fluent"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;「私はこうして英語がペラペラになった」&lt;/h2&gt;
&lt;p&gt;上で述べたように、巷には「私はこうして英語がペラペラになった」という、「英語指南書」が溢れています。同じような内容のブログ記事をいくつか見たことのある方も居るのではないでしょうか。&lt;/p&gt;
&lt;p&gt;こういった「私はこうして英語がペラペラになった」というストーリーをいくつも見てみると、ある決まったパターンがあるのに気が付きます。&lt;/p&gt;
&lt;p&gt;ステップ１：全く英語のできなかった私。TOEICでも低い点数しか取れなかった。&lt;/p&gt;
&lt;p&gt;ステップ２：海外赴任や留学などで、英語が必要になった。最初は、英語が全然分からなくて困った。&lt;/p&gt;
&lt;p&gt;ステップ３：○○という教材に出会って勉強したら、英語力が上がった！&lt;/p&gt;
&lt;p&gt;ステップ４：だからあなたも○○という教材を使うべき！&lt;/p&gt;
&lt;p&gt;というパターンです。&lt;/p&gt;
&lt;p&gt;ここで、ストーリーの詳細は「○○という教材」だったり、「○○という勉強法」だったり、「○○英会話学校」だったりと色々と変わるわけです。ここが、例えば「一日何分こういった練習をして、各文について何回繰り返す」のように妙に具体的だったりするので面白いのですが、結局は、だから、あなたも「○○」を買うべき、「○○という勉強法」をするべき、というわけです。ステップ３〜４が大事だ、ということを強調します。&lt;/p&gt;
&lt;p&gt;でも、こういうストーリーをいくつを見てみると、実は、全てのストーリーに共通して大切なのは、「ステップ２」なのです。&lt;strong&gt;「英語が全然できなくて困った」という体験が、実はあなたの英語力を向上させるのです。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;こういったストーリーを経験した人にとっての「モチベーション・グラフ」は、上の２番目のグラフ（赤線）のようになっています。具体的な教材は、実はあまり重要ではないのです。モチベーション・グラフや、環境をハックしたことそのものが重要なのです。&lt;/p&gt;
&lt;p&gt;&lt;span style="display:block;text-align:center"&gt;
&lt;img alt="モチベーション・グラフ3" src="images/pbel-motivation3.png"&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;こう書くと、「いや、自分は普段の業務でドキュメントを読んだりするために英語が必要だ」という声が聞こえてきそうです。特に、技術職などの場合、英語でドキュメントを読まなければならない場合も多いですよね。ただ、この場合も、英語がスラスラと読めなかったらどのぐらい困るかというと、あんまり困らないわけです。ほとんどの分野では、多くの情報が日本語に翻訳されていますし、読むだけならゆっくりと時間をかけ、辞書を引き、必要であれば翻訳エンジンを使って意味を理解できれば良いのです。この場合、あなたの英語力はそのレベルで停滞することになります。「モチベーション・グラフ」の傾きが足らないので、それより高いレベルまで行かないのです。&lt;/p&gt;
&lt;p&gt;&lt;a name="toeic"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;TOEIC はどうなの&lt;/h2&gt;
&lt;p&gt;ここで「自分は、仕事などで TOEIC が必要だから英語を勉強している」という声が聞こえてきそうです。実際、TOEIC が入学・就職・昇進などで必要だから英語を勉強しているという方も、大勢居るかもしれません。&lt;/p&gt;
&lt;p&gt;TOEIC が必要になると、上の「モチベーション・グラフ」の形が変わります。点数が低いと入学・就職・昇進などで不利になるので、「嬉しくない」というわけですね。これだけ見ると、「英語力を上げるには、TOEIC の勉強をして良い点を取れば良い」という気がしてきます。&lt;/p&gt;
&lt;p&gt;ただ、私は「TOEIC の勉強をがんばって続けたら、英語がペラペラになった」という人を一人も知りません。&lt;/p&gt;
&lt;p&gt;&lt;span style="display:block;text-align:center"&gt;
&lt;img alt="TOEIC" src="images/pbel-toeic.jpg"&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;TOEIC は、非常に良く設計されたテストですが、問題が２つあります。一つは、「簡単すぎる」ということです。外国人がアメリカなど英語圏に留学する際によく必要とされる TOEFL と比べて、TOEIC は非常に簡単です。「TOEICは、英語のできない韓国や日本などの東アジアの国のために設計された」という噂もあるぐらいで、満点を取っても、ネイティブレベルには及ばないレベルの英語力しか測れません。&lt;/p&gt;
&lt;p&gt;もう一つの問題は、「スピーキングとライティングが無い」というものです。日本人の多くが受ける、いわゆる「TOEIC」と呼ばれているテストは、正式には「TOEIC Listening &amp;amp; Reading Test」といい、リスニングとリーディングの問題しかありません。聞く・読むという受動的な能力は、もちろん英語を使う上で非常に重要な能力なのですが、それだけでは、話したり書いたりといった「英語を使う」ことが本当にできるか、という能力は測れないのです。&lt;/p&gt;
&lt;p&gt;この２つの問題から、「TOEIC の点は取れるのだけど、英語が実際に使えない」ということが起こるのです。これをグラフにしてみると、下の図のようになります。&lt;/p&gt;
&lt;p&gt;&lt;span style="display:block;text-align:center"&gt;
&lt;img alt="TOEICの点数と英語力" src="images/pbel-toeic.png"&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;多くの人の頭の中の「理想」では、左の図（青）のように、TOEIC の点と英語力の間には、強い相関があります。「TOEICの点を上げていったら、スムーズにネイティブレベルに達する」ということです。&lt;/p&gt;
&lt;p&gt;ただ、現実は、TOEIC の点と「真の英語力」の間には、右の図（赤）のように、弱い相関しかありません。そもそも、「真の英語力を測定する」ということは、どんなテストを使っても難しいものですが、スピーキングとライティングが無い TOEIC においてはなおさらです。しかも、テストが簡単なので、点数のレンジが狭く、満点を取ったとしてもネイティブレベルの英語力の保証は全くありません。&lt;/p&gt;
&lt;p&gt;ここまで、TOEIC の悪口とも思えることをつらつら書きましたが、実は、私は「英語検定試験などを使って、モチベーション・グラフをハックする」ことに対して反対しているわけではなく、非常に積極的です。この点については後述します。&lt;/p&gt;
&lt;p&gt;&lt;a name="hacking-environment"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;環境をハックする&lt;/h2&gt;
&lt;p&gt;ここで、ちょうど良いので、ここまでの内容を一旦まとめ、英語の学習を継続させ、英語力が上達するための原則を書いておきます。&lt;/p&gt;
&lt;p&gt;日本人が英語ができない最大の理由は、「必要がない」からです。それを、上では典型的な日本人のモチベーション・グラフと、海外に留学・赴任した日本人のモチベーション・グラフを使って表しました。また、「人は周りの5人の平均になる」という通り、ほとんどの人が、海外に行った後でも、「日本人の輪」の中で過ごしており、英語力の上達の足かせになっているということを見てきました。&lt;/p&gt;
&lt;p&gt;これら２つの問題を解決する共通のコツが、&lt;strong&gt;「環境をハックする」&lt;/strong&gt; ことなのです。そのための方法はいくつかありますが、原則となるのは、「周りの5人を変える」のと「英語の学習そのものを目標としないが、英語を話さなければならない何らかの活動にコミットしてしまう」ということです。&lt;/p&gt;
&lt;p&gt;突然「英語を話さなければならない何らかの活動」と言っても、すぐにアイデアの出てくる方は少ないと思います。そういった方にオススメなのが &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;「もし自分が英語がペラペラにできたとしたら、何をしたいか」&lt;/strong&gt; &lt;/p&gt;
&lt;p&gt;と逆算して考えるというコツです。上で、「外国人に道を聞かれた時にサッと英語で対応したい」といった方も、こう聞かれたら「英語がペラペラにできたら、道に迷った外国人に道案内をひたすらしたい！」と答える人は、あまり居ないかもしれません（もしあなたがそういったことに興味があるなら、通訳案内士などに挑戦されてみることをオススメします）。&lt;/p&gt;
&lt;p&gt;「海外移住したい」「外国人と友達になりたい」「外資系企業に転職したい」などの色々な夢が出てくるかもしれません。&lt;/p&gt;
&lt;p&gt;次のステップは、これらの夢をスケールダウンさせることです。「海外移住」はすぐに無理でも、旅行に行ったり、短中期的に海外に住むことはできるかもしれません。外国人とすぐに友達になれなくても、外国人の友達を持つ日本人と友達になる、そういった外国人が集まりそうな場所に顔を出す、といったことはできそうです。「外資系企業」にいきなり転職は無理でも、そういった企業で働いている人に話を聞いてみたり、現在の勤務先でも、国際的なビジネスに関係のある部署に異動を申し出る、といったことなら可能かもしれません。&lt;/p&gt;
&lt;p&gt;これで、あなたは英語力向上までの第一歩を踏み出しました。&lt;strong&gt;上に書いた「旅行」「友達」「部署異動」のどの例も、あまり「英語の勉強」という気がしないですよね。それがまさにポイントなのです。&lt;/strong&gt; その第一歩が、環境をハックし、モチベーション・グラフの形を変え、英語の学習を継続させるためにとても大切なのです。&lt;/p&gt;
&lt;p&gt;このステップで、すぐに具体的なアクションが思いつかなかった方も問題ありません。以下では、多くの人に当てはまる具体的な方法を書いていきたいと思います。&lt;/p&gt;
&lt;p&gt;&lt;a name="methods"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;具体的にどうすればよいのか&lt;/h2&gt;
&lt;p&gt;&lt;a name="favorite-materials"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;自分の好きな教材を使う&lt;/h3&gt;
&lt;p&gt;&lt;span style="display:block;text-align:center"&gt;
&lt;img alt="自分の好きな教材を使う" src="images/pbel-materials.jpg"&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;「英語の勉強を勉強だと思わせない」一番大切な方法は、&lt;strong&gt;自分の好きな教材を使う、&lt;/strong&gt; という方法です。「教材」というと教科書や問題集を思い浮かべる方も多いかもしれません。そういった場合は、「学習材料」と言い換えても良いかもしれません。&lt;/p&gt;
&lt;p&gt;英会話学校やオンライン英会話で、既に英語のレッスンを受けているという方は、&lt;strong&gt;教材を自分で選ぶ&lt;/strong&gt; と良いです。そういったところで向こうから指定される教材というのは、教科書のように紋切り型で、万人に受ける内容しか書いてありません。しかし、自分が特定の内容、例えば、アメリカのあるスポーツチームであったり、ある芸能人であったり、特定の学術分野であったりするなら、その分野の教材を使ってはいけない理由は無いはずです。&lt;/p&gt;
&lt;p&gt;巷にあふれる英語教材や英語学習に関するアドバイスを見ると、英語学習のコツとして、「CNN、BBC、VOAなどの海外メディアを聞こう」と書いてあることが多いのですが、これは、あなたが海外ニュースや時事問題に特に関心があるのでなければ、あまり良いアドバイスとは言えません。最近では「&lt;a href="https://www.ted.com/#/"&gt;TED&lt;/a&gt;」を英語学習の教材として薦めるアドバイスも多い気がします。&lt;/p&gt;
&lt;p&gt;そもそも、CNN、BBC、VOA などの海外メディアが、英語教材としてオススメされる理由は何なのでしょうか。もちろん、アナウンサーの英語が標準的なので安心できる面もあるのですが、一つ目の理由は、歴史的な背景です。一昔前までは、英語教育に携わる人は、通訳者や英文学者などの「英語のプロ」がメインでした。そのような方々は、そもそも英語に関するモチベーションが高く、学習者が勉強を継続できる教材とは何か、といったことに関心が浅いことに加え、本人が国際ニュースや時事問題などを専門としている場合も多いため、どうしても自然と CNN や BBC などの国際メティアを教材として使いがちです。&lt;/p&gt;
&lt;p&gt;もう一つの理由は、一昔前、インターネットがこのように普及する前は、そもそも日本に居ながら生の英語に安価に触れられるメディアがCNN、BBC、VOA ぐらいしか無かった、ということです。&lt;/p&gt;
&lt;p&gt;しかし、今の時代は全く違います。今や、ニュースサイト、YouTube などの動画サイト、ソーシャルメディア、オンライン・コースなどで、生の英語に無尽蔵に触れることができます。このように、自分が本当に興味のある教材を簡単に見つけることができる環境の中、CNN や BBC から皆が始めなければならない理由は、何もありません。&lt;/p&gt;
&lt;p&gt;Time 紙や Newsweek 紙などを、英語教材として薦めるアドバイスもありますが、これも同じ理由で、あまり真面目に受け取らない方が良いと思います。それよりも、個人的には「英語の２ちゃんねる」である Reddit や、Hacker News 、自分の興味のある分野のブログなどを見ていた方がよほど面白く、学習が継続します。&lt;/p&gt;
&lt;p&gt;&lt;span style="display:block;text-align:center"&gt;
&lt;img alt="TED" src="https://upload.wikimedia.org/wikipedia/commons/thumb/4/42/Chris_Anderson_2007.jpg/2560px-Chris_Anderson_2007.jpg"&gt;
ソース: &lt;a href="https://www.flickr.com/photos/pmo"&gt;Pierre Omidyar&lt;/a&gt; - &lt;a href="https://www.flickr.com/photos/pmo/418251559"&gt;Flickr&lt;/a&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;同じように、最近では、TED を英語学習の教材として薦めるアドバイスも多くあります。確かに、TED は完全に無料で閲覧でき、興味深いトークも数多くあります。プレゼンテーションとしての質も高いので、特にパブリック・スピーキング等を学ぶにはとても良い教材です。&lt;/p&gt;
&lt;p&gt;ただ、普段から日本語で社会問題や科学技術に関するプレゼンテーションを見ないような人が TED を頑張って見て学習しても、やはり続かないのです。&lt;/p&gt;
&lt;p&gt;ちなみに、TED がこのように広く英語教材として使われている「大人の理由」は、実はその利用規約にあります。TED は、内容を改変しない限り、教材等として自由に二次利用しても良いという割と緩い利用規約をとっており、それが現在のように TED を教材としたアプリやウェブサイト等の二次教材の氾濫につながっているのです。でも、TED の利用規約が緩いかどうかは、教材として面白く学習が継続するかどうかに全く関係が無いことは明らかですよね。&lt;/p&gt;
&lt;p&gt;もちろん、CNN や BBC にも、色々な番組があり、「たまたま面白い教材を見つけたら、CNN だった」ということは十分可能です。Time 紙や Newsweek 紙も同じです。ただ、そういった最大公約数的な教材にとらわれるのではなく、「面白い教材は、人によって違う」ということを受け入れ、自由に考えて欲しいと思っています。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;「日本語でもやらないことを、英語でしたって続くわけがない」&lt;/strong&gt; これは、英語学習の鉄則です。&lt;/p&gt;
&lt;p&gt;&lt;a name="recommended-materials"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;オススメの教材&lt;/h3&gt;
&lt;p&gt;「面白い教材は、自分で探せ」といっても、そもそも英語があまり出来ないから教材を探しているのに、教材を探すには英語力が必要で、少し鶏と卵的ですよね。このセクションでは、私自身が、教材として実際使ってきたものをいくつか紹介します。ちなみに、コンピューターやエンジニアリングの話題に偏っています。もし、違う分野に興味があるのでしたら、同じ分野に興味があり、英語ができる人などにオススメの教材を聞いてみるのも良いかもしれません。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;MOOC&lt;/strong&gt;  …  &lt;a href="https://www.coursera.org"&gt;Coursera&lt;/a&gt; や &lt;a href="https://www.edx.org/"&gt;edX&lt;/a&gt; など、オンラインでプログラミング・機械学習などのトピックを学べるプラットフォームが数多く出てきましたね。これは、技術的なトピックと英語を両方学べるとても良い方法だと思います。コースによっては、英語や日本語で字幕が付いているものもあります。ただし、無料で動画を見るだけでは挫折してしまうので、思い切って最初に課金してしまうのが良いかもしれません。最初にお金を払ってしまうと、がんばって終わらせようという気になれます。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;YouTube の講義・トーク・チュートリアル動画&lt;/strong&gt;  …  これらも、上記の MOOC と似ているのですが、もし興味のあるトピックがあれば、それに関するチュートリアルやトークなどの動画が YouTube にきっとあるはずなので、それらを見て勉強するのはいかがでしょうか。自分は、ご飯を食べながら、など、時間のある時によくこういうたぐいの動画を見ています。(その結果が、まさにこのブログでもあるわけですが。)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://news.ycombinator.com/"&gt;Hacker News&lt;/a&gt;&lt;/strong&gt; ... プログラミングや技術などのトピックに興味があるならオススメです。毎日、数多くのニュースがアップされ、それに対して世界中の英語話者（非ネイティブも多い）が非常に質の高い議論をしています。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Podcast&lt;/strong&gt;  …  ポッドキャストはアメリカで特に流行っていて、ありとあらゆる分野のポッドキャストを聞くことができます。自分は、経済やテクノロジー、スタートアップの分野などの Podcast 購読して通勤や運動中などに聞いています。これまでに最も長く続いている英語教材です。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;リーディング&lt;/strong&gt;  …  リーディングについては、他にも教材として使えるものが大量にあって、特定のものをオススメするのが難しいのですが、自分が興味があって、かつ、自分の英語レベルよりも少しだけ低く、割と簡単に読み進められるようなものが良いかと思います。日本語で読書はされますか？もし読書をされるのでしたら、日本語で読んでいる興味のある分野の本を代わりに英語で読んでみるというのはどうでしょうか。小説でも雑誌でもノンフィクションでも、何でもかまいません。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ドラマ・映画&lt;/strong&gt;  …  自分のドラマや映画も、最高の英語教材になり得ます。ネイティブレベルのドラマや映画を完全に字幕無しで見るのはかなり難しいので、始めは英語字幕付き、それでも分からなければ、日本語字幕で一度見てから英語字幕付きで見るというのが良いでしょう。英語学習のアドバイスを見ると、「Friends」や「The Big Bang Theory」などのテレビドラマがオススメされていることが多いのですが、これも、上の BBC や CNN と同じで、これらのドラマが本当に面白くて見たいという場合を除いては、別に無理して見る必要も無いと思います。ちなみに、個人的なオススメは &lt;a href="https://www.imdb.com/title/tt2575988/"&gt;Silicon Valley&lt;/a&gt; で、将来、ソフトウェアエンジニアとして、特に、スタートアップで働くことに興味があるのでしたら是非オススメです。スラングや汚い言葉も多いので、決して簡単ではないのですが、英語字幕をオンにして見るのがコツで、そうすると意味が大体わかるので、面白く飽きずに最後まで見られるかと思います。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a name="not-recommended-materials"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;オススメしない教材・勉強法&lt;/h3&gt;
&lt;p&gt;オススメの教材を紹介したついでに、個人的にオススメしない教材や勉強法についても一言触れておこうかと思います。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; 単語帳 &lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;学校や TOEIC 受験の時などに単語帳で勉強した経験のある方も少なくないと思います。その中で、単語帳で単語を覚えておいたおかげで、いざ英語を使う時にさっと口から出てきた、という経験をお持ちの方がどのぐらい居るでしょうか。あまり居ないのではないのでしょうか。&lt;/p&gt;
&lt;p&gt;私の個人的な意見は、&lt;strong&gt;「単語帳は英語力の向上に全く役に立たない」&lt;/strong&gt; というものです。なぜ役に立たないかというと、単語が文脈から切り離されており、しかも、単語を日本語と結びつけて覚えてしまうからです。単語や言い回しは文脈の中で覚えないと使えません。また、単語を日本語と結びつけて覚えているだけでは、普段、話したり聞いたりする際についつい頭の中で「翻訳」する癖が付いてしまいます。最初の取り掛かりとしてはそれで良いのですが、その癖がずっと続くと、英語力が向上する足かせになってしまいます。&lt;/p&gt;
&lt;p&gt;ただ、初期の段階では、単語力が圧倒的に足らないので、英語をインプットするのが大変かもしれません。そういった場合は、難易度の低い教科書や、上で述べたような「自分の好きな教材」を学習する途中で、副次的に覚えていくのが良いと思います。&lt;/p&gt;
&lt;p&gt;これも私の考えですが、英語ができる人は単語を多く知っているため、そういう人を見ると、英語ができない人は、どうしても「単語をたくさん覚えれば、英語ができるようになるに違いない」と考えてしまいます。もちろん、語彙力というのは言語運用能力の大きな要素ではあるのですが、英語ができる人の語彙というのは通常、大量のインプット・アウトプット、外界とのインタラクションを通じて「獲得」されたものであって、単語帳などの人工的な手段によって丸暗記されたものではないのです。&lt;/p&gt;
&lt;p&gt;このことを表すのに私がよく使う例えですが、水泳選手はほぼ例外なく筋肉が付いており体がムキムキなのですが、筋トレだけしていれば水泳が上手くなるわけではないですよね。「筋力」というのは水泳のための重要な要素なのですが、それだけ鍛えても総合的な「水泳の能力」は向上しないのです。&lt;/p&gt;
&lt;p&gt;またこれも「大人の事情」なのですが、単語帳は単語を集めてきて意味・例文を付けるだけで誰でも作れ、出版できるので、出版社としては楽に作れ、売りやすいのです。単語自体には著作権も無いのでやりやすい。買う方もそれで英語力が伸びた気になるのでよく売れるのです。一方で、きちんと設計・執筆された教科書というのは、非常に作るのが難しいものです。単語や文法項目を、学習者が体系立てて覚えられるように並び替え、それを使った会話文などを書きます。単語帳は誰でも作れるのに対し、本当に良い教科書を作れるのは限られた専門家のみです。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; 英語でブログやソーシャルメディアを書く &lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="display:block;text-align:center"&gt;
&lt;img alt="英語でブログを書く" src="images/pbel-blogging.jpg"&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;「英語でブログを書いてみよう」というアドバイスをしている書籍をいくつか見かけたことがあるのですが、これも、あまり良いアドバイスであるとは言えません。まず、続きません。日本語でも、定期的にブログを書き続けて公開しているということは、そういった経験の無い方の想像以上に難しいものです。また、誰にもフィードバックをもらえません。単に書くだけでは、もし間違えて滅茶苦茶な英語を書いていても、訂正してくれる人も居ないでしょう。会話と違って、「通じているかどうか」すら分かりません。最後に、そんなブログ、おそらく誰も読みません。英語の練習のために拙い文法で書かれた知らない人の日記を読みたいと思う人は誰も居ないでしょう。&lt;/p&gt;
&lt;p&gt;もし、どうしても日記などを書いて書く能力を向上させたいということであれば、他のネイティブから添削がもらえる &lt;a href="https://lang-8.com/"&gt;Lang-8&lt;/a&gt; というサービスを使ってみることをオススメします。&lt;/p&gt;
&lt;p&gt;もし、あなたが英語でどうしても発信したい内容があって、英語で発信する必要があるのであれば、ブログを英語で書けばよいのですが、それは英語学習の「結果」であって、「目的」とは考えない方がいいかもしれません。私も、最近は英語でブログを執筆をしており、Facebook や Twitter を英語で書いていますが、それは単に Facebook 上の友人の多くが非日本語話者であって英語で書かないとほとんどの人が理解できないからそうしているだけであって、英語の学習が目的ではないのです。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; 言語交換 &lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;言語交換とは、例えば日本語を勉強したい英語話者と、英語を勉強したい日本人がペアになって、お互いに英語を教え合うという方法です。言語交換のためにユーザー同士をマッチングさせるサービスやアプリなどがたくさんあって、私はこれをけっこうやったことがあるのですが、やはり続きません。理由は、お互いに素人なので他愛のないおしゃべりで終わってしまうことと、「英語を学びたい＝英語で話したい」こちらの意図と、「日本語を学びたい＝日本語で話したい」向こうの意図が相反するので、例えば「半分ずつきっちり時間を区切って話す」のようにしないと、結局、上手な方の言語で話して全て終わってしまうのです。&lt;/p&gt;
&lt;p&gt;ただ、言語交換、新たな友人を作るための手段としてはとても良いものだと思います。日本には、日本語をもっと学びたい、日本人ともっと仲良くなりたいと思っている外国人がたくさん居ます。また、海外にも「日本に行ったこと無いが、日本語を日本人と話してみたい」という方がたくさん居ます。そういった方々とつながる手段、とっかかりとしてはありだと思います。&lt;/p&gt;
&lt;p&gt;&lt;a name="hacking-5-neighbors"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;周りの5人をハックする&lt;/h3&gt;
&lt;p&gt;上で、「人は周りの5人の平均になる」ということを書きました。自分の周りの5人の英語力が平均的に低いような環境に居る人が、自分だけ英語が上達するのは至難の業です。ということは逆に言うと、自分の周りの5人の英語力が向上すれば、言い換えると、英語のできる人に囲まれるようにすれば、自分の英語が上達する可能性はずっと高くなります。これが、「周りの5人をハックする」ということです。もちろん、家族と縁を切ったり、友達と絶交したりといったことを推奨しているわけでないことは、分かっていただければと思います。&lt;/p&gt;
&lt;p&gt;これには色々な方法があって、実現する手段も人それぞれなのですが、一番わかりやすいのが、&lt;strong&gt;「英語しか話せない人と結婚する」&lt;/strong&gt; という方法でしょう。ほぼ間違いなくペラペラになります。もちろん、ほとんどの人にとって現実的ではないでしょうし、英語を上達させるために人と結婚するというのも変な話なので、あまり真面目に取らないで欲しいのですが、例として分かりやすいので出しました。（この変種で、英語しか話せない人と付き合うというのもあって、こちらの方がいくらか現実的かもしれません）&lt;/p&gt;
&lt;p&gt;&lt;span style="display:block;text-align:center"&gt;
&lt;img alt="旅行に行く" src="images/pbel-travel.jpg"&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;もっと多くの人にとって現実的なのは、&lt;strong&gt;「旅行に行く」&lt;/strong&gt; ことでしょう。英語圏の国に行けば、自然と英語を学習するモチベーションが上がりますし（上のモチベーション・グラフをハックする）、現地で実際に英語を使う機会もあるかもしれません。前述の通り、日本では、英語学習産業のせいで、「日本に居ながら英語が上達する」という歪んだメッセージが浸透していますが、「周りの5人をハックする」のに手っ取り早く手頃な方法が「旅行に行く」なのです。&lt;/p&gt;
&lt;p&gt;ただ、日本語を話す家族や仲の良い友人達と旅行に行くと、単に現地で日本語をひたすら話して、現地の人ともほとんど話すことなく終わってしまうので、楽しければ良いのですが、英語の学習という意味で考えるとイマイチですよね。&lt;/p&gt;
&lt;p&gt;そこで個人的にオススメしているのが、&lt;strong&gt;「一人でホームステイに行く」&lt;/strong&gt; です。もちろん、家族や仕事の事情があって難しい人も居るでしょうが、AirBnB や &lt;a href="https://www.homestay.com/"&gt;Homestay.com&lt;/a&gt;では、一週間などの短期でも、ホストファミリーと交流できる宿を探すことができます。私も、２年前に、&lt;a href="http://masatohagiwara.net/blog/autonomy-remarkability-and-career-capital.html"&gt;韓国に一人で短期ホームステイに行った&lt;/a&gt;おかげで、韓国語がだいぶ上達しました。&lt;/p&gt;
&lt;p&gt;また、外国に行かなくても、日本で開催されている &lt;strong&gt;何らかの集まりに参加してみる&lt;/strong&gt; のも良いかもしれません。その場合、「英語カフェ」のように、語学の上達そのものを目的にしないものではない方が良いでしょう。これも、どんな方法が良いかは人それぞれなのですが、例えばプログラミングや機械学習系のミートアップや勉強会、スポーツや音楽の同好会など、何か目的があって日本人と英語話者が自然と集まっているようなところが良いと思います。&lt;/p&gt;
&lt;p&gt;最後に、住んでいる地域や個人の事情なので、旅行や集まりに参加するといったことが難しいかもしれません。そういった場合は、&lt;strong&gt;インターネット上のコミュニティを探してみる&lt;/strong&gt; というのも手かもしれません。最近では、Slack などのオンラインチャットを使い、興味を持った人同士が集まり英語で情報交換をしているコミュニティが大量にありますので、探して入ってみましょう。こういったコミュニティを見つけるには、「○○ slack community」（○○は自分の興味のあるトピック）などで検索してみると良いでしょう。&lt;/p&gt;
&lt;p&gt;&lt;a name="social-commitment"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;社会的な制約を使う&lt;/h3&gt;
&lt;p&gt;「モチベーション・グラフ」をハックする方法としては、他には「社会的な制約を使う」という方法があります。「制約」というと分かりにくいのですが、「コミットメント」と言い換えても良いかもしれません。&lt;/p&gt;
&lt;p&gt;人間は社会的な動物なので、「人前で恥をかきたくない」「人をがっかりさせたくない」という気持ちが強く働く傾向があります。この傾向を逆に利用して、英語のモチベーション・グラフをハックするために使うということです。&lt;/p&gt;
&lt;p&gt;一つ例を挙げると、例えば &lt;strong&gt;勉強会で発表したり、勉強会を主催する&lt;/strong&gt; という方法があります。勉強会の中に英語話者が居て英語で議論ができればもちろん良いのですが、日本語でも、例えば英語で何かを調査した結果を何日までに発表する、ということを約束してしまえば、英語を使わざるを得ない状況に自分を追い込むことができます。&lt;/p&gt;
&lt;p&gt;ただ一点、日本でよくあるのは、輪講形式の、英語の論文や教科書を順番に読んで日本語に訳し、発表していくというものですが、これは訳して分かった気になるだけなので、特に英語の勉強としてはほとんど意味がありません。もし、参加者の英語レベルが一定以上であるなら、オススメなのは、英語の論文や教科書を順番に読んで、英語で直接発表・議論することです。そうすると、本当に分かってないと議論ができないので、英語もコンテンツも本当に自分のものになります。私は、大学院生の時、日本人＋友人の留学生たちに声をかけて、この形式の（完全に英語だけで技術を勉強する）勉強会を主催していたのですが、英語力が飛躍的に向上したのを覚えています。&lt;/p&gt;
&lt;p&gt;上の TOEIC のセクションで少し述べましたが、社会的な制約として、&lt;strong&gt;英語検定試験を受ける&lt;/strong&gt; というのも、モチベーション・グラフの形を変える良い方法だと思います。英語検定試験を受けることを周りにあらかじめ公言しておけば、点数が低かったり合格しなかった場合に少し恥ずかしいので、英語学習のモチベーションになり得ます。&lt;/p&gt;
&lt;p&gt;ただ、TOEIC は前述した２つの問題があって、初心者以外にはオススメしていません。自分の総合的な英語力を測定し、モチベーションを向上させるという目的であれば、英検や TOEFL のような、スピーキングやライティングの問題も含まれ、もう少し高い難易度までカバーしている試験が良いでしょう。&lt;/p&gt;
&lt;p&gt;&lt;span style="display:block;text-align:center"&gt;
&lt;img alt="Scott Young 氏" src="images/pbel-scott-young.png"&gt;
Scott Young 氏（右）。&lt;a href="https://www.youtube.com/watch?v=vPnHJ1OoK5o"&gt;本人の YouTube ビデオ&lt;/a&gt;より。
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;この「社会的な制約」をうまく使った例が、ブロガーである Scott Young 氏による、&lt;a href="https://www.scotthyoung.com/blog/myprojects/the-year-without-english-2/"&gt;「一年間外国語生活プロジェクト」&lt;/a&gt;です。このプロジェクトで Young 氏は、スペイン・ブラジル・中国・韓国を旅しながら、現地語を学ぶことによって、なるべく英語（彼の母語）を話さないように一年間生活することを公言し、実際に実行に移しました。本人の外国語がどのぐらい上達したかは、&lt;a href="https://www.youtube.com/watch?v=vPnHJ1OoK5o"&gt;当プロジェクト&lt;/a&gt;のビデオを見て欲しいのですが、英語話者が一年間で学習したレベルとしては文句無いレベルまで達しています。これも、家族や生活など、さまざまな制約があり、誰にでも真似できるものではないのですが、プロジェクトにコミットし、言語を学ぶ例として参考になるところが多いですね。&lt;/p&gt;
&lt;p&gt;&lt;a name="hobbies-in-english"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;英語を使うことでしかできないことを趣味にする&lt;/h3&gt;
&lt;p&gt;これは少し難易度が高いのですが、うまく行くと英語上達の非常に大きなパワーとなるので一考の価値があります。これの最もわかりやすい例が、卓球の福原愛選手で、幼い時から中国人のコーチに教わり、その後も試合やトレーニングで中国に頻繁に行く機会があったため、中国語が非常に堪能です。もちろん、彼女が中国語が上達するために卓球を始めたわけではないのは確かなのですが、他の色々なことにも応用できる事例です。&lt;/p&gt;
&lt;p&gt;&lt;span style="display:block;text-align:center"&gt;
&lt;img alt="アメリカ人からジャズ・ピアノを習う" src="images/pbel-piano.jpg"&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;もう一つ、これは私自身の例なのですが、私は音楽が好きで、アメリカに来てからジャズ・ピアノをアメリカ人の先生から直接学んでいました。クラシック・ピアノであれば日本人やアジア系の先生も多いのですが、ジャズになるとアメリカ人の先生から直接英語で学ぶしか方法がありませんでした（別に、人種で探していたわけではありませんが）。副作用として、普段、仕事や生活で使わないような語彙や言い回しまで色々と学ぶことができ、英語もずいぶんと上達しました。&lt;/p&gt;
&lt;p&gt;他にも、例えばアメリカでしか流行っていないようなマイナーなゲームや映画、ミュージシャンなどにハマる、日本語であまり情報の無いような特定の料理や芸能人などに詳しくなる、など、色々な方法があります。もし、普段から興味があるが、とっかかりが見つからなかったような趣味について、「英語と組み合わせてみる」という方法もあっても良いかもしれません。&lt;/p&gt;
&lt;p&gt;&lt;a name="principles"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;英語学習の原則&lt;/h2&gt;
&lt;p&gt;最後に、英語を学習する上で、重要な原則を挙げておきます。これらは、主に第二言語習得の分野で、データや研究によって科学的な裏付けがされているものばかりです。&lt;/p&gt;
&lt;p&gt;&lt;a name="sleep"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;睡眠&lt;/h3&gt;
&lt;p&gt;まず、最も大切な原則から。英語の学習の前と後は、きちんと寝てください。最近の脳科学の進歩から、睡眠は、脳の記憶を整理し、日中学んだことを長期記憶へと定着させる作用があることが分かっています。特に、夜寝る前と朝起きた後に勉強するのが効果的です。私はこれを &lt;strong&gt;「睡眠サンドイッチ法」&lt;/strong&gt; と呼んでいます。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;睡眠をサンドイッチしない学習は、学習ではありません。&lt;/strong&gt; 上記で「3000時間が必要」という話をしましたが、睡眠不足の状態でする学習はカウントしないように注意してください。&lt;/p&gt;
&lt;p&gt;&lt;a href="https://yomidr.yomiuri.co.jp/article/20180517-OYTEW259122/"&gt;日本人の平均睡眠時間は先進諸国の中でも最下位&lt;/a&gt;というデータもあり、日本人が英語ができないのは、ひょっとしたらこの睡眠の短さから来る学習の非効率性も関与しているのかもしれませんね。&lt;/p&gt;
&lt;p&gt;ちなみに、英語でのプレゼンや交渉、面接など、英語を使う大切な用事がある場合、当日の直前になって焦って詰め込みをするのではなく、&lt;strong&gt;「前日に予習をして、寝る」&lt;/strong&gt; 方が効果的です。もっと良いのは、&lt;strong&gt;前々日までに予習を終わらせておき、前日にパーティーや飲み会なので英語で口を慣らせておき、よく寝て当日に臨む&lt;/strong&gt; のが良いです。一度だまされたと思ってやってみてください。当日の英語のスムーズさが全然違います。これも、睡眠のおかげです。&lt;/p&gt;
&lt;p&gt;また、睡眠を挟むのと同時に、復習する間隔も大切です。例えば、トータルで１０時間、英語の学習をする場合、一日で一気に１０時間やるよりも、１日１時間ずつ、１０日かけてやる方がはるかに記憶に定着します。もっと良いのは、復習する間隔をだんだんと広げていくことです（これは&lt;a href="https://ja.wikipedia.org/wiki/%E9%96%93%E9%9A%94%E5%8F%8D%E5%BE%A9"&gt;間隔反復法&lt;/a&gt;と呼ばれています）。下の図で言うと、左ではなく右のように学習間隔を伸ばす方が効果的です。&lt;/p&gt;
&lt;p&gt;&lt;span style="display:block;text-align:center"&gt;
&lt;img alt="練習の間隔を分散させる" src="images/pbel-distributed-practice.png"&gt;
（&lt;a href="http://masatohagiwara.net/shenzhen2018.html"&gt;筆者作成のスライドより引用&lt;/a&gt;）
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a name="implicit-learning"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;意識的な勉強と、無意識の学習&lt;/h3&gt;
&lt;p&gt;次に、２つ目に大切な原則です。言語が上達するためには、「無意識の学習」が非常に大切です。「意識的な勉強」とは、例えば「fundamental は、基本的な、という意味の形容詞だ」という単語の定義や、「主語が三人称単数で現在の場合は、動詞に-sが付く」という文法など、言葉に出して表せる知識を覚えるものです。一方で、「無意識の学習」は、例えば自転車に乗ったりといったことや、スポーツでの特定の動きなどが、いつのまにか、考えなくてもできるようになる、というプロセスのことを指します。「自動化」とも呼ばれています。&lt;/p&gt;
&lt;p&gt;英語の場合だと、例えば「Every morning my mom ___ up at 6 o'clock」という文を見せられた時に、「えっーと、主語が単数で・・」と考えることなく、とっさに「gets」もしくは「wakes」と答えられるようになることを指します。日本語だと、例えば助詞の使い分け、などがこれに相当するでしょう。これを読んでいる皆さんは日本語話者だと思うのですが、例えば「学校に行く」と「学校へ行く」がどう違うか、日本語を学習中の外国人に聞かれたら説明できるでしょうか。こういった細かなニュアンスの違いを、母語話者は皆「なんとなく体で分かっている」のです。上に書いた「三単現のｓ」も、学校で学んで多くの人が知っているのですが、いざ喋る時になるとスラスラと使えるのはごく一部です（これは別に恥ずかしいことでも何でもなく、３単現のｓは文法項目の中でも特に習得が難しいことが知られています）。&lt;/p&gt;
&lt;p&gt;学校で習う英語は、単語の意味を日本語に結びつけて覚えたり、文法規則を教科書から学んだりといった、「意識的な勉強」が中心です。一方で、英語で話したり聞いたりといった、特に即時性を必要とする言語能力においては、この「無意識の学習」が大きな役割を占めます。幸い、無意識な学習は、繰り返しによって自動的に起きます。このために大切なのが、以下に述べるインプット・アウトプットなのです。&lt;/p&gt;
&lt;p&gt;&lt;a name="input-output"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;インプット・アウトプット&lt;/h3&gt;
&lt;p&gt;インプットとアウトプット、両方が重要です。「聞き流すだけで英語が上手くなる」という宣伝文句の教材などがありますが、英語に耳が慣れるということはあるかもしれませんが、聞き流すだけでは本質的な英語力は向上しません。インプットは、自分が理解のできる英語を、大量にインプットすることが大切です。そのために、上で述べたような「モチベーション・ハック」をし、自分に合った教材を見つけることが大切なのです。&lt;/p&gt;
&lt;p&gt;アウトプットの際には、ネイティブ（もしくはネイティブレベルに英語が上手な相手）と話すことは重要です。日本に居ると、上で述べたように、「日本に居ながら英語が上達する」という宣伝文句が氾濫しているせいで、ネイティブと話すことが過小評価されているような気がします。なぜネイティブと話すことが良い練習かというと、もちろん、相手の発音や表現から学べるということもあるのですが、それ以外にも重要な点があります：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;即時性 …  話している時は、すぐに反応しなければいけないので、英語を使う能力が増強される&lt;/li&gt;
&lt;li&gt;「記憶を取り出す」 …  記憶を定着させるためには、その記憶を取り出す練習をすることが重要。会話するためには、発音・語彙・文法など、学んだことを常に取り出して使わなければいけない。&lt;/li&gt;
&lt;li&gt;フィードバック …  通じているかどうかがすぐに分かる（上の「ブログを英語で書く」との大きな違い）。&lt;/li&gt;
&lt;li&gt;見た目の情報 …  視覚情報も使える場合、身振りなどが見えるので意味が伝わりやすく、しかも、相手の口の形などを自然と観察できるので、こちらの発音も改善する&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;さて、スピーキングの話になると、「とにかく文法や発音なんて気にせず話せ」「通じればいい」という意見をよく見るのですが、これは程度問題だと思っています。確かに、日本の英語学習者には、「文法や発音が完璧になるまであまり話したくない」「失敗したら恥ずかしい」と感じる人が多い傾向があって、インプット・アウトプットをする際の足かせになっているケースが多いような気がします。しかも、発音については「臨界期仮説」と呼ばれる年齢の影響が確かにあって、特に思春期を過ぎてから英語を学習しても、ネイティブのような発音になることは非常に難しいことが数々の研究によって示されています。&lt;/p&gt;
&lt;p&gt;しかし、間違った文法や発音で話し続けると、それが間違って定着してしまう、化石化、という現象があります。スポーツや楽器でも、まずはきちんとしたフォームが大切だということを疑う人は少ないでしょう。間違ったフォームでは、上手くならないばかりか、下手をしたら怪我をしてしまう可能性もあります。それと同様に、特に学習をし始めの頃は、正しい文法と発音をなるべく真似る努力をすべきです。&lt;/p&gt;
&lt;p&gt;&lt;a name="shadowing"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;シャドーイング&lt;/h3&gt;
&lt;p&gt;&lt;span style="display:block;text-align:center"&gt;
&lt;img alt="会話" src="images/pbel-conversation.jpg"&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;地理的な要因や個人的な事情で、ネイティブが近くに居ない、という場合もあるかと思います。その場合は、英会話学校やオンライン英会話などで探すことになるのですが、一人でもスピーキングの練習ができる方法として、「シャドーイング」という方法があり、これは個人的にもオススメしています。これは、英語を聞きながらそれを真似して、すぐ後について繰り返して発音する練習方法で、リスニングやスピーキング、発音・イントネーション、語彙力の向上など、英語力の色々な側面に効果があります。&lt;/p&gt;
&lt;p&gt;ちなみに、英語で文を聞き終わってからはじめて、その文を繰り返す「リピーティング」というシャドーイングに似た方法もあるのですが、これも頭の中で文を完全に保持しながら自分の口ですべて再現しなければならないので、非常に効果的です。聞いてから話し始めるまでの間隔がシャドーイングより長いのでずっと難しく、やってみるとすぐ疲れてしまうのですが、一人でできるスピーキングの方法としてオススメしています。&lt;/p&gt;
&lt;p&gt;&lt;a name="finding-a-good-teacher"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;良い先生を探す&lt;/h3&gt;
&lt;p&gt;私がこれまで英会話学校、オンライン英会話、個人教師などをさんざん試して分かったことは、「良い先生と悪い先生の間には、天と地ほどの距離がある」ということです。&lt;/p&gt;
&lt;p&gt;残念ながら、英会話学校やオンライン英会話で割り当てられる「先生」の多くが、&lt;strong&gt;「英語が話せるただの人」&lt;/strong&gt; です。他愛のない会話をして、たまに使いもしないイディオムを教えてくれる程度の「授業」をたくさん受けました。日本語でも、話が合うかとうか分からない他人といきなりスカイプで繋がれて「３０分話せ」と言われたら変な気持ち（人によっては、苦痛）ですよね。もちろん、こういった授業も、スピーキングの練習になるので、価値が無いと言っているわけではありません。英語ネイティブと仲良くなるためには、「面白いイディオム」を知っているのは、実はけっこう重要だったりするのです。&lt;/p&gt;
&lt;p&gt;一方、私の知っている「良い先生」は、英語がネイティブであるというだけでなく、言語学や音声学、第二言語習得などの学問分野の基礎があり、生徒と５分程度会話しただけでその生徒の発音や文法、イディオムなど、どこに問題があるかを素早く観察し、その後の改善ロードマップを提示できます。文法的なことについて説明を求められても、「英語ではこうなっている」と言葉を濁すことなく、こちらが納得する形で説明をしてくれます。学習者の個別のニーズにも敏感で、上の「好きな教材を使う」といったことへの理解も深いです。このような先生に当たると、その後の英語力の伸びが全く違ってきます。&lt;/p&gt;
&lt;p&gt;スポーツでも、良いコーチを持つことが重要であることを否定する人はあまり居ないと思います。幸い、インターネットやスマートフォンの普及に伴って、良いネイティブの先生を探す敷居は格段に低くなりました。英語の練習のための先生を探す際には、ぜひ良い先生を探してください。&lt;/p&gt;
&lt;p&gt;&lt;a name="summary"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;おわりに&lt;/h2&gt;
&lt;p&gt;本ガイドをここまで読んでくださり、ありがとうございました。&lt;/p&gt;
&lt;p&gt;最後に、一つだけお願いがあります。このガイドを読んだご感想やご質問、「私は、こうして英語が上達した」などのエピソードなどがありましたら、&lt;a href="mailto:hagisan@gmail.com"&gt;私の方までぜひ教えてください&lt;/a&gt;。左のメニューバーの「お問い合わせ」からも私にメッセージを送ることができます。&lt;/p&gt;
&lt;p&gt;このガイドに書いた内容は、これまで私にメール等でご質問やご感想を送ってくださった読者の方々へのアドバイスが元になっています。この場を借りてお礼申し上げます。&lt;/p&gt;
&lt;p&gt;本ガイドは、&lt;a href="http://creativecommons.org/licenses/by-nc-sa/4.0/"&gt;Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License&lt;/a&gt; にて公開します。出典を明記した上で非営利目的であれば自由にコピーしていただいて構いません。&lt;/p&gt;
&lt;p&gt;Images are designed by &lt;a href="http://www.freepik.com"&gt;Freepik&lt;/a&gt;&lt;/p&gt;</content></entry><entry><title>グーグルマップの城壁—データからデータを作り出すAI戦略</title><link href="http://englishforhackers.com/google-maps-moat.html" rel="alternate"></link><published>2019-03-22T00:00:00-04:00</published><updated>2019-03-22T00:00:00-04:00</updated><author><name>Masato Hagiwara</name></author><id>tag:englishforhackers.com,2019-03-22:/google-maps-moat.html</id><summary type="html">&lt;p&gt;グーグルマップが、&lt;a href="https://japanese.engadget.com/2019/03/22/google/"&gt;ゼンリンから自社独自のデータに切り替えたことによるクオリティの低下&lt;/a&gt;が話題になっていますが、グーグルマップが何年も前から、衛星写真やストリートビューなどのデータから、機械学習の手法を駆使して地図データをすごい勢いで充実させていることはあまり知られていません。&lt;/p&gt;
&lt;p&gt;Google や Apple の地図サービスなどの事情に詳しい Justin O'Beirne 氏による&lt;a href="https://www.justinobeirne.com/google-maps-moat"&gt;「グーグルマップの城壁」&lt;/a&gt;と題されたこの記事では、Google がいかに画像認識と機械学習の技術を駆使し、「データからデータを作り出す」ことに成功し、自社サービスに他社が追いつけないような「城壁」を築くのに成功したか、ということが書かれています。&lt;/p&gt;
&lt;p&gt;個人的な話ですが、最近、&lt;a href="http://masatohagiwara.net/leaving-startup-and-becoming-independent.html"&gt;機械学習エンジニアとして独立して&lt;/a&gt;から、主にスタートアップ等を対象に、人工知能・機械学習系のプロジェクト戦略についてコンサルティングをしています。特に、機械学習やデータが鍵となるサービスでは、いかにデータを収集し、機械学習の技術を活用しながら新たなデータを生み出したり、競合が追いつけない勢いでプロダクトを改善するか、ということが非常に大切になってきます。その戦略を考える上でも、非常に示唆に富む記事になっています。&lt;/p&gt;
&lt;p&gt;本記事の内容については、&lt;a href="https://www.justinobeirne.com/google-maps-moat"&gt;オリジナルの記事&lt;/a&gt;をご参照ください。（2019/3/24追記：本記事に含まれていたまとめは、著作権上の問題から削除しました。）&lt;/p&gt;
&lt;p&gt;なお、グーグルマップに関する他の記事を見ていると、例えば Android …&lt;/p&gt;</summary><content type="html">&lt;p&gt;グーグルマップが、&lt;a href="https://japanese.engadget.com/2019/03/22/google/"&gt;ゼンリンから自社独自のデータに切り替えたことによるクオリティの低下&lt;/a&gt;が話題になっていますが、グーグルマップが何年も前から、衛星写真やストリートビューなどのデータから、機械学習の手法を駆使して地図データをすごい勢いで充実させていることはあまり知られていません。&lt;/p&gt;
&lt;p&gt;Google や Apple の地図サービスなどの事情に詳しい Justin O'Beirne 氏による&lt;a href="https://www.justinobeirne.com/google-maps-moat"&gt;「グーグルマップの城壁」&lt;/a&gt;と題されたこの記事では、Google がいかに画像認識と機械学習の技術を駆使し、「データからデータを作り出す」ことに成功し、自社サービスに他社が追いつけないような「城壁」を築くのに成功したか、ということが書かれています。&lt;/p&gt;
&lt;p&gt;個人的な話ですが、最近、&lt;a href="http://masatohagiwara.net/leaving-startup-and-becoming-independent.html"&gt;機械学習エンジニアとして独立して&lt;/a&gt;から、主にスタートアップ等を対象に、人工知能・機械学習系のプロジェクト戦略についてコンサルティングをしています。特に、機械学習やデータが鍵となるサービスでは、いかにデータを収集し、機械学習の技術を活用しながら新たなデータを生み出したり、競合が追いつけない勢いでプロダクトを改善するか、ということが非常に大切になってきます。その戦略を考える上でも、非常に示唆に富む記事になっています。&lt;/p&gt;
&lt;p&gt;本記事の内容については、&lt;a href="https://www.justinobeirne.com/google-maps-moat"&gt;オリジナルの記事&lt;/a&gt;をご参照ください。（2019/3/24追記：本記事に含まれていたまとめは、著作権上の問題から削除しました。）&lt;/p&gt;
&lt;p&gt;なお、グーグルマップに関する他の記事を見ていると、例えば Android 上のユーザーの位置情報なども使って、道路の情報を改善していると思われる箇所があるそうです。本記事に書かれた内容以外にも、利用できる様々なデータを使ってグーグルマップを改善していると思われます。&lt;/p&gt;</content></entry><entry><title>教科書より分かりやすい！数学を動画で学ぶ英語YouTubeチャネル 3Blue1Brown がオススメ</title><link href="http://englishforhackers.com/learn-basic-math-on-3blue1brown-youtube-channel.html" rel="alternate"></link><published>2019-01-24T00:00:00-05:00</published><updated>2019-01-24T00:00:00-05:00</updated><author><name>Masato Hagiwara</name></author><id>tag:englishforhackers.com,2019-01-24:/learn-basic-math-on-3blue1brown-youtube-channel.html</id><summary type="html">&lt;p&gt;&lt;img src="images/3blue1brown.jpg" width="320"/&gt;&lt;/p&gt;
&lt;p&gt;「どの教科書より圧倒的に分かりやすい」&lt;/p&gt;
&lt;p&gt;「なんでもっと早くこれを知らなかったのか」&lt;/p&gt;
&lt;p&gt;「全ての数学のコースで必修にすべき！」&lt;/p&gt;
&lt;p&gt;と、絶賛の声でコメント欄が溢れる、人気 YouTube チャンネル、「&lt;a href="https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw"&gt;3Blue1Brown&lt;/a&gt;」をご存知でしょうか？&lt;/p&gt;
&lt;p&gt;スタンフォード大数学科卒の Grant Sanderson 氏によって作成・公開されている、数学の基礎的な概念をたいへん分かりやすいビジュアルと共に解説する動画シリーズです。&lt;/p&gt;
&lt;p&gt;全ての動画が、数学の概念を「直感的に理解」することを目標に作られているので、数学が苦手な人はもちろん、概念や定理などを一通り学んだことがある人が見ても、「こういう事だったのか」と目からウロコなのは間違いありません。&lt;/p&gt;
&lt;p&gt;例えば、「線形代数の基本」と題された動画シリーズの初回「ベクトルとは何か」では、ベクトルという概念がこのように紹介されています：&lt;/p&gt;
&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/fNk_zzaMoSs" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;他にも「線形変換と行列」と題されたこの動画では、行列の掛け算はそもそも何をするものなのか、なぜその形になるのかを、直感的に解説しています：&lt;/p&gt;
&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/kYB8IZa5AuE" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;「線形代数の基本」シリーズの全ての動画は、&lt;a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab"&gt;こちら&lt;/a&gt;から全て見ることができます。また、同様に&lt;a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr"&gt;「微積分学の基礎」シリーズ&lt;/a&gt;もあります。&lt;/p&gt;
&lt;p&gt;動画には、全て英語字幕が付いているので（残念ながら日本語字幕はありませんが）、聞き取れないところも字幕で確認しながら学習を進めることができます。&lt;/p&gt;
&lt;p&gt;個人的にオススメなのが …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;img src="images/3blue1brown.jpg" width="320"/&gt;&lt;/p&gt;
&lt;p&gt;「どの教科書より圧倒的に分かりやすい」&lt;/p&gt;
&lt;p&gt;「なんでもっと早くこれを知らなかったのか」&lt;/p&gt;
&lt;p&gt;「全ての数学のコースで必修にすべき！」&lt;/p&gt;
&lt;p&gt;と、絶賛の声でコメント欄が溢れる、人気 YouTube チャンネル、「&lt;a href="https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw"&gt;3Blue1Brown&lt;/a&gt;」をご存知でしょうか？&lt;/p&gt;
&lt;p&gt;スタンフォード大数学科卒の Grant Sanderson 氏によって作成・公開されている、数学の基礎的な概念をたいへん分かりやすいビジュアルと共に解説する動画シリーズです。&lt;/p&gt;
&lt;p&gt;全ての動画が、数学の概念を「直感的に理解」することを目標に作られているので、数学が苦手な人はもちろん、概念や定理などを一通り学んだことがある人が見ても、「こういう事だったのか」と目からウロコなのは間違いありません。&lt;/p&gt;
&lt;p&gt;例えば、「線形代数の基本」と題された動画シリーズの初回「ベクトルとは何か」では、ベクトルという概念がこのように紹介されています：&lt;/p&gt;
&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/fNk_zzaMoSs" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;他にも「線形変換と行列」と題されたこの動画では、行列の掛け算はそもそも何をするものなのか、なぜその形になるのかを、直感的に解説しています：&lt;/p&gt;
&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/kYB8IZa5AuE" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;「線形代数の基本」シリーズの全ての動画は、&lt;a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab"&gt;こちら&lt;/a&gt;から全て見ることができます。また、同様に&lt;a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr"&gt;「微積分学の基礎」シリーズ&lt;/a&gt;もあります。&lt;/p&gt;
&lt;p&gt;動画には、全て英語字幕が付いているので（残念ながら日本語字幕はありませんが）、聞き取れないところも字幕で確認しながら学習を進めることができます。&lt;/p&gt;
&lt;p&gt;個人的にオススメなのが、この&lt;a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi"&gt;「ニューラル・ネットワーク」のシリーズ&lt;/a&gt;。この初回の「ニューラル・ネットワークとは何か」と題されたビデオでは、順伝播型ニューラル・ネットワークが、数字を認識する仕組みを分かりやすく可視化して解説しています：&lt;/p&gt;
&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/aircAruvnKk" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;また、こちらのビデオ「誤差逆伝播法とは実際何なのか」では、誤差逆伝播法を分かりやすく解説しています：&lt;/p&gt;
&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/Ilg3gGewQ5U" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;このチャンネルには、数学の他のコンセプトや、なんと物理の基本的な概念についても、解説する動画が多く上がっています。個人的に感動したのが、この フーリエ変換のビデオ。講義などで習ったこともあり、日常でもよく利用している概念ですが、こんなに分かりやすい解説を見たのは初めてです：&lt;/p&gt;
&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/spUNpyF58BY" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;なお、Grant 氏は、スタンフォード大学卒業後、Khan Academy でコンテンツ作成に携わった後、現在ではフルタイムで 3Blue1Brown の動画制作にコミットしているそう。見やすく綺麗なビジュアルは、自身で作成した &lt;a href="https://github.com/3b1b/manim"&gt;manim&lt;/a&gt; というオープンソースのアニメーションエンジンで作られています。「3Blue1Brown」という名前は、Grant氏の目の色から来ているそうです。&lt;/p&gt;</content></entry><entry><title>人工知能の第一人者 Andrew Ng 氏がアドバイスする機械学習キャリアの築き方</title><link href="http://englishforhackers.com/andrew-ng-building-a-career-in-machine-learning.html" rel="alternate"></link><published>2018-12-18T00:00:00-05:00</published><updated>2018-12-18T00:00:00-05:00</updated><author><name>Masato Hagiwara</name></author><id>tag:englishforhackers.com,2018-12-18:/andrew-ng-building-a-career-in-machine-learning.html</id><summary type="html">&lt;p&gt;現在は、AI Fund の立ち上げ、Landing.ai の CEO 、そして Stanford の教授として活躍する人工知能の第一人者 Andrew Ng 氏。ACM (国際計算機学会) の生涯学習のウェビナーにて「機械学習キャリアの築き方」という内容で話をした動画が、とても良いアドバイスが満載でしたので、ここでまとめと共に紹介します。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Andrew Ng 氏" src="images/andrew-ng.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=4kiHsIaK9_w"&gt;Andrew Ng on Building a Career in Machine Learning
&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;AI は技術的ブレイクスルー&lt;ul&gt;
&lt;li&gt;多くの機会&lt;/li&gt;
&lt;li&gt;多くのキャリア機会&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;素晴らしいキャリアを築いている人は何が違うのか&lt;/li&gt;
&lt;li&gt;どうやってインパクトのある、人の役に立つ仕事ができるか&lt;/li&gt;
&lt;li&gt;技術ポートフォリオ&lt;ul&gt;
&lt;li&gt;分野 x 知識の深さ&lt;/li&gt;
&lt;li&gt;広さと深さ、両方を持つ&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;広さ&lt;ul&gt;
&lt;li&gt;広さを身につけるには、コースを取る (大学、MOOC)＋論文を読む&lt;/li&gt;
&lt;li&gt;コースとして整備されていない知識 …&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;p&gt;現在は、AI Fund の立ち上げ、Landing.ai の CEO 、そして Stanford の教授として活躍する人工知能の第一人者 Andrew Ng 氏。ACM (国際計算機学会) の生涯学習のウェビナーにて「機械学習キャリアの築き方」という内容で話をした動画が、とても良いアドバイスが満載でしたので、ここでまとめと共に紹介します。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Andrew Ng 氏" src="images/andrew-ng.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=4kiHsIaK9_w"&gt;Andrew Ng on Building a Career in Machine Learning
&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;AI は技術的ブレイクスルー&lt;ul&gt;
&lt;li&gt;多くの機会&lt;/li&gt;
&lt;li&gt;多くのキャリア機会&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;素晴らしいキャリアを築いている人は何が違うのか&lt;/li&gt;
&lt;li&gt;どうやってインパクトのある、人の役に立つ仕事ができるか&lt;/li&gt;
&lt;li&gt;技術ポートフォリオ&lt;ul&gt;
&lt;li&gt;分野 x 知識の深さ&lt;/li&gt;
&lt;li&gt;広さと深さ、両方を持つ&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;広さ&lt;ul&gt;
&lt;li&gt;広さを身につけるには、コースを取る (大学、MOOC)＋論文を読む&lt;/li&gt;
&lt;li&gt;コースとして整備されていない知識 (例: AI + 地震予測、AI + 物流 etc.)は、論文を読むしかない&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;深さ&lt;ul&gt;
&lt;li&gt;プロジェクト(インターン、会社で働く)、オープンソースで身につける&lt;/li&gt;
&lt;li&gt;履歴書に書いてある項目よりも、「実際に何ができるか」が重要&lt;/li&gt;
&lt;li&gt;スキル&lt;/li&gt;
&lt;li&gt;開発力&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;「泥臭い仕事」の重要さ&lt;ul&gt;
&lt;li&gt;一般の人が考える「AI」：「禅の心境で、ホワイトボードの前に立って人類の未来について哲学する」&lt;/li&gt;
&lt;li&gt;実際の AI：データセットを綺麗にする。学習曲線をプロットしてひたすら見る。PCA の結果について議論する&lt;/li&gt;
&lt;li&gt;多くの重要なプロジェクトには、泥臭い仕事が必要&lt;/li&gt;
&lt;li&gt;メディアが作り上げた AI のイメージのせいで、泥臭い仕事をしていると心配になる&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;生涯学習&lt;ul&gt;
&lt;li&gt;Andrew Ng 氏でも、常に研究論文を持ち歩いて読んでいる&lt;/li&gt;
&lt;li&gt;会社の CEO として働きながらも、毎週何本か研究論文を読む&lt;/li&gt;
&lt;li&gt;週末に頑張ってたくさん勉強する、のではなく、毎週少しづつでも良いので生涯に渡って学習しつづける覚悟をする&lt;/li&gt;
&lt;li&gt;AI キャリアでの成功は、履歴書の項目よりも、あなたの実際にスキルと相関がある&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;AI の「未開の機会」&lt;ul&gt;
&lt;li&gt;ソフトウェア業界の外にある&lt;/li&gt;
&lt;li&gt;農業、物流、小売、etc.&lt;/li&gt;
&lt;li&gt;GDP の成長は、比較的伝統的なセクターの成長から来る&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;質問: 学部卒業後、大学院に進学するか、民間企業で働くべきか&lt;ul&gt;
&lt;li&gt;トップレベルの修士・Ph.Dプログラムに進学するのも、民間のトップレベルの AI/ML チームに就職するのも、どちらも良い選択肢。全てに応募して、自分に一番大切なものを選ぶ&lt;/li&gt;
&lt;li&gt;修士、博士の学位は有用だが、必須ではない&lt;/li&gt;
&lt;li&gt;一緒に働く人が最も重要な要素。自分が一緒に働く10〜30人の人と、上司（もしくは指導教官）の影響が一番大きい&lt;/li&gt;
&lt;li&gt;会社や大学の「ブランド」にこだわらない。大企業は内部の差異が激しい。働くチームは非常に大切。少数精鋭の AI グループにジョインできるならOK。そうでなければ微妙&lt;/li&gt;
&lt;li&gt;例：スタンフォード大学の卒業生。「ブランド」大企業に就職後、Java の決済システムにアサインされ、その後の AI キャリアが停滞してしまう&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;質問: 一つの専門を極めるか、複数の分野に強くなるべきか&lt;ul&gt;
&lt;li&gt;皆が学際的に活躍する必要はない&lt;/li&gt;
&lt;li&gt;分野外の専門家と一緒に働く&lt;/li&gt;
&lt;li&gt;AI の仕事は楽しい (Landing.ai は農業＋AI。トラクターを運転できる！)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;質問: AI のある分野に詳しくなるのにどのぐらい時間をかければ良いか&lt;ul&gt;
&lt;li&gt;今の世界では、少し学習して、それを現実の問題に応用、それを踏まえてさらに学習、というサイクルが回しやすい&lt;/li&gt;
&lt;li&gt;MOOC の卒業生が機械学習を使って最適なコーヒーの煎り方を学習&lt;/li&gt;
&lt;li&gt;機械学習のコースを修了した後、10〜20本程度、研究論文を読んでみる。50本、100本と読むと、新しい研究アイデアが湧くようになる&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;質問: 開発者にとって５年後に重要になる領域や手法は&lt;ul&gt;
&lt;li&gt;小さいデータに対する機械学習。100万枚の画像からモデルを学習するのは誰でもできる。100枚の教師画像から学習するには、スキルが一層重要になる&lt;/li&gt;
&lt;li&gt;一般化能力。人間の放射線科医のように、古い機器、解像度の高くない X 線画像からも正しく診断する&lt;/li&gt;
&lt;li&gt;ソフトウェアエンジニアの世界→アジャイル、コードレビュー等の開発技法が進化した。機械学習分野ではまだ未発達。統計的な仕様を理解できるプロダクト・マネジャーの育成&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;質問: AI が浸透する社会においてのエンジニア、プログラミングの役割は&lt;ul&gt;
&lt;li&gt;昔は、ごく少数の人々だけが読み書きできた。識字率の向上により人々のコミュニケーションは劇的に変わった&lt;/li&gt;
&lt;li&gt;現在は、ごく少数の人々だけが多くの人に影響をあたえるプログラミングができる。将来は、コーディングが全ての人の基礎知識となる&lt;/li&gt;
&lt;li&gt;皆がコーディングできるようになれば、夫婦経営の小さな店がディスプレイをプログラミングして特売情報を表示できるようになる&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;質問: ソフトウェア以外の分野の AI の仕事を探す方法&lt;ul&gt;
&lt;li&gt;特に良いアドバイスは無い&lt;/li&gt;
&lt;li&gt;問題: AI チームやプロジェクトを立ち上げられるCEO、管理職の不足&lt;/li&gt;
&lt;li&gt;どのプロジェクトに取り組むべきか？会社の戦略とAI 戦略をどう合わせるか、が分からない人が多い&lt;/li&gt;
&lt;li&gt;2019年にローンチ予定の "AI for Everyone" コース。技術的な知識の無い人のための人工知能コースをリリース予定&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;質問: 学習リソースが乱立。どれが有用かをどう見極めるか&lt;ul&gt;
&lt;li&gt;入門の時は、コースを使って学習するのが効率的&lt;/li&gt;
&lt;li&gt;次に、検索エンジンを使って関連する論文を5本ほど列挙する&lt;/li&gt;
&lt;li&gt;その5本を表面的に読む。関係のあまり無い論文を除外する&lt;/li&gt;
&lt;li&gt;関係の深い論文を深く理解する&lt;/li&gt;
&lt;li&gt;深く理解した論文をベースに、さらに検索して（もしくは引用をたどって) 論文を見つける&lt;/li&gt;
&lt;li&gt;これを繰り返す&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;質問: 会社の上層部に AI のベネフィットを理解してもらうには&lt;ul&gt;
&lt;li&gt;"AI for Everyone" のコースを取ってもらう！&lt;/li&gt;
&lt;li&gt;今は、この情報が散在している。HBR の記事をいくつか転送するのもあり&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;質問: 仕事のあり方が変わっていく。これにどう対処するか&lt;ul&gt;
&lt;li&gt;子供が居る人、生涯学習の考え方を身に着けさせる&lt;/li&gt;
&lt;li&gt;個人・社会のレベル。無条件のベーシック・インカムには賛同しない。例えば、学習している限りもらえる条件付きベーシック・インカムを導入し、社会貢献できる可能性を上げる&lt;/li&gt;
&lt;li&gt;色々な職種でスキルを持った人材が不足している&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;質問: 最初に機械学習に興味を持ったきっかけは？&lt;ul&gt;
&lt;li&gt;Ng 氏の父親の出版した論文&lt;/li&gt;
&lt;li&gt;10代の時に、父親が、医学に機械学習を応用していた&lt;/li&gt;
&lt;li&gt;論文中に「コンピュータは人間の医師を置き換えるものではなく、助けるものだ」という一節。&lt;/li&gt;
&lt;li&gt;子供が居る人は、自分の機械学習の仕事を見せてみるのもあり&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Andrew Ng 氏の英語は、早口で聞き取りにくいところもありますが、わかない箇所等は上記メモを参照いただければと思います。また、動画中には、以下のような言い回し・イディオムがいくつか出てきますので、意味を知りたい方は調べてみましょう。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;goes without saying&lt;/li&gt;
&lt;li&gt;FOMO (feat of missing out)&lt;/li&gt;
&lt;li&gt;dirty work&lt;/li&gt;
&lt;li&gt;jack of all trades&lt;/li&gt;
&lt;li&gt;mom and pop store&lt;/li&gt;
&lt;li&gt;no strings attached&lt;/li&gt;
&lt;/ul&gt;</content></entry><entry><title>モチベーションがありすぎるあなたへ</title><link href="http://englishforhackers.com/you-probably-have-too-much-motivation.html" rel="alternate"></link><published>2018-11-11T00:00:00-05:00</published><updated>2018-11-11T00:00:00-05:00</updated><author><name>Masato Hagiwara</name></author><id>tag:englishforhackers.com,2018-11-11:/you-probably-have-too-much-motivation.html</id><summary type="html">&lt;p&gt;モチベーションはあって、良いアイデアがすぐ浮かび色々なことに挑戦するのだが、すぐに他のことに興味が移ったりして何一つとして成し遂げられない・・・。自分も含め、こんな症状で困っている方も多いのではないでしょうか。少し前ですが、ブロガーの &lt;a href="https://www.scotthyoung.com/"&gt;Scott Young 氏&lt;/a&gt;が記事を書いていたので、抄訳とともに紹介します。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;元記事: &lt;a href="https://www.scotthyoung.com/blog/2018/03/21/too-much-motivation/"&gt;モチベーションがありすぎるあなたへ&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;自己啓発の分野では、多くの人が、モチベーションが足らなくて困っている、という見方が主流だ。ただ、自分が見る限り、そのまったく逆の症状、つまり、モチベーションがありすぎて困っているという人が少なからず居るようだ。つまり、やりたいことが多すぎて、一つのプロジェクトをやり遂げることが困難であると感じる人たちである。&lt;/p&gt;
&lt;p&gt;このような人たちは、&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;アイデア、シナリオ、目標を考えるモチベーションが過剰にあり、&lt;/li&gt;
&lt;li&gt;このアイデアのために必要なことを実行するモチベーションが普通、もしくは少ない&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;という特徴を持つ。このような人に「モチベーションの上がる本」などを読ませても、全く効果がないどころか、逆効果になってしまう。&lt;/p&gt;
&lt;h3&gt;自分がモチベーション過剰か知る方法&lt;/h3&gt;
&lt;p&gt;自分がモチベーション過剰かどうか、どうやったら分かるだろうか？&lt;/p&gt;
&lt;p&gt;私もそのような性格を持つ一人なので、同じ性格を持つ人を見ると割と簡単に分かる。新しいプロジェクトや、旅行先、ライフハックなど、新しいことに対してすぐに興味を持つが、深くコミットせずに新しいプロジェクトにすぐに飛び移るので、ひどい結果しか生まれないのだ。&lt;/p&gt;
&lt;p&gt;あたながこのような性格の持ち主か知るためには …&lt;/p&gt;</summary><content type="html">&lt;p&gt;モチベーションはあって、良いアイデアがすぐ浮かび色々なことに挑戦するのだが、すぐに他のことに興味が移ったりして何一つとして成し遂げられない・・・。自分も含め、こんな症状で困っている方も多いのではないでしょうか。少し前ですが、ブロガーの &lt;a href="https://www.scotthyoung.com/"&gt;Scott Young 氏&lt;/a&gt;が記事を書いていたので、抄訳とともに紹介します。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;元記事: &lt;a href="https://www.scotthyoung.com/blog/2018/03/21/too-much-motivation/"&gt;モチベーションがありすぎるあなたへ&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;自己啓発の分野では、多くの人が、モチベーションが足らなくて困っている、という見方が主流だ。ただ、自分が見る限り、そのまったく逆の症状、つまり、モチベーションがありすぎて困っているという人が少なからず居るようだ。つまり、やりたいことが多すぎて、一つのプロジェクトをやり遂げることが困難であると感じる人たちである。&lt;/p&gt;
&lt;p&gt;このような人たちは、&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;アイデア、シナリオ、目標を考えるモチベーションが過剰にあり、&lt;/li&gt;
&lt;li&gt;このアイデアのために必要なことを実行するモチベーションが普通、もしくは少ない&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;という特徴を持つ。このような人に「モチベーションの上がる本」などを読ませても、全く効果がないどころか、逆効果になってしまう。&lt;/p&gt;
&lt;h3&gt;自分がモチベーション過剰か知る方法&lt;/h3&gt;
&lt;p&gt;自分がモチベーション過剰かどうか、どうやったら分かるだろうか？&lt;/p&gt;
&lt;p&gt;私もそのような性格を持つ一人なので、同じ性格を持つ人を見ると割と簡単に分かる。新しいプロジェクトや、旅行先、ライフハックなど、新しいことに対してすぐに興味を持つが、深くコミットせずに新しいプロジェクトにすぐに飛び移るので、ひどい結果しか生まれないのだ。&lt;/p&gt;
&lt;p&gt;あたながこのような性格の持ち主か知るためには、以下の質問をしてみると良い。あなたの人生の中で、一度は目標を立てたが失敗してしまったことについて、それが、単純に興味を失ったために失敗したのか、他にもっと優先順位の高いことをやり始めたため失敗したのか、自分に聞いてみることだ。後者が多いのなら、あなたはモチベーション過剰の可能性が高い。&lt;/p&gt;
&lt;p&gt;新しい目標にスイッチするということは、一回一回を見れば、正しいことのように見える。ただ、これがパターン化すると、自分に対して悪影響が出始める可能性がある。&lt;/p&gt;
&lt;h3&gt;モチベーション曲線&lt;/h3&gt;
&lt;p&gt;&lt;img src="https://www.scotthyoung.com/blog/wp-content/uploads/2018/03/2018-03-21-A1-768x714.jpg" width="30%" align="left"/&gt;&lt;/p&gt;
&lt;p&gt;モチベーションは、心の中ではおそらく単一の状態ではなく、複数の状態が競争している状態なのだ。モチベーションの量を曲線として描いてみると、おそらくこの図のようになる。最初はだんだん強くなり、あるレベルを超えたところで実際に行動に移す。行動を続けるとしばらくモチベーションは持続するが、そのうち普通のレベルに戻ってしまう。&lt;/p&gt;
&lt;div style="clear: left;"&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src="https://www.scotthyoung.com/blog/wp-content/uploads/2018/03/2018-03-21-A2-768x780.jpg" width="30%" align="left"/&gt;&lt;/p&gt;
&lt;p&gt;運が良ければ、プロジェクトを続けるためのモチベーションレベルをもっと低いところに持ってくることができるかもしれない。&lt;/p&gt;
&lt;div style="clear: left;"&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src="https://www.scotthyoung.com/blog/wp-content/uploads/2018/03/2018-03-21-A3-768x617.jpg" width="30%" align="left"/&gt;&lt;/p&gt;
&lt;p&gt;たまには、モチベーションを一時的に上げることができるかもしれない。長期的なプロジェクトではこれはほぼ必須だと言える。ただ、これは安定しておらず、やはりプロジェクトを続けるためのモチベーションレベルを下げるという点の方が重要だ。&lt;/p&gt;
&lt;p&gt;私はこれまで、習慣化、目標設定、プロジェクト計画などについて色々と書いてきたが、基本的なアイデアは、プロジェクトの最初にはモチベーションが過剰なのを利用して、プロジェクトを長期的に続けるために必要な行動が、モチベーションレベルよりも低いところにあるように、計画的にプロジェクトを設計するというものだ。&lt;/p&gt;
&lt;div style="clear: left;"&gt;&lt;/div&gt;

&lt;h3&gt;複数プロジェクトの場合&lt;/h3&gt;
&lt;p&gt;上のグラフは、プロジェクトや目標が一つしかない場合だ。複数のプロジェクトや目標に対するモチベーションがあるとき、事態はもっと複雑になる。新しいプロジェクトが、古いプロジェクトのモチベーションを奪うためだ。これによって、古いプロジェクトのモチベーションは急激に下がってしまうことになる。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://www.scotthyoung.com/blog/wp-content/uploads/2018/03/2018-03-21-A4-768x645.jpg" width="30%" align="left" /&gt;&lt;/p&gt;
&lt;p&gt;あなたがこのような人なら、曲線の形は同じだが、同じ時間・努力・情熱に対して競合関係にある複数のモチベーションがあることになる。こうすると、新しいアイデアを求め、モチベーション的には中立的にある「必要な行動をする」ということをないがしろにする、という終わりのないサイクルに陥ってしまう。&lt;/p&gt;
&lt;div style="clear: left;"&gt;&lt;/div&gt;

&lt;h3&gt;モチベーション過剰にどう対処するか&lt;/h3&gt;
&lt;p&gt;私にとって、このような傾向に対処するために最も効果的だった方法は、&lt;strong&gt;単一プロジェクト法&lt;/strong&gt;と呼ぶものだ。これは、同時に実行するのを、優先度が最高の一つのプロジェクトだけに制限するという方法である。そのプロジェクトが完了するまで、他のことは全て二の次、メインのプロジェクトを置き換えることは絶対にしないというものだ。&lt;/p&gt;
&lt;p&gt;実際には、仕事での目標、プライベートの趣味の目標など、人生には複数の、通常は競合関係にない目標がある場合がある。このようなプロジェクト間の区別は曖昧になりがちなので、実際には簡単ではない場合もある。&lt;/p&gt;
&lt;p&gt;ただし、このような場合を考えても、単一プロジェクト法はまず試すには良い方法であると言える。メインのプロジェクトが一つしか無いと想定して計画を立て、この方法を使い新しいプロジェクトに浮気してしまう傾向を防いでしまおう。&lt;/p&gt;
&lt;p&gt;もう一つ、モチベーション過剰に有効だと思う方法は、&lt;strong&gt;プロジェクトを実際に始めることなく計画する&lt;/strong&gt;ことだ。新しいアイデアを思いついたら、それを忘れたくないと思うのは自然なので、時間やエネルギーを向ける前に、今のメインのプロジェクトが終わったら、それをどう実行するかを計画するだけにしておくといういうものだ。現実にどんな行動をしなければならないかを考えるだけで、移りたい衝動を緩和できる可能性が高い。&lt;/p&gt;
&lt;p&gt;最後に、&lt;strong&gt;短期的な、目標の低いプロジェクトを設定する&lt;/strong&gt;というのも、実際にはほとんど使われていないが有効な方法だ。新しいプロジェクトに急に興味を持つ傾向が自分にあると分かったら、何年もかかるプロジェクトではなく、例えば一ヶ月で完成できるプロジェクトを選ぶというものだ。&lt;/p&gt;
&lt;p&gt;小さいプロジェクトでは目標が低すぎる、と思うかもしれない。だが、人生の色々な目標は１，２ヶ月興味を続ければ十分効果のあるものが多い。例えば、１か月間だけ、運動するという目標に対して頑張ったなら、その後は習慣を続けて他の目標にシフトしても良い。&lt;/p&gt;
&lt;p&gt;この「小さいプロジェクト法」と、モチベーション過剰な人との違いは、はじめから短期目標であると分かっているので、それに合わせて計画できるという点だ。一時的な感情でプロジェクトに失敗してしまうのと違って、例えば運動の目標が１ヶ月だけだと分かっていれば、それが終わった後も、バックグラウンドの行動として継続できるように計画できるかもしれない。&lt;/p&gt;
&lt;p&gt;最後に。モチベーション過剰というこの欠点も、見方を変えれば長所でもある。何かを達成するやる気があるということだ。ただし、人生を豊かにすると自分が信じている衝動をコントロールするという、直感に反することが必要になる。もしこれを実際に実行できれば、モチベーション過剰に任せて新しいアイデアを常に追い求めて、実際にやり遂げるということをしないという状態に比べて、あなたの未来はずっと明るくなるだろう。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;いかがでしょうか？私自身も、自分のこの「モチベーション過剰」にずっと悩んでいたので、この Scott Young 氏の記事には非常に共感するところが多かったです。&lt;/p&gt;
&lt;p&gt;個人的には、Young 氏のアドバイスに加えて、自分にとって以下の方法が非常に効果的だったと思います。&lt;/p&gt;
&lt;p&gt;一つは、Young 氏の「短期的な、目標の低いプロジェクトを設定する」というアドバイスに似ていますが、目標を極端に低いところから始め、だんだんと高くしていく、という方法です。始めは、１週間で終わるような、絶対に達成できるだろうという目標から始め、それが達成できたら、次は２週間継続する、というように、少しずつ高く設定していくというものです。目標設定は、筋トレと似ていて、実際にやり遂げることによってやり遂げるための能力が少しずつ高まります。それを利用するというものです。&lt;/p&gt;
&lt;p&gt;もう一つは、他の人との約束を利用するというものです。人間は社会的な生き物ですので、人の前で恥をかいたり、人との約束を破ったりすることを極端に避ける傾向があります。大きなプロジェクトを達成したい時には、締切や、契約、昇進・昇給など、社会的な約束を結び付けられるとうまく行く傾向があります。例えば、自分がよく使う例が、何かを勉強したいと思った時には、社内勉強会で勉強成果を発表する約束を先にしてしまうという方法です。発表する段階になって準備不足では恥をかくので、準備中は必然とそのプロジェクトの優先度が高くなり、達成できる可能性を上げることができます。&lt;/p&gt;</content></entry><entry><title>強化学習入門 - Google DeepMind の David Silver 氏による強化学習コース</title><link href="http://englishforhackers.com/david-silver-reinforcement-learning.html" rel="alternate"></link><published>2018-09-01T00:00:00-04:00</published><updated>2018-09-01T00:00:00-04:00</updated><author><name>Masato Hagiwara</name></author><id>tag:englishforhackers.com,2018-09-01:/david-silver-reinforcement-learning.html</id><summary type="html">&lt;script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async&gt;&lt;/script&gt;

&lt;p&gt;「無料でアクセスできる最高の強化学習のコース」と名高い、Google DeepMind / University College London の David Silver 氏による強化学習のコース。&lt;a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html"&gt;こちらのページから、全ての講義スライドと講義ビデオが見られる&lt;/a&gt;。&lt;/p&gt;
&lt;h2&gt;講義1: 強化学習入門&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;教科書&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;An Introduction to Reinforcement Learning&lt;ul&gt;
&lt;li&gt;直感的, このコースで参照&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Algorithms for Reinforcement Learning&lt;ul&gt;
&lt;li&gt;理論, 厳密&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;強化学習とは&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;様々な分野と関係&lt;/li&gt;
&lt;li&gt;工学、機械学習、神経科学（脳の報酬システムと関係）&lt;/li&gt;
&lt;li&gt;機械学習の３つの分類&lt;ul&gt;
&lt;li&gt;教師あり学習、教師なし学習、強化学習&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;他の機械学習アルゴリズムとの違い&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;教師の代わりに、報酬信号しかない&lt;/li&gt;
&lt;li&gt;報酬がすぐに得られるとは限らない&lt;/li&gt;
&lt;li&gt;時間の概念が重要。iid (独立同分布)データではない&lt;/li&gt;
&lt;li&gt;エージェントが環境に影響を及ぼす→データも変わる&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;強化学習の例 …&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async&gt;&lt;/script&gt;

&lt;p&gt;「無料でアクセスできる最高の強化学習のコース」と名高い、Google DeepMind / University College London の David Silver 氏による強化学習のコース。&lt;a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html"&gt;こちらのページから、全ての講義スライドと講義ビデオが見られる&lt;/a&gt;。&lt;/p&gt;
&lt;h2&gt;講義1: 強化学習入門&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;教科書&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;An Introduction to Reinforcement Learning&lt;ul&gt;
&lt;li&gt;直感的, このコースで参照&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Algorithms for Reinforcement Learning&lt;ul&gt;
&lt;li&gt;理論, 厳密&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;強化学習とは&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;様々な分野と関係&lt;/li&gt;
&lt;li&gt;工学、機械学習、神経科学（脳の報酬システムと関係）&lt;/li&gt;
&lt;li&gt;機械学習の３つの分類&lt;ul&gt;
&lt;li&gt;教師あり学習、教師なし学習、強化学習&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;他の機械学習アルゴリズムとの違い&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;教師の代わりに、報酬信号しかない&lt;/li&gt;
&lt;li&gt;報酬がすぐに得られるとは限らない&lt;/li&gt;
&lt;li&gt;時間の概念が重要。iid (独立同分布)データではない&lt;/li&gt;
&lt;li&gt;エージェントが環境に影響を及ぼす→データも変わる&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;強化学習の例&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ヘリコプターの曲芸を学習&lt;/li&gt;
&lt;li&gt;バックギャモンで世界チャンピオンに勝つ&lt;/li&gt;
&lt;li&gt;投資ポートフォリオの管理&lt;/li&gt;
&lt;li&gt;発電所の制御&lt;/li&gt;
&lt;li&gt;人間型ロボットを歩かせる&lt;/li&gt;
&lt;li&gt;Atari の複数のゲームをプレイする&lt;/li&gt;
&lt;li&gt;Q：強化学習アルゴリズムは、人間の反応時間に比べて速く操作ができるので有利ではないか？ → A: 人間の反応時間に合わせてあるので、公平なはず&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;強化学習問題&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;報酬 \( R_t \) --&amp;gt; スカラー値のフィードバック信号。時刻 t においてどのぐらい「うまく行っているか」&lt;/li&gt;
&lt;li&gt;報酬の合計の期待値を最大化させるのが目的&lt;/li&gt;
&lt;li&gt;報酬に関する仮定：全てのゴールは、累積報酬の期待値を最大化させる問題に帰着できる&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;継続的な意思決定&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;目的：将来の報酬の合計を最大化させる行動を選択する&lt;/li&gt;
&lt;li&gt;貪欲的に行動するべきではない → 行動が長期にわかって効果を残す。報酬がすぐ得られるとは限らない。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;エージェントと環境&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;エージェントが環境を観察 \( O_t \)&lt;/li&gt;
&lt;li&gt;行動 \( A_t \)&lt;/li&gt;
&lt;li&gt;報酬 \( R_t \)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;履歴と状態&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;履歴 \( H_t = A_1, O_1, R_1, ..., A_t, O_t, R_t \)&lt;/li&gt;
&lt;li&gt;状態 \( S_t = f(H_t) \)&lt;/li&gt;
&lt;li&gt;環境状態 \( S^e_t \) → エージェントからは見えない&lt;/li&gt;
&lt;li&gt;エージェント状態 \( S^a_t \) &lt;/li&gt;
&lt;li&gt;マルコフ性: \( P[S_{t+1} | S_t] = P[S_{t+1} | S_1, ..., S_t] \)&lt;ul&gt;
&lt;li&gt;次の状態は、現在の状態だけに依存する&lt;/li&gt;
&lt;li&gt;現在の状態が分かれば、履歴は不要&lt;/li&gt;
&lt;li&gt;状態は、未来の十分統計量&lt;/li&gt;
&lt;li&gt;ヘリコプターの例：現在の位置、速度、角度、各速度 etc.  位置だけではマルコフ性が成立しない&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;完全に観察可能な環境&lt;ul&gt;
&lt;li&gt;\( O_t = S^a_t = S^e_t \) → マルコフ決定過程 (Markov Decision Process; MDP)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;部分的に観察可能な環境&lt;ul&gt;
&lt;li&gt;部分観測マルコフ決定過程 (Partially Observable MDP; POMDP)&lt;/li&gt;
&lt;li&gt;エージェント状態を、環境状態とは独立に構築する必要がある&lt;ul&gt;
&lt;li&gt;方法1: 状態に対する信念（確率分布）を維持する&lt;/li&gt;
&lt;li&gt;方法2: 前の状態と、現在の観察から、次の状態を予測する RNN を構築する&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;エージェント&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;方策: エージェントがどのように意思決定するか&lt;ul&gt;
&lt;li&gt;状態 s から行動 a への関数&lt;/li&gt;
&lt;li&gt;決定的な方策: \( a = \pi(s) \)&lt;/li&gt;
&lt;li&gt;確率的な方策: \( \pi(a | s) = P[A = a | S = s]\)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;価値関数: それぞれの状態/行動がどのぐらい良いか&lt;ul&gt;
&lt;li&gt;将来の報酬に対する予測&lt;/li&gt;
&lt;li&gt;方策に依存&lt;/li&gt;
&lt;li&gt;\( v_\pi(s) = E_\pi[R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + ... | S_t = s] \)&lt;/li&gt;
&lt;li&gt;時間による割引 \( \gamma \) → 遠い未来より近い未来の報酬を優先&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;モデル: エージェントによる環境の表現（「エージェントが環境がどういう仕組みで動いていると思っているか」）&lt;ul&gt;
&lt;li&gt;遷移モデル: 次の状態を予想する&lt;/li&gt;
&lt;li&gt;報酬モデル: 次の報酬を予想する&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;エージェントの分類&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;価値ベース: 価値関数を使う&lt;/li&gt;
&lt;li&gt;方策ベース: 方策を使う&lt;/li&gt;
&lt;li&gt;Actor Critic: 価値関数と方策の両方を使う&lt;/li&gt;
&lt;li&gt;モデル無し: 価値関数・方策のどちらかもしくは両方、モデル無し&lt;/li&gt;
&lt;li&gt;モデル有り: 価値関数・方策のどちらかもしくは両方、モデル有り&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;学習とプランニング&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;強化学習: 環境が未知の状態からスタート、エージェントが環境と相互作用し、方策を改善する&lt;/li&gt;
&lt;li&gt;プランニング: 環境のモデルが与えられる、相互作用せずにモデルを使って計算、方策を改善する&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;探索と搾取&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;探索: 報酬をあきらめてでも、環境に関する情報を得る&lt;/li&gt;
&lt;li&gt;搾取: 既に知っている情報を使い、報酬を最大化する&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;講義2: マルコフ決定過程&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;マルコフ決定過程 (MDP)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;環境が完全に観察可能&lt;/li&gt;
&lt;li&gt;状態が、過程を完全に規定する&lt;/li&gt;
&lt;li&gt;多くの強化学習問題が、MDP として定式化可能&lt;/li&gt;
&lt;li&gt;部分観測マルコフ決定過程 (POMDP) も、MDP に変換可能&lt;/li&gt;
&lt;li&gt;バンディットアルゴリズムも、状態が一つしかない MDP&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;マルコフ性&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;次に何が起こるかは、今の状態だけに依存&lt;/li&gt;
&lt;li&gt;Lecture 1 参照&lt;/li&gt;
&lt;li&gt;状態遷移確率 \( P_{ss'} = P[S_{t+1} = s' | S_t = s] \) 行列で表現可能&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;マルコフ過程&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;状態列 \( S_1, S_2, ... \) がマルコフ性を満たすとき → マルコフ過程&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;マルコフ報酬過程 (Markov Reward Process; MRP)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;R: 報酬関数 \( R_s = E[ R_{t+1} | S_t = s] \)&lt;/li&gt;
&lt;li&gt;利得 \( G_t = \sum_{k=0}^\infty \gamma^k R_{t+k+1} \)&lt;/li&gt;
&lt;li&gt;なぜ割引率 \( \gamma \) を使うか → 数学的に便利。報酬が発散するのを防ぐ。未来に行くほど不確定。直近の未来の報酬を優先。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;価値関数&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;状態 s に居るときの利得の期待値 \( v(s) = E[G_t | S_t = s] \)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ベルマン方程式&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;\( v(s) = E[R_{t+1} + \gamma v(S_{t+1}) | S_t = s] \)&lt;/li&gt;
&lt;li&gt;価値観数は、1) すぐ次の報酬、2) 次の状態の価値（＋割り引き）の２つに分解できる&lt;/li&gt;
&lt;li&gt;行列表現: \( v = R + \gamma Pv \)&lt;ul&gt;
&lt;li&gt;v: \( v = (v(1), ..., v(n))^T \)&lt;/li&gt;
&lt;li&gt;R: \( R = (R(1), ..., R(n))^T \)&lt;/li&gt;
&lt;li&gt;P: 状態 i から j への遷移確率行列&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;解析的に解ける: \( v = (I - \gamma P)^{-1}R \)&lt;ul&gt;
&lt;li&gt;小さい MDP にしか適用できない。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;マルコフ決定過程&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;マルコフ報酬過程 + 行動&lt;/li&gt;
&lt;li&gt;報酬 R: \( R^a_s = E[R_{t+1} | S_t = s, A_t = a] \)&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;方策&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;\( \pi(a | s) = P[A_t = a | S_t = s] \) → エージェントの振る舞いを完全に規定&lt;ul&gt;
&lt;li&gt;時間 \( t \) に依存しない&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;MDP と方策 \( \pi \) が与えられたとき、状態系列 \( S_1, S_2, ... \) はマルコフ過程 → マルコフ決定過程を「平ら」にする&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;価値関数&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;状態価値関数は、方策 \( \pi \) に依存： \( v_\pi(s) = E_\pi[G_t | S_t = s] \)&lt;/li&gt;
&lt;li&gt;行動価値関数: \( q(s, a) = E[G_t | S_t = s, A_t = a] \)&lt;/li&gt;
&lt;li&gt;ベルマン方程式を使って、直近の報酬と次の状態の価値に分解できる&lt;ul&gt;
&lt;li&gt;状態価値関数: \( v_\pi(s) = \sum_{a \in A} \pi(a|s) q_\pi(a, s) \)&lt;/li&gt;
&lt;li&gt;行動価値関数: \( q(s, a) = R^a_s + \gamma \sum_{s' \in S}P^a_{ss'} v_\pi(s') \)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ベルマン方程式の再帰適用: \( v_\pi(s) = \sum_{a \in A} \pi(a|s)( R^a_s + \gamma \sum_{s' \in S} P^a_{ss'} v_\pi(s') ) \)&lt;/li&gt;
&lt;li&gt;\( q_\pi \) にも同じことができる&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;最適状態価値関数&lt;ul&gt;
&lt;li&gt;全ての方策の中で、価値関数が最大となるもの: \( v_*(s) = \max_\pi v_\pi(s) \)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;最適行動価値関数&lt;ul&gt;
&lt;li&gt;\( q_{*}(s, a) = \max_\pi q_\pi(s, a) \) → これがあれば、MDP は「解けた」（各状態において、どう行動すべきかが分かる）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;最適方策&lt;ul&gt;
&lt;li&gt;ある方策が他の方策より良いとは？ \( \pi \ge \pi' if v_\pi(s) \ge v_\pi'(s), \forall s \)&lt;/li&gt;
&lt;li&gt;定理: 他のあらゆる方策よりも良い最適方策 \( \pi_* \) が存在する。複数存在する場合もある&lt;/li&gt;
&lt;li&gt;\( q_*(s, a) \) を最大化する行動を取ることで、最適方策が得られる&lt;/li&gt;
&lt;li&gt;\( v_* \) と \( q_* \) についても、上記のベルマン方程式が適用できる&lt;ul&gt;
&lt;li&gt;ただし、\( \sum_{a \in A} \) は \( \max_a \) に置き換わる&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;非線形 (max が入る) →　閉じた形での解は存在しない&lt;/li&gt;
&lt;li&gt;繰り返し&lt;ul&gt;
&lt;li&gt;価値反復 (Value Iteration)&lt;/li&gt;
&lt;li&gt;方策反復 (Policy Iteration)&lt;/li&gt;
&lt;li&gt;Q 学習&lt;/li&gt;
&lt;li&gt;Sarsa &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;MDP の拡張&lt;/li&gt;
&lt;li&gt;無限 / 連続 MDP&lt;/li&gt;
&lt;li&gt;部分観測マルコフ決定過程 (POMDP)&lt;/li&gt;
&lt;li&gt;割り引きの無い, 平均報酬 MDP&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;講義3: 動的計画法による計画&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;はじめに&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;動的計画法&lt;ul&gt;
&lt;li&gt;「動的」: 逐次的、時間&lt;/li&gt;
&lt;li&gt;「計画」≒ 方策&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;動的計画法がいつ使えるか&lt;ul&gt;
&lt;li&gt;最適なサブ構造に分解し、そこから最適解が求められる場合&lt;ul&gt;
&lt;li&gt;例: グラフの最短経路問題&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;サブ問題がお互いに関係しており、何回も現れる場合 → キャッシュできる&lt;/li&gt;
&lt;li&gt;MDPはこの両方を満たす&lt;ul&gt;
&lt;li&gt;ベルマン方程式&lt;/li&gt;
&lt;li&gt;問題の再帰的な分解&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;例&lt;ul&gt;
&lt;li&gt;スケジュール&lt;/li&gt;
&lt;li&gt;文字列アルゴリズム&lt;/li&gt;
&lt;li&gt;グラフアルゴリズム&lt;/li&gt;
&lt;li&gt;グラフィカルアルゴリズム&lt;/li&gt;
&lt;li&gt;生物情報学&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;動的計画法を使った計画&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;MDP の情報が全て分かっている前提&lt;/li&gt;
&lt;li&gt;予測: MDP と方策 \( \pi \) が分かっている時に、価値関数 \( v_\pi \) を求める&lt;/li&gt;
&lt;li&gt;操作: MDP が分かっている時に、最適方策 \( \pi_* \) を求める&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;方策反復&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;MDP を解くための仕組みの一つ&lt;/li&gt;
&lt;li&gt;方策 \( \pi \) を評価する&lt;/li&gt;
&lt;li&gt;ベルマン方程式を逆向きに繰り返し適用&lt;/li&gt;
&lt;li&gt;任意の \( v_1 \) からスタート。ベルマン方程式を適用し、\( v_2 \) を得る。&lt;/li&gt;
&lt;li&gt;\( v_{k+1}(s) \) を計算するためには&lt;ul&gt;
&lt;li&gt;1ステップ先読みする。\( v_{k+1}(s) = \sum_{a \in A} \pi(a|s)( R^a_s + \gamma \sum_{s' \in S} P^a_{ss'} v_k(s') ) \)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;これを繰り返すと、\( v_* \) に収束する&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;方策をどう改善するか&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;方策 \( \pi \) が与えられた時、&lt;ul&gt;
&lt;li&gt;まず、方策 \( \pi \) を評価し、\( v_\pi(s) \) を得る&lt;/li&gt;
&lt;li&gt;\( v_\pi(s) \) に従い、貪欲に行動し、方策 \( \pi' \) を得る&lt;/li&gt;
&lt;li&gt;「格子世界」の例では、\( \pi' \) が最適方策 \( \pi_* \)&lt;/li&gt;
&lt;li&gt;一般的には、これを繰り返すと、最適方策に収束する&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;決定的な方策 \( \pi \) からスタート&lt;ul&gt;
&lt;li&gt;貪欲に行動することで、この方策を改善できる。 \( \pi'(s) = \arg\max_{a \in A} q_\pi(s, a) \)&lt;/li&gt;
&lt;li&gt;→ 価値関数も改善する&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;この繰り返し的な改善がストップする時 → ベルマン最適方程式を満たす → 方策は最適である \( v_\pi(s) = v_*(s) \)&lt;/li&gt;
&lt;li&gt;方策評価が収束するまで繰り返す必要があるか？ \( k \) 回繰り返せば十分。ただし \( k = 1 \) ではだめ。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;価値反復&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;MDP を解くためのもう一つの仕組み&lt;/li&gt;
&lt;li&gt;最適原則&lt;ul&gt;
&lt;li&gt;方策 \( \pi(a|s) \) は、以下の条件を満たす時、またその時に限って、最適価値関数 \( v_\pi(s) = v_*(s) \) を満たす。&lt;ul&gt;
&lt;li&gt;任意の状態 \( s' \) が \( s \) から到達可能&lt;/li&gt;
&lt;li&gt;状態 \( s' \) が、最適価値関数を満たす \( v_\pi(s') = v_*(s') \)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;もし、部分問題に対して最適解が分かっている時、\( v_*(s') \)&lt;ul&gt;
&lt;li&gt;１ステップ先読みする： \( v_*(s) \leftarrow \max_{a \in A} R^a_s + \gamma \sum_{s' \in S} P^a_{ss'}v_*(s') \)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;最適方策 \( \pi \) を探す&lt;ul&gt;
&lt;li&gt;\( v_1 \to v_2 \to ... \to v_* \)&lt;/li&gt;
&lt;li&gt;\( v_{k+1}(s) \) から \( v_k(s') \) を更新&lt;/li&gt;
&lt;li&gt;方策反復とは違い、方策を明示的に使わない&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;まとめ&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;予測 → ベルマン期待値方程式 - 反復方策評価&lt;/li&gt;
&lt;li&gt;操作 → ベルマン期待値方程式+貪欲的方策更新 - 方策反復&lt;/li&gt;
&lt;li&gt;操作 → ベルマン最適方程式 - 価値反復&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;拡張&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;非同期動的計画法&lt;ul&gt;
&lt;li&gt;他の状態の更新が終わるまで待たない。最適値に収束する&lt;/li&gt;
&lt;li&gt;In-place 動的計画法　&lt;ul&gt;
&lt;li&gt;価値関数の表を直接書き換える&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;優先度付き sweeping&lt;ul&gt;
&lt;li&gt;どの状態を次に更新するか優先度をつける&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;リアルタイム 動的計画法&lt;ul&gt;
&lt;li&gt;エージェントに関係のある状態だけ更新する&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ベルマン方程式は、状態数が多い時に効率が悪い&lt;ul&gt;
&lt;li&gt;サンプリング&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;講義4: モデルフリー予測&lt;/h2&gt;
&lt;p&gt;モデルフリー予測 = 未知の MDP の価値関数を推定する&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;モンテカルロ (MC) 学習&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;経験のエピソードから直接学習する&lt;/li&gt;
&lt;li&gt;エピソードが終了する必要あり&lt;/li&gt;
&lt;li&gt;方策 \( \pi \) の下で、経験のエピソード \( S_1, A_1, R_2, ..., S_k \sim \pi \) から \( v_\pi \) を学習&lt;/li&gt;
&lt;li&gt;復習： 利得 \( G_t = R_{t+1} + \gamma R_{t+2} + ... + \gamma^{T-1}R_T \)&lt;/li&gt;
&lt;li&gt;復習： 価値関数 \( v_\pi(s) = E_\pi[G_t | S_t = s] \)&lt;/li&gt;
&lt;li&gt;モンテカルロ方策評価：各エピソードについて、状態 \( s \) を最初に訪問した時に&lt;ul&gt;
&lt;li&gt;カウンターと利得の合計を更新。&lt;/li&gt;
&lt;li&gt;\( V(s) \) → 利得の合計 / 訪問回数&lt;/li&gt;
&lt;li&gt;十分多くの \( N(s) \) を観察すると、\( V(s) \) は \( v_\pi(s) \) に収束&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;各訪問ごとに更新する場合&lt;ul&gt;
&lt;li&gt;訪問回数を、各訪問ごとに更新&lt;/li&gt;
&lt;li&gt;これでも、\( V(s) \to v_\pi(s) \)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;「差分」を使った系列の平均計算&lt;ul&gt;
&lt;li&gt;系列 \( x_1, x_2, ... \) を全て観測し終わらなくても、それまでの平均 \( \mu_k \) を計算することができる&lt;/li&gt;
&lt;li&gt;\( \mu_k = \mu_{k-1} + \frac{1}{k} (x_k - \mu_{k-1}) \) &lt;/li&gt;
&lt;li&gt;直感的な説明： 新しい値を観測した時、それまでの推定値と大きくことなる場合は、推定値を大きく更新する。&lt;/li&gt;
&lt;li&gt;\( \mu_{k-1} \) →.それまでの推定値。\( x_k - \mu_{k-1} \) → 新しい値を観測した時の「驚き」。\( \frac{1}{k} \) → 学習率&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;差分モンテカルロ更新&lt;ul&gt;
&lt;li&gt;エピソード毎に更新（全てのエピソードの「和」を保持しない）&lt;/li&gt;
&lt;li&gt;\( N(S_t) \leftarrow N(S_t) + 1 \) &lt;/li&gt;
&lt;li&gt;\( V(S_t) \leftarrow V(S_t) + \frac{1}{N(S_t)}(G_t - V(S_t)) \)&lt;/li&gt;
&lt;li&gt;注：エピソードが終わるまで待つ必要あり&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;時間差分 (Temporal Difference) 学習&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;経験のエピソードから直接学習する (モンテカルロ法と同じ)&lt;/li&gt;
&lt;li&gt;エピソードが終了してなくても良い → ブートストラップ法 (モンテカルロ法との差異)&lt;/li&gt;
&lt;li&gt;MC と TD の違い&lt;ul&gt;
&lt;li&gt;MC: 実際の利得 \( G_t \) を使って更新: \( V(S_t) \leftarrow V(S_t) + \alpha(G_t - V(S_t)) \)&lt;/li&gt;
&lt;li&gt;TD: 利得の予測値を使って更新: \( V(S_t) \leftarrow V(S_t) + \alpha(R_{t+1} + \gamma V(S_{t+1}) - V(S_t)) \)&lt;ul&gt;
&lt;li&gt;\( R_{t+1} + \gamma V(S_{t+1}) \) は TD ターゲットと呼ばれる&lt;/li&gt;
&lt;li&gt;\( \delta_t = R_{t+1} + \gamma V(S_{t+1}) - V(S_t) \) は TD 誤差と呼ばれる&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;TD の長所・欠点&lt;ul&gt;
&lt;li&gt;最終結果を見る前に学習できる。最終結果の無いエピソードでも学習できる&lt;/li&gt;
&lt;li&gt;偏り (Bias) と分散　(Variance) のトレードオフ&lt;ul&gt;
&lt;li&gt;真の TD ターゲット \( R_{t+1} + \gamma v_\pi(S_{t+1}) \) は \( v_\pi(S_t) \)の 不偏推定量&lt;/li&gt;
&lt;li&gt;TD ターケット \( R_{t+1} + \gamma V(S_{t+1}) \) は、偏りのある推定量　&lt;/li&gt;
&lt;li&gt;ただし、TD ターゲットは、利得よりも分散が小さい&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;MC は分散大、偏りゼロ。TD は分散小、偏りあり。&lt;/li&gt;
&lt;li&gt;TD(0) は \( v_\pi(s) \) に収束。初期値に敏感&lt;/li&gt;
&lt;li&gt;MC は、観察された利得との平均二乗誤差を最小化する解に収束する&lt;/li&gt;
&lt;li&gt;TD は、データに対して尤度最大の MDPを学習し、それを解く&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ブートストラップとサンプリング&lt;ul&gt;
&lt;li&gt;MC → ブートストラップ無し&lt;/li&gt;
&lt;li&gt;DP → ブートストラップ有り&lt;/li&gt;
&lt;li&gt;TD → ブートストラップ有り&lt;/li&gt;
&lt;li&gt;MC → サンプリング有り&lt;/li&gt;
&lt;li&gt;DP → サンプリング無し&lt;/li&gt;
&lt;li&gt;TD → サンプリング無し&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;TD(λ)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;TD ターゲットを計算するときに、nステップ先読みする&lt;ul&gt;
&lt;li&gt;例：\( G_t^{(2)}  = R_{t+1} + \gamma R_{t+2} + \gamma^2 V(S_{t+2}) \)&lt;/li&gt;
&lt;li&gt;n を大きくすると、MC 法に収束する&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;nステップ先読みを平均する&lt;ul&gt;
&lt;li&gt;例：\( \frac{1}{2} G_t^{(2)} + \frac{1}{2} G_t^{(4)} \)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;γ利得 → 全てのnステップ利得の幾何平均&lt;ul&gt;
&lt;li&gt;\( G_t^\lambda = (1-\lambda) \sum_{n=1}^\infty \lambda^{n-1} G_t^n \)&lt;/li&gt;
&lt;li&gt;なぜ幾何平均? 前の値を保持しなくて良いので、計算コストが低い。TD(0) と同じコストで計算できる。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;TD(λ) の「前向き」観点&lt;ul&gt;
&lt;li&gt;\( V(S_t) \leftarrow V(S_t) + \alpha (G^\lambda_t - V(S_t)) \)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;MC と同じように、エピソードが収束するまで待つ必要がある&lt;/li&gt;
&lt;li&gt;TD(λ) の「後ろ向き」観点&lt;ul&gt;
&lt;li&gt;Eligibility Trace&lt;ul&gt;
&lt;li&gt;最近性と、頻度を両方考慮する量&lt;/li&gt;
&lt;li&gt;\( E_0(s) = 0, E_t(s) = \gamma E_{t-1}(s) + {\mathbf 1}(S_t = s) \)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;\( V(s) \leftarrow V(s) + \alpha \delta_t E_t(s) \)&lt;/li&gt;
&lt;li&gt;\( \lambda = 0 \) の時 → TD(0) と等価&lt;/li&gt;
&lt;li&gt;\( \lambda = 1 \) の時 → 更新の合計は　MC と同じ&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;講義5: モデルフリー制御&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;はじめに&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;非常に多くの問題が、MDP としてモデル化できる&lt;/li&gt;
&lt;li&gt;On-policy (方策オン型)&lt;ul&gt;
&lt;li&gt;「行動しながら学ぶ」&lt;/li&gt;
&lt;li&gt;学習している方策と、サンプルを生成する方策が同じ&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Off-policy (方策オフ型)&lt;ul&gt;
&lt;li&gt;「他の人の行動から学ぶ」&lt;/li&gt;
&lt;li&gt;学習している方策と、サンプルを生成する方策が違う&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;方策オン型　MC 制御&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;復習: 方策反復：1) 方策評価 \( v_\pi \) の推定 と、2) 方策改善 (貪欲的方策改善) を繰り返す&lt;/li&gt;
&lt;li&gt;ここに、MC 法による方策評価を組み込むことはできるか？&lt;ul&gt;
&lt;li&gt;問題点：\( V(s) \) に従って貪欲に行動しようとしても、MDP の完全な情報が必要 → 解法: 代わりに \( Q(s, a) \) を使う&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;\( Q = q_\pi \) を MC で評価する&lt;ul&gt;
&lt;li&gt;問題点：探索問題。貪欲的に行動すると、必要な状態に到達することができない&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;探索&lt;ul&gt;
&lt;li&gt;ε-貪欲探索&lt;ul&gt;
&lt;li&gt;確率εでランダムな行動を（一様分布に従って）取る&lt;/li&gt;
&lt;li&gt;ε-貪欲探索に従う新しい方策 \( \pi' \) は、前の方策 \( \pi \) よりも良い&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;MC で方策を評価し、ε-貪欲探索をすると？&lt;ul&gt;
&lt;li&gt;最適方策 \( \pi_* \) に到達する。どのぐらいかかるか分からない&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;モンテカルロ制御&lt;ul&gt;
&lt;li&gt;エピソードの完了 → Q を更新&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;GLIE (Greedy in the Limit with Infinite Exploration) &lt;ul&gt;
&lt;li&gt;全ての状態-行動ペアを、無限回探索する　&lt;/li&gt;
&lt;li&gt;方策が貪欲方策に収束する&lt;/li&gt;
&lt;li&gt;GLIE モンテカルロ制御&lt;ul&gt;
&lt;li&gt;各状態-行動ペアについて、\( G_t \) の平均を \( Q(S_t, A_t) \) として保持&lt;/li&gt;
&lt;li&gt;新しいQ値に従い、ε-貪欲方策を更新&lt;/li&gt;
&lt;li&gt;最適な行動価値関数 \( q_*(s, a) \) に収束&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;方策オン型 TD 学習&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;アイデア：MC の代わりに TD を制御ループの時に使う&lt;ul&gt;
&lt;li&gt;TD を \( Q(S, A) \) の推定に使う&lt;/li&gt;
&lt;li&gt;Sarsa: なぜ Sarsa? (S, A) → R → S' → A'&lt;/li&gt;
&lt;li&gt;更新式: \( Q(S, A) \leftarrow Q(S, A) + \alpha( R + \gamma Q(S', A') - Q(S, A) ) \)&lt;/li&gt;
&lt;li&gt;方策改善には、ε貪欲探索を使う&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Sarsa は、\( Q(s, a) \to q_*(s, a) \) に収束する (条件付きだが、ほとんどの場合成り立つ)&lt;/li&gt;
&lt;li&gt;nステップ Sarsa&lt;ul&gt;
&lt;li&gt;nステップ先までの報酬を考慮。例: \( q_t^{(2)} = R_{t+1} + \gamma R_{t+2} + \gamma^2 Q(S_{t+2}) \)&lt;/li&gt;
&lt;li&gt;\( Q(S_t, A) \leftarrow Q(S_t, A) + \alpha (q_t^{(n)} - Q(S_t, A)) \)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Sarsa(λ) の前向き観点&lt;ul&gt;
&lt;li&gt;\( q^\lambda_t = (1 - \lambda) \sum_{n=1}^\infty \lambda^{n-1}q_t^{(n)} \)&lt;/li&gt;
&lt;li&gt;\( Q(S_t, A) \leftarrow Q(S_t, A) + \alpha (q^\lambda_t - Q(S_t, A)) \)&lt;/li&gt;
&lt;li&gt;問題点：時間軸上で先読みしている。エピソードの最後まで待ちたくない&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Sarsa(λ) の後ろ向き観点&lt;ul&gt;
&lt;li&gt;Eligibility Trace \( E_t(s, a) \) を定義 (TD(λ) と違い、状態と行動のペアに対して定義)&lt;/li&gt;
&lt;li&gt;\( \delta_t = R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \) &lt;/li&gt;
&lt;li&gt;\( Q(s, a) \leftarrow Q(s, a) + \alpha \delta_t E_t(s, a) \)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;方策オフ型&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;別の方策 \( \mu \) に従いながら、対象の方策 \( \pi \) を評価&lt;/li&gt;
&lt;li&gt;なぜこれが重要か&lt;ul&gt;
&lt;li&gt;人間や他のエージェントから学ぶ&lt;/li&gt;
&lt;li&gt;古い方策によって作られた経験から学習&lt;/li&gt;
&lt;li&gt;探索的な方策から、最適な方策を学習&lt;/li&gt;
&lt;li&gt;一つの方策から、複数の方策を学習&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;重要サンプリング&lt;ul&gt;
&lt;li&gt;異なる分布の期待値を予測する&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;方策オフ型のモンテカルロ法に重要サンプリングを適用する&lt;ul&gt;
&lt;li&gt;非常に大きい分散。ほとんど使えない。&lt;/li&gt;
&lt;li&gt;TD 学習を使うことが必須&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Q-Learning&lt;ul&gt;
&lt;li&gt;振る舞いを規定する方策 \( \mu \) によって取られた（実際の）行動 \( A_{t+1} \)&lt;/li&gt;
&lt;li&gt;学習中の方策によって取られた（仮想的な）行動 \( A' \)&lt;/li&gt;
&lt;li&gt;\( Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha (R_{t+1} + \gamma Q(S_{t+1}, A') - Q(S^t, A)) \)&lt;/li&gt;
&lt;li&gt;方策オフ型 Q-Learning → 学習したい方策が貪欲的な場合 (SarsaMax)&lt;ul&gt;
&lt;li&gt;最適価値関数に収束&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;講義6: 価値関数の近似&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;はじめに&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;大規模な強化学習&lt;ul&gt;
&lt;li&gt;バックギャモン: \( 10^{20} \) 個の状態&lt;/li&gt;
&lt;li&gt;囲碁: \( 10^{170} \) 個の状態&lt;/li&gt;
&lt;li&gt;ヘリコプター: 連続的な状態 → もはや参照テーブルを作ることができない&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;価値 \( V(s) \) もしくは \( Q(s, a) \)&lt;ul&gt;
&lt;li&gt;テーブルが巨大になってメモリに載らない、もしくは載ったとしてもスパースすぎて学習が遅い&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;関数近似&lt;ul&gt;
&lt;li&gt;\( v_\pi(s) \) を \( \widehat{v} (s, {\bf w})\)で近似&lt;/li&gt;
&lt;li&gt;\( q_\pi(s, a) \) を \( \widehat{q} (s, a, {\bf w}) \) で近似&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;３つのタイプの関数近似&lt;ul&gt;
&lt;li&gt;s → 関数 → \( \widehat{v} (s, {\bf w})\)&lt;/li&gt;
&lt;li&gt;s, a → 関数 → \( \widehat{q} (s, a, {\bf w}) \)&lt;/li&gt;
&lt;li&gt;s → 関数 → \( \widehat{q} (s, a_1, {\bf w}), ..., \widehat{q} (s, a_m, {\bf w}) \)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;関数の近似法&lt;ul&gt;
&lt;li&gt;特徴量の線形和 → 微分可能&lt;/li&gt;
&lt;li&gt;ニューラル・ネットワーク → 微分可能&lt;/li&gt;
&lt;li&gt;決定木&lt;/li&gt;
&lt;li&gt;近傍法&lt;/li&gt;
&lt;li&gt;フーリエ・ウェーブレット基底&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;非定常、iid でない入力&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;逐次的手法&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;勾配降下法&lt;ul&gt;
&lt;li&gt;\( J(w) \) パラメータ \( w \) の微分可能な関数&lt;/li&gt;
&lt;li&gt;J の勾配 \( \nabla_w J(w) = (\frac{\partial J(w)}{\partial w_1}, .. )^T \)&lt;/li&gt;
&lt;li&gt;\( \Delta w \) を、\( \nabla_w J(w) \) に比例して更新&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;確率的勾配降下法による価値関数の近似&lt;ul&gt;
&lt;li&gt;価値関数の真の値（オラクル）が分かったとしたら、目的関数 (平均二乗誤差)は、&lt;ul&gt;
&lt;li&gt;\( J(w) = E_\pi [ (v_\pi(S) - \widehat{v}(S, w))^2 ] \)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;期待値ではなく、インスタンス1つづつ更新する&lt;ul&gt;
&lt;li&gt;\( \Delta W = \alpha(v_\pi(S) - \widehat{v}(S, w)) \nabla_w \widehat{v}(S, w) \)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;特殊な場合：関数が特徴量の線形和の場合&lt;ul&gt;
&lt;li&gt;目的関数は、パラメータ w の二次関数&lt;/li&gt;
&lt;li&gt;確率的勾配降下法によって、大域解が求まる&lt;/li&gt;
&lt;li&gt;テーブル参照は、線形価値関数近似の特殊な場合&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;逐次的予測アルゴリズム&lt;ul&gt;
&lt;li&gt;オラクル \( v_\pi(s) \) の代わりに、ターゲット(予測値)を使う → ターゲットを使って教師あり学習をするのと同様&lt;ul&gt;
&lt;li&gt;MC: \( G_t \)&lt;/li&gt;
&lt;li&gt;TD(0): \( R_{t+1} + \gamma \widehat{v}(S_{t+1}, w) \)&lt;/li&gt;
&lt;li&gt;TD(λ): \( G^\lambda_t \)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;MC&lt;ul&gt;
&lt;li&gt;\( (S_1, G_1), (S_2, G_2), ... \) を「訓練データ」として使うのと同等&lt;/li&gt;
&lt;li&gt;非線形価値関数を使っている時でも、(局所)最小解に収束する&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;TD(0)&lt;ul&gt;
&lt;li&gt;\( R_{t+1} + \gamma \widehat{v}(S_{t+1}, w) \) は、偏りのあるサンプル&lt;/li&gt;
&lt;li&gt;線形TD(0) は、大域解(の近く)に収束する&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;TD(λ)&lt;ul&gt;
&lt;li&gt;\( (S_1, G^\lambda_1), (S_2, G^\lambda_2), ... \) を訓練データとして使うのと同等&lt;/li&gt;
&lt;li&gt;前向き観点: \( G^\lambda_t \) を使う&lt;/li&gt;
&lt;li&gt;後向き観点&lt;ul&gt;
&lt;li&gt;Eligibility Trace: \( \gamma \lambda E_{t-1} + x(S_t) \) → 各特徴量について計算&lt;/li&gt;
&lt;li&gt;\( \Delta_w = \alpha \delta_t E_t \)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;価値関数近似による制御&lt;ul&gt;
&lt;li&gt;価値関数近似 \( \widehat{q}(s, a, w) \) ＋ ε-貪欲方策&lt;/li&gt;
&lt;li&gt;例: ニューラル・ネットワークにより \( q \) を近似: \( \widehat{q}(S, A, w) = q_\pi(S, A) \)&lt;/li&gt;
&lt;li&gt;上と同じ議論：真の値（オラクル）を仮定し、それとの平均二乗誤差&lt;/li&gt;
&lt;li&gt;素性は、状態 S と行動 A の関数になる&lt;/li&gt;
&lt;li&gt;MC, TD(0), TD(λ) を、上と同様に定義できる&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ブートストラップ法を使うべきか？どう λ を設定するか？&lt;ul&gt;
&lt;li&gt;多くの問題において、\( \lambda = 0.9 \) あたりで性能が最高になる&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;収束&lt;ul&gt;
&lt;li&gt;評価&lt;ul&gt;
&lt;li&gt;TD は、方策オフ型の場合、モデルが線形でも非線形でも、収束しない（発散・振動）する場合がある&lt;/li&gt;
&lt;li&gt;Gradient TD は、この問題を解決&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;制御&lt;ul&gt;
&lt;li&gt;最適値に近づくにつれて、「ぶれ」（＝最適値に近づいたり遠ざかったりする）の問題が起こる&lt;/li&gt;
&lt;li&gt;非線形では収束が保証されない&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;バッチ手法&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;勾配降下法は、サンプル効率的ではない&lt;/li&gt;
&lt;li&gt;最小二乗予測&lt;ul&gt;
&lt;li&gt;経験 \( D = { (s_1, v_1^\pi), ..., (s_T, V_T^\pi) } \)&lt;/li&gt;
&lt;li&gt;\( LS(w) = E_D[(v^\pi - \widehat{v}(s, w)^2] \)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;経験再生を使った確率的勾配降下法&lt;ul&gt;
&lt;li&gt;経験 D から \( (s, v^\pi) \) をサンプリング、確率的勾配降下法を適用&lt;/li&gt;
&lt;li&gt;\( LS(w) \) に収束&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;経験再生を使った DQN (Deep Q-Networks)&lt;ul&gt;
&lt;li&gt;行動は、ε貪欲方策に従う&lt;/li&gt;
&lt;li&gt;遷移 \( s_t, a_t, r_{t+1}, s_{t+1} \) を、全てメモリー D に保存しておく&lt;/li&gt;
&lt;li&gt;D から、ミニバッチ (64個のインスタンス) を取り出す&lt;/li&gt;
&lt;li&gt;Q-Learning ターゲットをこのミニバッチを使って計算&lt;/li&gt;
&lt;li&gt;ターゲットとQ-network の最小二乗誤差を最小化&lt;/li&gt;
&lt;li&gt;安定化のためのテクニック&lt;ul&gt;
&lt;li&gt;経験再生を使うと、インスタンス間の相関が減る&lt;/li&gt;
&lt;li&gt;2つのネットワークを使う。片方をフリーズさせ、最新の予測を使うかわりにそのネットワークを使う&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;線形最小二乗誤差予測&lt;ul&gt;
&lt;li&gt;線形モデルを使えば、閉じた形で解が求まる → 効率的&lt;/li&gt;
&lt;li&gt;MC, TD, TD(λ) と組み合わせて、LSMC, LSTD, LSTD(λ) を得る&lt;/li&gt;
&lt;li&gt;TD に比べて、収束性が向上&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;線形最小二乗誤差制御 (LSPI)&lt;ul&gt;
&lt;li&gt;収束性が向上&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;講義7: 方策勾配法&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;はじめに&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;これまでは、価値関数から (例えば、ε貪欲法を使って) 方策を直接生成した&lt;/li&gt;
&lt;li&gt;価値関数をモデル化する代わりに、方策を直接モデル化する&lt;ul&gt;
&lt;li&gt;\( \pi_\theta(s, a) = P[a | s, \theta] \)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;長所&lt;ul&gt;
&lt;li&gt;良い収束性&lt;/li&gt;
&lt;li&gt;高次元もしくは行動が連続空間の場合&lt;/li&gt;
&lt;li&gt;確率的な方策を学べる&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;確率的な方策が良い場合&lt;ul&gt;
&lt;li&gt;じゃんけん&lt;ul&gt;
&lt;li&gt;もし、方策が決定的なら、相手にそのことを利用されてしまう&lt;/li&gt;
&lt;li&gt;最適な方策は、確率的にランダムな手を出すこと&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Aliasing (偽信号 -&amp;gt; 2つ以上の状態がお互いに見分けられない場合) が起こる場合&lt;ul&gt;
&lt;li&gt;確率的に行動するのが最適&lt;/li&gt;
&lt;li&gt;素性のせいで、環境の表現が制限される場合も、これに相当&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;方策の目的関数&lt;ul&gt;
&lt;li&gt;1) 開始状態の値を使う 2) 状態の平均値を使う 3) 1ステップ毎の平均報酬&lt;/li&gt;
&lt;li&gt;どれを使っても同じ手法 (方策勾配) になる&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;方策最適化&lt;ul&gt;
&lt;li&gt;\( J(\theta) \) を最小化する \( \theta \) を見つける&lt;/li&gt;
&lt;li&gt;様々な手法が使える&lt;ul&gt;
&lt;li&gt;勾配を使わない手法&lt;/li&gt;
&lt;li&gt;勾配を使う手法 (例: 勾配降下法) &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Finite Difference 方策勾配法&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;勾配の方向に登り、極大解を探す \( \Delta \theta = \alpha_\theta J(\theta) \)&lt;/li&gt;
&lt;li&gt;Finite Differences&lt;ul&gt;
&lt;li&gt;各次元について、少し ε だけ値を変えて、\( J(\theta) \) がどう変化するか見る → 勾配の近似&lt;/li&gt;
&lt;li&gt;高次元の場合、非効率的&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;モンテカルロ方策勾配法&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;尤度比&lt;ul&gt;
&lt;li&gt;方策勾配を解析的に計算する&lt;/li&gt;
&lt;li&gt;\( \pi_\theta \) が微分可能で、勾配 \( \nabla_\theta \pi_\theta(s, a) \) が分かっているとすると&lt;/li&gt;
&lt;li&gt;\( \nabla_\theta \pi_\theta(s, a) = \pi_\theta(s, a) \nabla_\theta \log \pi_\theta(s, a) \)&lt;/li&gt;
&lt;li&gt;スコア関数 \( \nabla_\theta \log \pi_\theta(s, a) \) &lt;/li&gt;
&lt;li&gt;これに従うと、尤度最大化 (MLE)&lt;/li&gt;
&lt;li&gt;Softmax 方策&lt;ul&gt;
&lt;li&gt;\( \pi_\theta(s, a) \propto \exp{ \phi(s, a)^T \theta )} \)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ガウシアン方策&lt;ul&gt;
&lt;li&gt;平均を、特徴量の線形和で表現 \( \mu(s) = \phi(s)^T \theta \)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;One-Step MDP&lt;ul&gt;
&lt;li&gt;尤度比トリックを使う:  \( \nabla_\theta J(\theta) = E_{\pi_\theta}[\nabla_\theta \log \pi_\theta(s, a)r] \) &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;方策勾配定理&lt;ul&gt;
&lt;li&gt;どの方策目的関数に対しても、\( \nabla_\theta J(\theta) = E_{\pi_\theta}[\nabla_\theta \log \pi_\theta(s, a) Q^{\theta_\pi}(s, a)] \)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Monte-Carlo Policy Gradient (REINFORCE)&lt;ul&gt;
&lt;li&gt;パラメータを勾配降下(上昇)法で更新&lt;/li&gt;
&lt;li&gt;\( Q^{\pi_\theta} \) の不偏サンプルとして、\( v_t \) (t から最後までの報酬和) を使う&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Actor-Critic 方策勾配法&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;モンテカルロ方策は、分散がまだ大きい&lt;/li&gt;
&lt;li&gt;Critic を使って、行動価値関数を近似&lt;/li&gt;
&lt;li&gt;Critic: パラメータ w を使う、Actor: パラメータ θ を使う&lt;/li&gt;
&lt;li&gt;Critic: \( Q_w(s, a) \) → 前回の講義と同様に推定&lt;/li&gt;
&lt;li&gt;分散を減らすトリック：ベースラインを使う&lt;ul&gt;
&lt;li&gt;期待値を変えずに、ベースラインを減らせる&lt;/li&gt;
&lt;li&gt;方策勾配から \( B(s) \) を引く&lt;/li&gt;
&lt;li&gt;状態価値関数 \( V^{\pi_\theta} \) をベースラインとして使うと良い&lt;/li&gt;
&lt;li&gt;Advantage Function \( A^{\pi_\theta}(s, a) = Q^{\pi_\theta}(s, a) - V^{\pi_\theta}(s) \)&lt;ul&gt;
&lt;li&gt;→ 方策勾配に組み込む&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;どうやって Advantage Function を推定するか&lt;ul&gt;
&lt;li&gt;方法1. ２つの異なるパラメータを使う&lt;/li&gt;
&lt;li&gt;方法2. TD 誤差を使う (期待値が Advantage Function と同じになる)&lt;ul&gt;
&lt;li&gt;\( V(s) \) だけを推定すれば良い&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Critic の変種 (異なる時間スケール、ターゲット)&lt;ul&gt;
&lt;li&gt;MC, TD(0), 前向き観点 TD(λ), 後ろ向き観点 TD(λ)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Actor の変種 (異なる時間スケール、ターゲット)&lt;ul&gt;
&lt;li&gt;MC → 利得 \( v_t \) &lt;/li&gt;
&lt;li&gt;TD → TD誤差 \( r + \gamma V_v(s_{t+1}) \)&lt;/li&gt;
&lt;li&gt;Eligibility Trace&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;全く偏りの無い方策勾配を求めることも可能 → Compatible function approximator&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;</content></entry><entry><title>ベストセラー「睡眠こそ最強の解決策である」著者マシュー・ウォーカー氏による Google 最新講演</title><link href="http://englishforhackers.com/matthew-walker-why-we-sleep-google-talk.html" rel="alternate"></link><published>2018-07-27T00:00:00-04:00</published><updated>2018-07-27T00:00:00-04:00</updated><author><name>Masato Hagiwara</name></author><id>tag:englishforhackers.com,2018-07-27:/matthew-walker-why-we-sleep-google-talk.html</id><summary type="html">&lt;p&gt;「自分は睡眠が少なくても大丈夫」と思っている人は、「ぜんぜん酔ってないから」と言いながら車を運転して帰ろうとする酔っぱらいぐらい危険である。&lt;/p&gt;
&lt;p&gt;&lt;img alt="マシュー・ウォーカー氏講演" src="images/matthew-walker.png"&gt;&lt;/p&gt;
&lt;p&gt;アメリカ・イギリスでベストセラーとなった「睡眠こそ最強の解決策である」の著者であり、睡眠研究の世界的第一人者であるマシュー・ウォーカー氏の主張は一貫しています。本書は、日本でも少し話題になったので、聞いたことのある方も少なからず居るのではないでしょうか。私も書籍を英語で全て読みましたが、評判どおり「これまでの考え方と人生を根本的に変えてしまうスゴ本」で、事あるごとに人にオススメしています。&lt;/p&gt;
&lt;p&gt;Center for Human Sleep Science （人類睡眠科学センター）創始者・所長でもある著者のマシュー・ウォーカー氏が、 Google で講演したビデオがありましたので、まとめてみました。書籍を全て読むのは時間がかかりますが、本トークであれば要点が３０分程度で把握できるので、時間の無い方、要点だけをざっと掴みたい方にオススメです。&lt;/p&gt;
&lt;p&gt;少し難し目の医学系・生物系の用語も出てきますが、ウォーカー氏の話し方もゆっくりで、字幕もついているので、割と簡単に理解できるでしょう。&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=aXflBZXAucQ"&gt;講演ビデオはこちら&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;生殖器&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;５時間しか寝ない男性は、もっと寝る男性に比べて睾丸が小さい&lt;/li&gt;
&lt;li&gt;女性の生殖能力についても同じ&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;記憶&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;学習した後で、記憶に「保存」するには、睡眠が必要 …&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;p&gt;「自分は睡眠が少なくても大丈夫」と思っている人は、「ぜんぜん酔ってないから」と言いながら車を運転して帰ろうとする酔っぱらいぐらい危険である。&lt;/p&gt;
&lt;p&gt;&lt;img alt="マシュー・ウォーカー氏講演" src="images/matthew-walker.png"&gt;&lt;/p&gt;
&lt;p&gt;アメリカ・イギリスでベストセラーとなった「睡眠こそ最強の解決策である」の著者であり、睡眠研究の世界的第一人者であるマシュー・ウォーカー氏の主張は一貫しています。本書は、日本でも少し話題になったので、聞いたことのある方も少なからず居るのではないでしょうか。私も書籍を英語で全て読みましたが、評判どおり「これまでの考え方と人生を根本的に変えてしまうスゴ本」で、事あるごとに人にオススメしています。&lt;/p&gt;
&lt;p&gt;Center for Human Sleep Science （人類睡眠科学センター）創始者・所長でもある著者のマシュー・ウォーカー氏が、 Google で講演したビデオがありましたので、まとめてみました。書籍を全て読むのは時間がかかりますが、本トークであれば要点が３０分程度で把握できるので、時間の無い方、要点だけをざっと掴みたい方にオススメです。&lt;/p&gt;
&lt;p&gt;少し難し目の医学系・生物系の用語も出てきますが、ウォーカー氏の話し方もゆっくりで、字幕もついているので、割と簡単に理解できるでしょう。&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=aXflBZXAucQ"&gt;講演ビデオはこちら&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;生殖器&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;５時間しか寝ない男性は、もっと寝る男性に比べて睾丸が小さい&lt;/li&gt;
&lt;li&gt;女性の生殖能力についても同じ&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;記憶&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;学習した後で、記憶に「保存」するには、睡眠が必要。学習の前にも、脳を「準備」するために睡眠が必要。&lt;/li&gt;
&lt;li&gt;徹夜すると、８時間十分に睡眠したグループに比べ、学習の効率が４０％も落ちる&lt;/li&gt;
&lt;li&gt;海馬＝脳の「受信トレイ」　睡眠不足になると、海馬の活動が無くなり、新しいことを覚えられなくなる。&lt;/li&gt;
&lt;li&gt;睡眠紡錘波 (sleep spindles) 「ファイルの転送」学習したことを長期記憶（大脳新皮質）に運ぶ&lt;ul&gt;
&lt;li&gt;覚えたことを長期記憶に保存するので、忘れにくい&lt;/li&gt;
&lt;li&gt;ファイルを短期記憶から削除したので、新しいことを覚えられる&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;医療&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;加齢に伴い、学習・記憶能力が低下する。同時に、(特に深い)眠りの質も低下する。この２つには相関以上の関係がある可能性&lt;/li&gt;
&lt;li&gt;深い眠りの欠乏が、加齢に伴う認知症、アルツハイマー病になる可能性を高める&lt;/li&gt;
&lt;li&gt;良いニュース：眠りであれば、対処できる可能性がある&lt;/li&gt;
&lt;li&gt;睡眠薬は、自然な眠りを作り出さない。ガンや死亡する率を高める&lt;/li&gt;
&lt;li&gt;ウォーカー氏のアプローチ：微弱な電流を使い脳を刺激。脳の記憶効率を上げる。加齢に伴う記憶力低下に効果があるか？自分で試さないこと！&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;教育&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;アメリカでの学校の始業時間について。7:25 に学校に着く場合、スクールバスは早い場合 5:30 に出発。&lt;/li&gt;
&lt;li&gt;ミネソタ州イーダイナ市 → 7:25 から 8:30 へ始業時間を遅らせた。SAT のトップ10% スコアが、1250 から 1500 へ改善。&lt;/li&gt;
&lt;li&gt;睡眠の改善によって、学業成績が改善、問題行動・無断欠席・精神病問題が低下&lt;/li&gt;
&lt;li&gt;生徒の平均寿命も増加。この年齢の死亡原因ナンバーワンである運転中の交通事故が激減&lt;/li&gt;
&lt;li&gt;ワイオミング州のティトン → 7:35 から 8:55 へ始業時間を遅らせ、交通事故数が 70% 減少&lt;/li&gt;
&lt;li&gt;教育における睡眠の重要性を見直す時が来ている&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;感情&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;睡眠不足になると、感情が不安定になる&lt;/li&gt;
&lt;li&gt;睡眠不足の被検者の例（ビデオ）&lt;/li&gt;
&lt;li&gt;扁桃体 (amygdala) 感情的な反応を作り出す。睡眠不足は、扁桃体の反応性を 60% 高める&lt;/li&gt;
&lt;li&gt;様々な精神病の状態と類似。睡眠は重大な精神病に重要な関係がある&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;身体&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;他の体の様々なシステムと睡眠にも深い関係がある&lt;/li&gt;
&lt;li&gt;１週間、睡眠不足になると、糖尿病予備群と診断されるほど、血糖値が混乱される&lt;/li&gt;
&lt;li&gt;70カ国、16億人の人を対象にした「実験」→夏時間。&lt;ul&gt;
&lt;li&gt;春に１時間睡眠時間が減ると、心臓発作の危険性が 24% 増加&lt;/li&gt;
&lt;li&gt;秋に１時間睡眠時間が増えると、逆に 21% 減少&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;免疫システム：ナチュラルキラー細胞 (NK細胞)。がん細胞を破壊する&lt;/li&gt;
&lt;li&gt;人間の体は、つねにがん細胞を作り出している。これらががんに発達するのを防ぐのがNK細胞&lt;/li&gt;
&lt;li&gt;一晩、４時間しか寝ないと、NK細胞の活動が 70% も低下&lt;/li&gt;
&lt;li&gt;短い睡眠と、様々な種類のがんとは因果関係がある。WHO は、夜勤の仕事を「可能性の高い発がん因子」に分類&lt;/li&gt;
&lt;li&gt;睡眠が足りないと、寿命が短くなり、残った寿命のクオリティも下がる&lt;/li&gt;
&lt;li&gt;がんと闘病中で、睡眠が足らない場合、がんはもっと早く成長する&lt;/li&gt;
&lt;li&gt;マウスでの実験。がん細胞を移植し、１ヶ月間、睡眠を制限すると、腫瘍サイズ・成長スピードが3倍になる。転移も観察&lt;/li&gt;
&lt;li&gt;DNA 遺伝子&lt;ul&gt;
&lt;li&gt;１週間、睡眠を6時間に制限すると、711個の遺伝子に異変。(アメリカ人の約半分が、平日6時間以下しか寝ない)&lt;/li&gt;
&lt;li&gt;免疫に関する遺伝子の活動が低下、腫瘍・炎症、ストレスなどに関する遺伝子の活動が活発に&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;「遺伝子組み換え食品」には反対するのに、睡眠不足によって自分に対して「遺伝子組み換え実験」をしている&lt;/li&gt;
&lt;li&gt;家の壊れた水道管のように、睡眠不足による悪影響は体のあちこちに浸透する&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;まとめ&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;睡眠は「贅沢」ではない&lt;/li&gt;
&lt;li&gt;睡眠は、生物学的に譲れない必需品。生命維持装置。&lt;/li&gt;
&lt;li&gt;睡眠不足は、先進国に広がる伝染病のようなもの。21世紀における大きな社会健康問題の一つ&lt;/li&gt;
&lt;li&gt;「怠けている」と汚名を着せられないように、十分に睡眠を取れる社会の実現を。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;英語&lt;/th&gt;
&lt;th&gt;日本語&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;testicle&lt;/td&gt;
&lt;td&gt;睾丸&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;virility&lt;/td&gt;
&lt;td&gt;生殖能力&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;impairment&lt;/td&gt;
&lt;td&gt;障害・機能低下&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;pull the all-nighter&lt;/td&gt;
&lt;td&gt;徹夜をする&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;deprivation&lt;/td&gt;
&lt;td&gt;奪うこと、欠乏&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;shuteye&lt;/td&gt;
&lt;td&gt;睡眠&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ace&lt;/td&gt;
&lt;td&gt;v. 高得点を取る&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;parenthetically&lt;/td&gt;
&lt;td&gt;ちなみに&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;symbiotic&lt;/td&gt;
&lt;td&gt;共生の&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;silver lining&lt;/td&gt;
&lt;td&gt;不幸中の幸い、明るい兆し&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;truancy&lt;/td&gt;
&lt;td&gt;無断欠席&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;life expectancy&lt;/td&gt;
&lt;td&gt;平均寿命&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;incessant&lt;/td&gt;
&lt;td&gt;絶え間ない&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;affable&lt;/td&gt;
&lt;td&gt;親しみやすい&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;psychiatric&lt;/td&gt;
&lt;td&gt;精神病の&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;delusional&lt;/td&gt;
&lt;td&gt;思い違い&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;profanity&lt;/td&gt;
&lt;td&gt;口汚い言葉&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;lucid&lt;/td&gt;
&lt;td&gt;正気の&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;hilarious&lt;/td&gt;
&lt;td&gt;とてもおかしい&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;tumor&lt;/td&gt;
&lt;td&gt;腫瘍&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;metastasize&lt;/td&gt;
&lt;td&gt;転移&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;mortality&lt;/td&gt;
&lt;td&gt;死亡率&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;erode&lt;/td&gt;
&lt;td&gt;侵食する&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;expression&lt;/td&gt;
&lt;td&gt;(遺伝子の)発現&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;inflict&lt;/td&gt;
&lt;td&gt;与える、傷などを負わせる&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;unscathed&lt;/td&gt;
&lt;td&gt;無傷の&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;nook and cranny&lt;/td&gt;
&lt;td&gt;ありとあらゆる場所&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;tamper&lt;/td&gt;
&lt;td&gt;改ざんする&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;decimation&lt;/td&gt;
&lt;td&gt;大量破壊&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;</content><category term="Productivity"></category><category term="Self Improvement"></category><category term="Science"></category></entry><entry><title>テスラ自動車 人工知能部門長のアンドレイ・カルパシー氏による「ソフトウェア 2.0 スタックの開発」最新講演</title><link href="http://englishforhackers.com/building-the-software-2.0-stack-by-andrej-karpathy.html" rel="alternate"></link><published>2018-06-13T00:00:00-04:00</published><updated>2018-06-13T00:00:00-04:00</updated><author><name>Masato Hagiwara</name></author><id>tag:englishforhackers.com,2018-06-13:/building-the-software-2.0-stack-by-andrej-karpathy.html</id><summary type="html">&lt;p&gt;テスラ自動車の人工知能部門長 (director) であるアンドレイ・カルパシー (Andrej Karpathy) 氏。
少し前に、&lt;a href="https://medium.com/@karpathy/software-2-0-a64152b37c35"&gt;ソフトウェア 2.0&lt;/a&gt;という概念を提唱したことで少し話題になったことを
覚えてらっしゃる方も居るかもしれません。ソフトウェア 2.0 とは、ニューラル・ネットワークに代表されるように、
枠組みと大量のデータを人間が与えるだけで問題を解けるように内部構造が最適化されるフレームワークのことを指します。&lt;/p&gt;
&lt;p&gt;この「ソフトウェア 2.0」のおかげで、これまで「人間がどうやってプログラムするか分からかった」ような問題、例えば、
画像認識や音声認識・音声合成、機械翻訳やゲーム（囲碁）などで、近年、目覚ましい性能の向上が達成されたことは、この分野の進歩を追っている方なら
既にご存知ではないでしょうか。&lt;/p&gt;
&lt;p&gt;本記事では、カルパシー氏の最新の講演「ソフトウェア 2.0 スタックの開発」のまとめおよび用語集を紹介します。
話が非常に面白く、また、機械学習を現実の問題に適用している方なら「あるある」とうなずける内容ばかりですので …&lt;/p&gt;</summary><content type="html">&lt;p&gt;テスラ自動車の人工知能部門長 (director) であるアンドレイ・カルパシー (Andrej Karpathy) 氏。
少し前に、&lt;a href="https://medium.com/@karpathy/software-2-0-a64152b37c35"&gt;ソフトウェア 2.0&lt;/a&gt;という概念を提唱したことで少し話題になったことを
覚えてらっしゃる方も居るかもしれません。ソフトウェア 2.0 とは、ニューラル・ネットワークに代表されるように、
枠組みと大量のデータを人間が与えるだけで問題を解けるように内部構造が最適化されるフレームワークのことを指します。&lt;/p&gt;
&lt;p&gt;この「ソフトウェア 2.0」のおかげで、これまで「人間がどうやってプログラムするか分からかった」ような問題、例えば、
画像認識や音声認識・音声合成、機械翻訳やゲーム（囲碁）などで、近年、目覚ましい性能の向上が達成されたことは、この分野の進歩を追っている方なら
既にご存知ではないでしょうか。&lt;/p&gt;
&lt;p&gt;本記事では、カルパシー氏の最新の講演「ソフトウェア 2.0 スタックの開発」のまとめおよび用語集を紹介します。
話が非常に面白く、また、機械学習を現実の問題に適用している方なら「あるある」とうなずける内容ばかりですので、
英語学習＋機械学習、両方の目的にオススメです。&lt;/p&gt;
&lt;p&gt;公園動画はこちら&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.figure-eight.com/building-the-software-2-0-stack-by-andrej-karpathy-from-tesla/"&gt;Building the Software 2.0 Stack&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;歴史&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;コンピュータの発明&lt;/li&gt;
&lt;li&gt;ソフトウェアの開発 → 問題を同定し、細かい問題に分解し、コンポーネントを書き、積み重ねる（「スタック」する）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;「スタック」アプローチの問題点&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;画像認識 → どうやって「画像認識スタック」を開発するか&lt;/li&gt;
&lt;li&gt;早期の画像認識スタック：脳の認知からスタートし、問題を細分化（例；部品を認識）&lt;/li&gt;
&lt;li&gt;画像認識（1990〜2010年）：特徴量を抽出し、最後に少しだけ機械学習を使う&lt;ul&gt;
&lt;li&gt;問題：特徴量の種類が多すぎる。全て別々の言語で書かれている&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;画像認識（今）： アーキテクチャを指定し、組成から最後まで最適化&lt;ul&gt;
&lt;li&gt;アーキテクチャそのものの最適化まで&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;データの規模が増えるに従い、エンジニアリングの仕事を最適化が肩代わり&lt;/li&gt;
&lt;li&gt;ソフトウェア 1.0 と 2.0 の違い&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;プログラムの空間&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;円。原点からの距離＝プログラムの複雑さ&lt;/li&gt;
&lt;li&gt;ソフトウェア 1.0：原点からあまり遠くない点&lt;/li&gt;
&lt;li&gt;ソフトウェア 2.0：領域（プログラムの集合）。計算量を使って、この領域の中でうまくいくプログラムを見つける&lt;/li&gt;
&lt;li&gt;カルパシー氏ツイート「悪いけど、あなたよりも勾配降下法の方がうまくプログラムが書けるよ」&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ソフトウェア 2.0&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;評価は簡単だが、どうやってプログラムを書いたらよいか分からない領域で威力を発揮&lt;/li&gt;
&lt;li&gt;Google はこの最先端。論文 "One Model to Learn Them All" では、単一のモデルで複数のタスクを解く&lt;/li&gt;
&lt;li&gt;AlphaZero データセットさえ不要&lt;/li&gt;
&lt;li&gt;ニューラル・ネットワークに限ったトレンドではない。例：プログラムの確率的最適化&lt;/li&gt;
&lt;li&gt;ロボット工学（ただし、人形・犬型ロボットで有名な Boston Dynamics の制御は完全にソフトウェア 1.0）&lt;/li&gt;
&lt;li&gt;注：ソフトウェア 2.0 がソフトウェア 1.0 を置き換えるわけではない。2.0 の周りには大量の 1.0 インフラが必要 &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;利点&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;計算的に均一（例：畳み込みとしきい値操作だけ）ハードウェアで置き換えやすい&lt;/li&gt;
&lt;li&gt;実行時間とメモリ空間が定数（ソフトウェア 1.0 は、実行時間の推定が難しい）&lt;/li&gt;
&lt;li&gt;変更が用意（例：精度を少し犠牲にしてもよいから、２倍高速化する）&lt;/li&gt;
&lt;li&gt;チューニングが可能（ソフトウェア 1.0 では、ライブラリは基本的に変更不能）&lt;/li&gt;
&lt;li&gt;性能が高い&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;テスラの自動運転&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;カルパシー氏がジョインした時、ソフトウェア 2.0 のコードは少量&lt;/li&gt;
&lt;li&gt;少しずつその範囲を拡大&lt;/li&gt;
&lt;li&gt;例：停まっている車を検出&lt;ul&gt;
&lt;li&gt;ソフトウェア 1.0：領域を検出し、その上にいくつかルールを書く&lt;/li&gt;
&lt;li&gt;ソフトウェア 2.0：「ニューラルネットがそう言ったから」&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;最適化がプログラムを書くなら、人間は何をするのか&lt;/li&gt;
&lt;li&gt;チームが２つに分かれる：1) データ作成 2) データ・インフラの開発&lt;/li&gt;
&lt;li&gt;データ作成者が、実はプログラミングしている&lt;/li&gt;
&lt;li&gt;PhD時代、モデルとアルゴリズムにほとんどの時間&lt;/li&gt;
&lt;li&gt;テスラ時代、3/4 の時間はデータセットを作っている&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;教訓&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ラベリングは単純なタスクではない&lt;ul&gt;
&lt;li&gt;例：車線の検出&lt;/li&gt;
&lt;li&gt;例：停まっている車&lt;/li&gt;
&lt;li&gt;例：信号&lt;/li&gt;
&lt;li&gt;例：制限速度&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;データのバランス&lt;ul&gt;
&lt;li&gt;電車 vs 路面電車&lt;/li&gt;
&lt;li&gt;制限速度 (45 mph が多いが、ごくたまに 85 mph)&lt;/li&gt;
&lt;li&gt;ウインカー&lt;/li&gt;
&lt;li&gt;黄色信号&lt;/li&gt;
&lt;li&gt;ほとんどの画像が、高速道路。雪道、坂などはあまり現れない&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ラベリングは、繰り返し的な手順である&lt;ul&gt;
&lt;li&gt;例：「雨検出器」イーロン・マスク「雨粒が見えるから、画像認識で解決しろ」&lt;/li&gt;
&lt;li&gt;トンネル内、太陽の逆光に弱かった。雨粒とシミの区別が難しい&lt;/li&gt;
&lt;li&gt;YouTubeのビデオ。コーンフレーク → ワイパーが動作。ケチャップ → 動作しない&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ソフトウェア 2.0 のツール&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;まだ存在しない。気づいている人がほとんど居ない&lt;/li&gt;
&lt;li&gt;ソフトウェア 2.0 の IDE （統合開発環境）とは？&lt;/li&gt;
&lt;li&gt;ソフトウェア 2.0 のための大規模で綺麗で多様なデータセットを作るのが、ほとんどの仕事&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;用語集&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;英語&lt;/th&gt;
&lt;th&gt;日本語&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;resourceful&lt;/td&gt;
&lt;td&gt;資源の豊富な&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;break down&lt;/td&gt;
&lt;td&gt;1) より小さなものに分解する, 2) 行き詰まる&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;put on 〜 hat&lt;/td&gt;
&lt;td&gt;〜 の役を演じる、〜 として行動する&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;gnarly&lt;/td&gt;
&lt;td&gt;俗) 難しい&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;lay out&lt;/td&gt;
&lt;td&gt;計画を立てる&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;forefront&lt;/td&gt;
&lt;td&gt;最前線&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;caveat&lt;/td&gt;
&lt;td&gt;注意&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;homogeneous&lt;/td&gt;
&lt;td&gt;均一の&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;sprawling, gross thing&lt;/td&gt;
&lt;td&gt;無秩序に広がった乱雑なもの&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;fine-tune&lt;/td&gt;
&lt;td&gt;微調整する&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;autonomous driving&lt;/td&gt;
&lt;td&gt;自動運転&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;trench&lt;/td&gt;
&lt;td&gt;塹壕。ここでは開発の現場、第一線の意味。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;conundrum&lt;/td&gt;
&lt;td&gt;難問&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;AI complete&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ja.wikipedia.org/wiki/AI%E5%AE%8C%E5%85%A8"&gt;AI完全&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;fleet&lt;/td&gt;
&lt;td&gt;ここでは、多数の自動車&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;blinker&lt;/td&gt;
&lt;td&gt;ウインカー&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;... is a thing&lt;/td&gt;
&lt;td&gt;重要なもの、流行っているもの&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;massage&lt;/td&gt;
&lt;td&gt;ここでは、こねくり回すこと&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;</content></entry><entry><title>Ilya Sutskever 氏によるメタ学習と自己学習の最前線 (MIT 講義まとめ)</title><link href="http://englishforhackers.com/ilya-sutskever-meta-learning-self-play.html" rel="alternate"></link><published>2018-05-25T00:00:00-04:00</published><updated>2018-05-25T00:00:00-04:00</updated><author><name>Masato Hagiwara</name></author><id>tag:englishforhackers.com,2018-05-25:/ilya-sutskever-meta-learning-self-play.html</id><summary type="html">&lt;p&gt;OpenAI の共同創立者兼研究ディレクターである Ilya Sutskever 氏。過去 5 年以内の論文の総引用数が 46,000 を超えるという、名実共に現在の人工知能・深層学習分野の第一人者でしょう。&lt;/p&gt;
&lt;p&gt;MIT の「汎用人工知能(Artificial General Intelligence)」という講義シリーズの中でのSutskever 氏の講演が上がっていましたので、今回は抄訳とともにご紹介します。
メタ学習や自己学習など、最先端の話題もさることながら、進化論への言及など、非常に示唆深い、分かりやすい講演でおすすめです。&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=9EN_HoEk3KY"&gt;講演ビデオはこちら&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;はじめに&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;そもそも深層学習はなぜうまくいくか&lt;ul&gt;
&lt;li&gt;定理：与えられたデータに対して、そのデータを生成できる最小のプログラムが、最も良い予測ができる&lt;/li&gt;
&lt;li&gt;ただし、データが与えられたときに、そのデータを生成できる最小のプログラムを見つけることは困難である&lt;/li&gt;
&lt;li&gt;最小のプログラムの代わりに、最小の「回路」なら、逆伝播を使って見つけることができる&lt;/li&gt;
&lt;li&gt;AI の基礎となる事実&lt;/li&gt;
&lt;li&gt;ニューラルネットの訓練 → ニューラル「式」を解いている&lt;/li&gt;
&lt;li&gt;N個の Nビットの数字を …&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;p&gt;OpenAI の共同創立者兼研究ディレクターである Ilya Sutskever 氏。過去 5 年以内の論文の総引用数が 46,000 を超えるという、名実共に現在の人工知能・深層学習分野の第一人者でしょう。&lt;/p&gt;
&lt;p&gt;MIT の「汎用人工知能(Artificial General Intelligence)」という講義シリーズの中でのSutskever 氏の講演が上がっていましたので、今回は抄訳とともにご紹介します。
メタ学習や自己学習など、最先端の話題もさることながら、進化論への言及など、非常に示唆深い、分かりやすい講演でおすすめです。&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=9EN_HoEk3KY"&gt;講演ビデオはこちら&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;はじめに&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;そもそも深層学習はなぜうまくいくか&lt;ul&gt;
&lt;li&gt;定理：与えられたデータに対して、そのデータを生成できる最小のプログラムが、最も良い予測ができる&lt;/li&gt;
&lt;li&gt;ただし、データが与えられたときに、そのデータを生成できる最小のプログラムを見つけることは困難である&lt;/li&gt;
&lt;li&gt;最小のプログラムの代わりに、最小の「回路」なら、逆伝播を使って見つけることができる&lt;/li&gt;
&lt;li&gt;AI の基礎となる事実&lt;/li&gt;
&lt;li&gt;ニューラルネットの訓練 → ニューラル「式」を解いている&lt;/li&gt;
&lt;li&gt;N個の Nビットの数字を、2つしか隠れ層を持たないニューラルネットでソートできる&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;強化学習&lt;ul&gt;
&lt;li&gt;エージェントを環境内で評価する枠組み&lt;/li&gt;
&lt;li&gt;完璧ではないが「そこそこ良い」結果を出すアルゴリズムが存在する&lt;/li&gt;
&lt;li&gt;問題点：報酬が環境によって与えられる点。現実では、エージェント(人間)が観察から報酬を「理解」する&lt;ul&gt;
&lt;li&gt;本当の「報酬」は、生存か死亡か。それ以外は全てそれに帰結される&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;アルゴリズム&lt;ul&gt;
&lt;li&gt;ロバストでシンプルだが、効率が良くない&lt;/li&gt;
&lt;li&gt;まとめ「ランダムに新しいことを試す。予測と現実を比較し、現実が予測を上回ったなら、これらの行動を強化するようにパラーメタを変える」&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;2つの手法&lt;ul&gt;
&lt;li&gt;Policy gradient&lt;ul&gt;
&lt;li&gt;報酬の合計の期待値 → 微分 → 奇跡的に、上の「まとめ」と同じになる&lt;/li&gt;
&lt;li&gt;方策オン型 (on-policy) &lt;/li&gt;
&lt;li&gt;自分自身の行動からしか学習できない&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Q-learning&lt;ul&gt;
&lt;li&gt;方策オフ型 (off-policy)&lt;/li&gt;
&lt;li&gt;自分自身ではなく、他のエージェントからのデータでも学習できる&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;強化学習のポテンシャル&lt;ul&gt;
&lt;li&gt;サンプル効率性が良い&lt;/li&gt;
&lt;li&gt;現在のアルゴリズムの効率は良くないが、徐々に改善している&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;メタ学習&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;学習方法を学習する&lt;/li&gt;
&lt;li&gt;システムを複数のタスクで訓練する&lt;/li&gt;
&lt;li&gt;インスタンスの代わりに、「タスク＋テストケース」を与える&lt;/li&gt;
&lt;li&gt;例&lt;ul&gt;
&lt;li&gt;[Mishra et al. 2017] 手書き文字を分類する&lt;/li&gt;
&lt;li&gt;[Zoph and Le, 2017] ニューラル構造サーチ&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;例 [Peng et al. 2017]&lt;ul&gt;
&lt;li&gt;方策をシミュレーションで学習し、物理的なロボットに移すことができるか&lt;/li&gt;
&lt;li&gt;問題：シミュレーターと現実が違う（特に摩擦などを再現するのが難しい）&lt;/li&gt;
&lt;li&gt;シミュレーター環境の物理条件にランダム性を加え、それに適応する方策を学習する&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;例 [Frans et al. 2017]&lt;ul&gt;
&lt;li&gt;方策の階層&lt;/li&gt;
&lt;li&gt;メタ学習を利用し、タスクを最速で達成するための基礎行動を学習&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;現在の機械学習&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;訓練時とテスト時の設定が同じ&lt;/li&gt;
&lt;li&gt;現実はそうではない（学校で学んだことが仕事で役立つとは限らない）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Hindsight Experience Replay (事後経験再現) [Andrychowicz et al. 2017]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;強化学習の問題：報酬が無いと学習できない&lt;/li&gt;
&lt;li&gt;「行動を経て、ゴールを達成できなかった」という経験、すなわち失敗から何かを学習できないか&lt;/li&gt;
&lt;li&gt;例: A に到達したかったのに、B に到達してしまった&lt;ul&gt;
&lt;li&gt;「A に到達できなかった。何も学べなかった」ではなく「A ではなく、B に到達する方法を学んだ」&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;A に到達するとき、A に到達する方法の方策オン型学習をしているのと同時に、B に到達する方法の方策オフ型学習をしている&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;自己学習&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;TD-Gammon [Tesauro, 1992]&lt;ul&gt;
&lt;li&gt;2つのニューラルネットにバックギャモンの対戦をさせる&lt;/li&gt;
&lt;li&gt;世界チャンピオンに勝つ戦略を生み出す&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;AlphaGo Zero&lt;ul&gt;
&lt;li&gt;囲碁の世界チャンピオンに外部データ無しで勝つ&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;DOTA 2&lt;ul&gt;
&lt;li&gt;1対1で世界チャンピオンに勝つ&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;自己学習の魅力&lt;ul&gt;
&lt;li&gt;エージェント自身が環境を作る&lt;/li&gt;
&lt;li&gt;敵対関係にあることで、お互いに学習する&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;相撲 [Bansal et al., 2017]&lt;ul&gt;
&lt;li&gt;お互いを外に出すだけ&lt;/li&gt;
&lt;li&gt;立つ、バランスを取るなど、何も分からない状態からスタート&lt;/li&gt;
&lt;li&gt;学習するには、自分と同じぐらい賢い相手が必要。いつも勝っていたら学習しない&lt;/li&gt;
&lt;li&gt;転移学習も起きる。相撲で学習したエージェントにランダムに力を加えてもバランスが取れる&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Dota 2 Bot&lt;ul&gt;
&lt;li&gt;はじめは弱かったが、数ヶ月で世界チャンピオンに匹敵するほどに&lt;/li&gt;
&lt;li&gt;「自己学習は、計算をデータに変換する」&lt;/li&gt;
&lt;li&gt;今後、コンピュータの能力が高まるにしたがって、さらに重要度を増す&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;未来&lt;ul&gt;
&lt;li&gt;自己学習の結果が、外のタスクにも有用になるためにはどうしたらよいか&lt;/li&gt;
&lt;li&gt;人間の脳は、過去200万年に渡って、急激にその容量が増大した&lt;ul&gt;
&lt;li&gt;Sutskever 氏の考え：「部族の中での立ち位置」が生存に重要であることに気づいた&lt;/li&gt;
&lt;li&gt;他の「大きい脳を持つ」人間とうまくやっていくには、もっと大きな脳を持つことが生存に有利&lt;/li&gt;
&lt;li&gt;制約の無い自己学習から、交渉、言語、政治などが生まれる可能性も。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;もし、このような環境から汎用知能が生まれ、かつ、Dota 2 Bot で見たような急激な能力の向上が引き継がれるなら、この汎用知能の能力も急速に向上するだろう&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;アラインメント [Christiano et al., 2017]&lt;ul&gt;
&lt;li&gt;目標をどうエージェントに伝えるか&lt;/li&gt;
&lt;li&gt;行動を２つ人間に見せて、どちらか良いかを選ぶだけ&lt;/li&gt;
&lt;li&gt;500回ぐらい選ぶと、シミュレーションした「足」が宙返りできるようになる&lt;/li&gt;
&lt;li&gt;Atari のゲームも学習できる&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;</content></entry><entry><title>機械学習：技術的負債の高金利クレジットカード</title><link href="http://englishforhackers.com/machine-learning-technical-debt.html" rel="alternate"></link><published>2018-05-04T00:00:00-04:00</published><updated>2018-05-04T00:00:00-04:00</updated><author><name>Masato Hagiwara</name></author><id>tag:englishforhackers.com,2018-05-04:/machine-learning-technical-debt.html</id><summary type="html">&lt;p&gt;Google で機械学習システムの開発に携わる D. Scully 氏らによる&lt;/p&gt;
&lt;p&gt;&lt;a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43146.pdf"&gt;Machine Learning: The High-Interest Credit Card of Technical Debt
(機械学習：技術的負債の高金利クレジットカード)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;という論文。機械学習は、複雑なシステムを素早く開発するにあたって非常に強力なツールとなるが、それと同時に、
大きな技術的負債（メンテナンスコスト）を抱えるリスクがある。そのリスク要因と対処法についてまとめたのが本論文。&lt;/p&gt;
&lt;p&gt;発表当時、日本でも少し話題になったので、日本語で検索するといくつか翻訳を目にすることができますが、
ここでは、あらためて抄訳を試みるとともに、読む上で重要となる単語・表現を最後に紹介したいと思います。&lt;/p&gt;
&lt;p&gt;概要&lt;/p&gt;
&lt;p&gt;機械学習は、複雑なシステムを素早く開発するにあたって非常に強力なツールとなるが、これらのメリットがタダで
享受できると考えるのは危ない。機械学習を使う場合、システムのレベルで非常に大きな技術的負債（メンテナンスコスト）
を抱えるリスクがある。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;機械学習と複雑なシステム&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;機械学習パッケージは、通常のコードとして複雑さの問題をはらんでいるのと同時に、
システムレベルで「隠れた」技術的負債を抱える恐れがある。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;本論文では、機械学習のコードと、大きなシステムレベルとの間の相互作用に焦点を当てる。
ここに隠れた技術的負債が溜まりやすい …&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ol&gt;</summary><content type="html">&lt;p&gt;Google で機械学習システムの開発に携わる D. Scully 氏らによる&lt;/p&gt;
&lt;p&gt;&lt;a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43146.pdf"&gt;Machine Learning: The High-Interest Credit Card of Technical Debt
(機械学習：技術的負債の高金利クレジットカード)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;という論文。機械学習は、複雑なシステムを素早く開発するにあたって非常に強力なツールとなるが、それと同時に、
大きな技術的負債（メンテナンスコスト）を抱えるリスクがある。そのリスク要因と対処法についてまとめたのが本論文。&lt;/p&gt;
&lt;p&gt;発表当時、日本でも少し話題になったので、日本語で検索するといくつか翻訳を目にすることができますが、
ここでは、あらためて抄訳を試みるとともに、読む上で重要となる単語・表現を最後に紹介したいと思います。&lt;/p&gt;
&lt;p&gt;概要&lt;/p&gt;
&lt;p&gt;機械学習は、複雑なシステムを素早く開発するにあたって非常に強力なツールとなるが、これらのメリットがタダで
享受できると考えるのは危ない。機械学習を使う場合、システムのレベルで非常に大きな技術的負債（メンテナンスコスト）
を抱えるリスクがある。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;機械学習と複雑なシステム&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;機械学習パッケージは、通常のコードとして複雑さの問題をはらんでいるのと同時に、
システムレベルで「隠れた」技術的負債を抱える恐れがある。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;本論文では、機械学習のコードと、大きなシステムレベルとの間の相互作用に焦点を当てる。
ここに隠れた技術的負債が溜まりやすい。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;境界&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;機械学習を使うそもそもの理由は、ソフトウェアのロジックが外部データに依存しているから&lt;/li&gt;
&lt;li&gt;→ 抽象的な振る舞いをデータから分離することができない&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;2.1 もつれ&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CACE (Change Anything, Change Everything) の原則：特徴量を変更・追加すると全てが影響を受ける&lt;/li&gt;
&lt;li&gt;特徴量だけではなく、ハイパー・パラメータ（正則化、学習の設定、訓練時のサンプリング等）にも言える&lt;/li&gt;
&lt;li&gt;解決法&lt;ul&gt;
&lt;li&gt;アンサンブル学習を使う&lt;/li&gt;
&lt;li&gt;高次元可視化ツールなどを使い、モデルの予測を観測する&lt;/li&gt;
&lt;li&gt;より高度な正則化法を使う&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ある程度の「もつれ」は、アルゴリズムに関わらず、機械学習に内在的なもの&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;2.2 隠れたフィードバック・ループ&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ユーザーの行動等、実世界のデータから訓練された機械学習システムが、実世界に影響を与えること&lt;/li&gt;
&lt;li&gt;例：CTR (クリック率) の予測モデル&lt;/li&gt;
&lt;li&gt;もし、「ユーザーが、先週にクリックしたニュース見出しの数」が特徴量に入っていると・・・。&lt;/li&gt;
&lt;li&gt;注意深く観察し、このようなループを除去すること&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;2.3 宣言してない使用者&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;知らないうちに、機械学習システムの予測を他のシステムが利用していることがある&lt;/li&gt;
&lt;li&gt;システムに変更を加えるのがとたんに難しく、コストがかかるようになる&lt;/li&gt;
&lt;li&gt;隠れたフィードバック・ループを作り出してしまうことも&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;データの依存性&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ソフトウェア開発では、依存性が、コードの複雑さを増大させ、技術的負債を作り出す&lt;/li&gt;
&lt;li&gt;機械学習では、データの依存性が負債を作り出す&lt;/li&gt;
&lt;li&gt;データの依存性は、静的解析などによる検出が難しい&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;3.1 不安定なデータの依存性&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;機械学習システムの入力が、時間が経つにつれて質的に変化することがある&lt;/li&gt;
&lt;li&gt;上記 CACE の原則の通り、機械学習システムに対して、診断しにくい、ときに重大な影響を及ぼす&lt;/li&gt;
&lt;li&gt;解決法: データのコピーにバージョン番号を振り管理する&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;3.2 十分に活用されていないデータの依存性&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;機械学習システムの精度にたいしてほとんど貢献していない入力や特徴量&lt;/li&gt;
&lt;li&gt;例: 古くなった特徴量、特徴量グループ、ε特徴量 (複雑さに対して精度の向上が小さい特徴量)&lt;/li&gt;
&lt;li&gt;解決法: 各特徴量を除去した場合の影響を定期的に評価する&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;3.3 データ依存性の静的解析&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;静的解析ツールの例 (&lt;a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41159.pdf"&gt;論文&lt;/a&gt;) 
  データ源と特徴量にアノテートでき、依存関係の木構造を分析できる&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;3.4 修正の伝搬&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;問題 A に対してモデル a が存在するとき、類似した問題 A' に対して、a を入力として a' を手っ取り早く作る&lt;/li&gt;
&lt;li&gt;a' は a に依存しているため、将来、モデルの改善を分析するのが難しくなる&lt;/li&gt;
&lt;li&gt;a → a' → a'' と何重にもなっている場合、事態はより深刻になる&lt;/li&gt;
&lt;li&gt;解決策: a に特徴量を追加して、修正を直接学習する&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;システムレベルのスパゲティ&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;機械学習システム設計における避けるべきパターン&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;4.1 グルーコード&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;機械学習の汎用パッケージを使うために、入力・出力のために大量のグルーコードが必要になる&lt;/li&gt;
&lt;li&gt;パッケージには利点もあるが、成熟したシステムにおいては、問題空間の構成そのものをリファクタリングしたほうがメリットが大きい&lt;/li&gt;
&lt;li&gt;解決策: システムと同じ言語で、機械学習アルゴリズムを再実装する&lt;/li&gt;
&lt;li&gt;機械学習システムでは、実際に「機械学習」をしている部分は非常に小さい (5%)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;4.2 パイプラインのジャングル&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;機械学習のデータを準備するパイプラインが、スクレイパー、結合、サンプリング、中間ファイルなどで乱雑になり、管理・エラー検出・修正などが大変になる&lt;/li&gt;
&lt;li&gt;解決策: データ収集、特徴量抽出に対して、大域的な目で考える。時には、一から再設計することも。&lt;/li&gt;
&lt;li&gt;組織の「研究」と「エンジニアリング」職が離れすぎているのが根本的な原因のことも。&lt;/li&gt;
&lt;li&gt;Google では、エンジニアと研究者を同じチームに「埋め込む」ハイブリッド型研究モデルが、このような技術的負債を未然に防いでいる&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;4.3 古くなった実験的コード&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;アルゴリズムなどを、プロダクションコードに対する条件付きブランチとして実験&lt;/li&gt;
&lt;li&gt;一つ一つの変更は、低コストだが、後方互換性の維持などによって、次第に技術的負債となる&lt;/li&gt;
&lt;li&gt;定期的に、削除できないかどうか見直す&lt;/li&gt;
&lt;li&gt;そもそも、実験用のコードはプロダクションコードとは隔離されるべき。再設計・実装が必要かどうか定期的に検討する&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;4.4 設定負債&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;どの特徴量・データを使うか、アルゴリズムの設定、前処理、後処理などの設定に負債が溜まる。プロダクションコードに比べ、厳密にテストされてない&lt;/li&gt;
&lt;li&gt;設定不変条件に対するアサーションが役立つことも。２つの設定の diff を左右に並べて表示するツールも役に立つ。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;外部世界の変化&lt;/p&gt;
&lt;p&gt;5.1 固定されたしきい値&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;メールがスパムかどうか、広告を表示するかどうかなど、モデルから何らかの決定をする場合、精度・再現率などのトレードオフを考慮しながら、手動で設定&lt;/li&gt;
&lt;li&gt;モデルが更新された場合、そのしきい値が無効になる&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;5.2 相関関係の変化&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;因果関係にはないが相関関係にある特徴量が、時間が経つにつれ相関関係が無くなると、機械学習システムの予測の振る舞いが変わる&lt;/li&gt;
&lt;li&gt;因果関係ではない相関関係は、隠れた負債となり得る&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;5.3 監視とテスト&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;単体テストとシステムテストは大切だが、それだけでは不十分。システムの振る舞いをリアルタイムで監視することが必要。&lt;/li&gt;
&lt;li&gt;何を監視するか&lt;ul&gt;
&lt;li&gt;予測バイアス。予測ラベルと実際のラベルの分布を比べる&lt;ul&gt;
&lt;li&gt;完璧ではない。実際のラベルの平均値を出力するだけのベースラインモデルでも、この分布は同じになる&lt;/li&gt;
&lt;li&gt;実際にはとても役に立つ。何かの問題を示唆する場合も。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;行動制約&lt;ul&gt;
&lt;li&gt;実世界で何かの行動を起こす場合、監視のため、その行動に制約を設けることが有効なことも&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;負債の支払い&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;機械学習そのものが悪いとか、何としても技術的負債を防ぐべき、と言っているわけではない&lt;/li&gt;
&lt;li&gt;技術的負債は、エンジニアと研究者の両方が意識すべき問題。精度の向上幅は小さいが、システムの複雑性を大幅に増大させるような研究的な解放は、賢い方法だとは言えない&lt;/li&gt;
&lt;li&gt;複雑な機械学習システムに対し、全体的な視野を持ち、エレガントな解法を解決するのは、とても見返りの大きい仕事である&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;私の所属する組織でも、研究者とエンジニアが同じチームに所属しながら、機械学習をプロダクションに導入していく
埋め込みモデルを採用しています。今のところ、素早いスピードで機械学習をサービスに活かせており、うまく機能しています。&lt;/p&gt;
&lt;p&gt;技術的負債を考える際にも、それをそもそも増大させないための組織論から考えるのが大事ということですね。
&lt;a href="https://en.wikipedia.org/wiki/Conway%27s_law"&gt;「システムのデザインは、その組織のコミュニケーション構造を反映する」という趣旨のコンウェイの法則&lt;/a&gt;にも通じるところがありますね。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;英語&lt;/th&gt;
&lt;th&gt;日本語&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;debt&lt;/td&gt;
&lt;td&gt;負債、借金 ("b" を発音しないことに注意)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;incur&lt;/td&gt;
&lt;td&gt;(コストなどを) 被る&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;compound&lt;/td&gt;
&lt;td&gt;悪化する&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;invariant&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ja.wikipedia.org/wiki/%E4%B8%8D%E5%A4%89%E6%9D%A1%E4%BB%B6"&gt;不変条件&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;net&lt;/td&gt;
&lt;td&gt;(ここでは) 正味の、最終的な&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;mitigation&lt;/td&gt;
&lt;td&gt;緩和&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;deleterious&lt;/td&gt;
&lt;td&gt;有害な&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;sought&lt;/td&gt;
&lt;td&gt;seek の 過去・過去分詞形&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;holistic&lt;/td&gt;
&lt;td&gt;全体的な&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;holistically&lt;/td&gt;
&lt;td&gt;全体的に&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;root cause&lt;/td&gt;
&lt;td&gt;そもそもの原因&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;tease apart&lt;/td&gt;
&lt;td&gt;紐解く、抽出する&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;sanity check&lt;/td&gt;
&lt;td&gt;整合性などのチェック&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;pay off / pay down&lt;/td&gt;
&lt;td&gt;(負債を)返す&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;</content></entry><entry><title>「あなたはどのようにして英語が流暢になりましたか」に対するHacker Newsのコミュニティの反応</title><link href="http://englishforhackers.com/how-did-you-become-fluent-in-english.html" rel="alternate"></link><published>2018-04-29T00:00:00-04:00</published><updated>2018-04-29T00:00:00-04:00</updated><author><name>Masato Hagiwara</name></author><id>tag:englishforhackers.com,2018-04-29:/how-did-you-become-fluent-in-english.html</id><summary type="html">&lt;p&gt;Hacker News にて、&lt;a href="https://news.ycombinator.com/item?id=16786453"&gt;Ask HN: How did you become fluent in English?&lt;/a&gt;
「あなたはどのようにして英語が流暢になりましたか」というスレッドが人気になっていたので、少し前のことですが、人気のコメントをかいつまんで紹介したいと思います。&lt;/p&gt;
&lt;p&gt;（Hacker News は、トップページに上がってくる記事と議論の質が高いので良く見ています。英語で技術系のトレンドを追いたい方は要チェックのサイトです。）&lt;/p&gt;
&lt;p&gt;以下、「どのようにして英語が流暢になりましたか？」の回答：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;たくさん読む。Reddit や Hacker News, Slaskdot など。本、小説も。たくさん聞く。映画を英語音声＋英語字幕で見る。
  たくさん書く。英語でメモを取る。コードのコメントを英語で書く。上達はあまり気にしない。時間がかかるが、続けていれば必ず効果は現れる。- bayindirh&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;真似をする。読み書きを最初に習うと、ひどい訛りがつく。言語の全体を把握してから、読んだり文法を詳しく勉強する。 - robocat&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;「間違ったことを言うかも …&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;p&gt;Hacker News にて、&lt;a href="https://news.ycombinator.com/item?id=16786453"&gt;Ask HN: How did you become fluent in English?&lt;/a&gt;
「あなたはどのようにして英語が流暢になりましたか」というスレッドが人気になっていたので、少し前のことですが、人気のコメントをかいつまんで紹介したいと思います。&lt;/p&gt;
&lt;p&gt;（Hacker News は、トップページに上がってくる記事と議論の質が高いので良く見ています。英語で技術系のトレンドを追いたい方は要チェックのサイトです。）&lt;/p&gt;
&lt;p&gt;以下、「どのようにして英語が流暢になりましたか？」の回答：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;たくさん読む。Reddit や Hacker News, Slaskdot など。本、小説も。たくさん聞く。映画を英語音声＋英語字幕で見る。
  たくさん書く。英語でメモを取る。コードのコメントを英語で書く。上達はあまり気にしない。時間がかかるが、続けていれば必ず効果は現れる。- bayindirh&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;真似をする。読み書きを最初に習うと、ひどい訛りがつく。言語の全体を把握してから、読んだり文法を詳しく勉強する。 - robocat&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;「間違ったことを言うかも」と心配しない。話す時に文法を気にしない。だんだんと直る。ネットゲームで他のプレイヤーとチャット＋話す。 - sh4z&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;その言語しか使えない環境に身を置く。10年本を読み続けても話せるようにはならない。自分は日本のアニメを10年間見続けたが10単語しか話せない。- techsin101&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;言語を浴びること。「勉強」と暗記は、意味を思い出すのには役に立つかもしれないが、自動化するには言語に接しつづけるしかない。スピーキン
グは、筋肉と反射神経を鍛えるようなもの。スポーツなどの身体活動と同じように、実際に練習すること。-pasbesoin&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;無理に上達しようとせず、日常に取り入れること。外国語は、プログラミング言語と同様、「達成する」ものではなく、能力の一つ。- bayindirh&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;小さいときに、英語圏の国に旅行したこと。英語ができれば「世界を旅して観察することができる」と分かったこと。その後、映画を英語だけで見る、本を英語だけで読むと決心したこと。これが功を奏した。その後、アメリカで留学しアメリカで働いた。-simonebrunozzi&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;学ぼうとしている言語の文化にどっぷりと浸かること。周りの皆が英語を話す環境なら、社会的なプレッシャーがモチベーションになる。英語を学ぶこととは関係のないプロジェクトで、ネイティブ・スピーカーと一緒に働くのも良い。自分の興味のあることと同様の興味を持つネイティブ・スピーカーを探そう。最後に、英語のコメディを見よう。Silicon Valley (HBO) とか。 -anonytrary&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;カントリー音楽、ラップ音楽をたくさん聞く。 -raitom&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;テレビドラマ。自分はテレビドラマを見すぎて、アメリカに始めてきた時に「ニューヨーク訛りがある」とタクシーの運転手に言われた程。-yzmtf2008&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;読んだり聞いたりするのに慣れたら、実際に話すこと。英語を話さずに流暢になることはない。自分は、上級英語会話のコースを取って、色々なことを英語で議論した -danmaz74&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;言語を学ぶことはダイエットに似ている。一回で終わるものではなく、人生に渡る取り組みである。-euske&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;実際に使うこと。仕事などで使う場面がなければ、何かを達成するのに英語が必要な趣味などを持つのもいい。新しいプログラミング言語や考え方を学ぶのに似ている。-jimmies&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;教材の良し悪しにこだわらない。「低俗な」コンテンツから日常のコミュニケーションを学べるし、飽きない。-firexcy&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;英語がネイティブレベルまでに上達する人間には２種類いる。ミュージシャンと映画中毒者だ。ミュージシャンは音を聞き取り、それを真似するのが上手い。-sunstone -brooklyn_ashey&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;オンラインで記事を読む。面白い内容の本を読む。ドラマや映画を英語字幕で見る。-remir&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;とにかくやること。大量の英語を読み、書き、聞き、話す。ネットゲームで他のユーザーと話す、授業で自分と違う出身国の人を探し、一緒に課題をやる。Stack Exchange で答えを書く。オーディオブックも、「ながら」勉強できるので良い。-lucb1e&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;勉強を「勉強」と感じないようにできれば、新しいことを学ぶ可能性が上がる。聞く、話す、読む、書くことが言葉を学ぶカギだ。
  技術、映画、ファッション、時事、音楽など、何でも良いが、自分がワクワクするようなトピックを選び、それについて話したり書いたりしよう。毎日、これらについて見たり、聞いたり、読んだりする時間を設けよう。-juzffoo&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;上記で紹介した他にも、色々なコメントがありますが、ここで流暢になった多くの人に共通すると思われる要素は、&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;多くの英語を聞く・話す・読む・書くこと。&lt;/li&gt;
&lt;li&gt;テレビドラマを英語音声＋英語字幕で見る。&lt;/li&gt;
&lt;li&gt;趣味やプロジェクトなど、何でもいいので、英語でできる興味を持てることを見つける。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ではないでしょうか。ほとんどの人が、机に向ってたくさん勉強して流暢になったのではないことに注目したいですね。&lt;/p&gt;
&lt;p&gt;自分も、今ではアメリカで生活しアメリカの会社で働いているので英語は流暢ですが、他のほとんどの日本人と同様、英語を学び始めたのは中学の義務教育からでした。
これまでの自分の英語の上達を振り返ってみると、やはり上で書いたようなことが重要だったのだなと実感します。このあたりはまた稿を改めて書いてみたいと思います。&lt;/p&gt;
&lt;p&gt;もし、「自分はこうして流暢になった」というコメントがあれば、ぜひ下のコメント欄にて共有していただけると幸いです。&lt;/p&gt;</content></entry><entry><title>YC サム・アルトマン氏による生産性を高めるアドバイス</title><link href="http://englishforhackers.com/sam-altman-productivity.html" rel="alternate"></link><published>2018-04-08T00:00:00-04:00</published><updated>2018-04-08T00:00:00-04:00</updated><author><name>Masato Hagiwara</name></author><id>tag:englishforhackers.com,2018-04-08:/sam-altman-productivity.html</id><summary type="html">&lt;p&gt;Y Combinator の代表サム・アルトマン氏による&lt;a href="https://blog.samaltman.com/productivity"&gt;「生産性」と題されたブログ記事&lt;/a&gt;。弱冠三十代にして Y Combinator の代表を勤め、OpenAI の創立に関わるなど、本人も書いているように、生産性が非常に高い人物の一人であることは言うまでもありませんが、生産性を高めるためのアドバイスが詳細に書かれています。&lt;/p&gt;
&lt;p&gt;本人の生産性や哲学等については、&lt;a href="https://www.youtube.com/watch?v=sYMqVwsewSg"&gt;以前のインタビュー&lt;/a&gt; などでも断片的に語られていますが、こういってまとめて書かれていると、生産性を高めるための本人の考え方や試行錯誤がよくわかります。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;はじめに&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;「複利効果」は金融だけではなくキャリアにも魔法のような効果がある。&lt;/li&gt;
&lt;li&gt;小さな差であっても50年間積み重ねることで大きな差に。生産性を高めることには意義がある&lt;/li&gt;
&lt;li&gt;10%多くの仕事をこなし、毎日1%ずつ向上すれば、そうではない人との差は大きなものに&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;何に取り組むか&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;生産性のために最も重要なのは、正しいことに取り組むこと&lt;/li&gt;
&lt;li&gt;「すごい人」は、他の人にはない、世界に対する信念がある&lt;/li&gt;
&lt;li&gt;自分が話す人皆と同じ考えなら、それは悪い兆候&lt;/li&gt;
&lt;li&gt;スケジュールの中に、考える時間を残す。本を読む、面白い人と時間を過ごす、自然の中で過ごす、など&lt;/li&gt;
&lt;li&gt;自分が好きなこと、大切だと思うことに取り組む&lt;/li&gt;
&lt;li&gt;他の人も、自分の好きなことをする時に最も生産性が高い。委譲する時はそれを意識する …&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;p&gt;Y Combinator の代表サム・アルトマン氏による&lt;a href="https://blog.samaltman.com/productivity"&gt;「生産性」と題されたブログ記事&lt;/a&gt;。弱冠三十代にして Y Combinator の代表を勤め、OpenAI の創立に関わるなど、本人も書いているように、生産性が非常に高い人物の一人であることは言うまでもありませんが、生産性を高めるためのアドバイスが詳細に書かれています。&lt;/p&gt;
&lt;p&gt;本人の生産性や哲学等については、&lt;a href="https://www.youtube.com/watch?v=sYMqVwsewSg"&gt;以前のインタビュー&lt;/a&gt; などでも断片的に語られていますが、こういってまとめて書かれていると、生産性を高めるための本人の考え方や試行錯誤がよくわかります。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;はじめに&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;「複利効果」は金融だけではなくキャリアにも魔法のような効果がある。&lt;/li&gt;
&lt;li&gt;小さな差であっても50年間積み重ねることで大きな差に。生産性を高めることには意義がある&lt;/li&gt;
&lt;li&gt;10%多くの仕事をこなし、毎日1%ずつ向上すれば、そうではない人との差は大きなものに&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;何に取り組むか&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;生産性のために最も重要なのは、正しいことに取り組むこと&lt;/li&gt;
&lt;li&gt;「すごい人」は、他の人にはない、世界に対する信念がある&lt;/li&gt;
&lt;li&gt;自分が話す人皆と同じ考えなら、それは悪い兆候&lt;/li&gt;
&lt;li&gt;スケジュールの中に、考える時間を残す。本を読む、面白い人と時間を過ごす、自然の中で過ごす、など&lt;/li&gt;
&lt;li&gt;自分が好きなこと、大切だと思うことに取り組む&lt;/li&gt;
&lt;li&gt;他の人も、自分の好きなことをする時に最も生産性が高い。委譲する時はそれを意識する&lt;/li&gt;
&lt;li&gt;自分の好きではないことを長い時間やっていることに気づいたら、転職を真剣に考える&lt;/li&gt;
&lt;li&gt;頭がよく、生産的で、楽しく、ポジティブな人と時間を過ごす。それとは反対の人は避ける&lt;/li&gt;
&lt;li&gt;近道は無い。大切なことを成し遂げるには、賢く、かつ、一所懸命に働く必要がある&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;優先順位&lt;ul&gt;
&lt;li&gt;リストを作る。その年、その月、その日ごとに、達成したいことを書き出す&lt;/li&gt;
&lt;li&gt;分類や作業量の見積もりなどはしない&lt;/li&gt;
&lt;li&gt;毎日、進められる仕事から先に取り組むと、勢いがつき、どんどん物事を成し遂げられる&lt;/li&gt;
&lt;li&gt;断固として断る。失礼なぐらいメールの返事が素っ気ない&lt;/li&gt;
&lt;li&gt;会議はたいてい時間の無駄。ただし、新しい機会との出会いのため、スケジュールに余裕を残す&lt;/li&gt;
&lt;li&gt;ほとんどの会議は、15〜20分、もしくは、２時間がちょうどいい時間。デフォルトが１時間なのはおかしい&lt;/li&gt;
&lt;li&gt;一日の各時間に対して、違うタイプの仕事をする。自分は朝が一番生産的な時間なので、スケジュールを入れない&lt;/li&gt;
&lt;li&gt;「生産性ポルノ」の罠に落ちない。生産性を完ぺきにしようとするあまり、正しい問題に取り組んでいるかどうかを忘れがち&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;身体的な要素&lt;ul&gt;
&lt;li&gt;人によって、何が最も効果的かは違う。実験する&lt;/li&gt;
&lt;li&gt;第一に睡眠。睡眠トラッカーが役に立つ&lt;/li&gt;
&lt;li&gt;気温が低く、暗くて静かな部屋で寝る。敷布団にこだわる。旅行の時は耳栓とアイマスクを使う&lt;/li&gt;
&lt;li&gt;旅先で眠れない時は、睡眠薬を少量使うことも&lt;/li&gt;
&lt;li&gt;朝、メールチェックをするときに LED ライトを浴びる (&lt;a href="https://www.amazon.com/gp/product/B075H39NDL/"&gt;オススメ&lt;/a&gt;)。&lt;/li&gt;
&lt;li&gt;第二に運動。筋トレ１時間を週３回＋高強度インターバルトレーニングを時々&lt;/li&gt;
&lt;li&gt;第三に栄養。朝食を抜くミニ断食スタイルが自分に適している&lt;/li&gt;
&lt;li&gt;砂糖の多い食事、辛すぎる食事は避ける&lt;/li&gt;
&lt;li&gt;朝と朝食後にエスプレッソ&lt;/li&gt;
&lt;li&gt;ベジタリアン。ビタミンB12、オメガ3、鉄、ビタミンD3を補充&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;その他&lt;ul&gt;
&lt;li&gt;オフィス：自然光、静かで割り込みのない環境&lt;/li&gt;
&lt;li&gt;頻繁にしなければいけない雑事はソフトウェアを書いて解決。タイプ速度を上げる。キーボードショートカットを学ぶ&lt;/li&gt;
&lt;li&gt;何もする気になれない期間（１〜２週間）がある。そのうち治る。ネガティブにさせる場面や人々を避ける&lt;/li&gt;
&lt;li&gt;少し仕事を引き受けすぎるぐらいがちょうどいい。物事を効率的にこなせるようになる。あまりにもたくさん引き受けるのは避ける&lt;/li&gt;
&lt;li&gt;家族や友人をないがしろにしない。幸せ度で言うとトータルでマイナスになる。自分の好きなことや、頭を整理してくれることもないがしろにしない&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;最後に&lt;ul&gt;
&lt;li&gt;間違った方向に生産性だけを上げても意味がない。何に取り組むかについてもっと考える&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;英語&lt;/th&gt;
&lt;th&gt;日本語&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;compound effect&lt;/td&gt;
&lt;td&gt;複利効果&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;conviction&lt;/td&gt;
&lt;td&gt;信念&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;morale&lt;/td&gt;
&lt;td&gt;士気&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;delegate&lt;/td&gt;
&lt;td&gt;委譲する&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;belittle&lt;/td&gt;
&lt;td&gt;ばかにする、軽く扱う&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;momentum&lt;/td&gt;
&lt;td&gt;勢い&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;terse&lt;/td&gt;
&lt;td&gt;そっけない&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;optimize&lt;/td&gt;
&lt;td&gt;最適化する&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;optimal&lt;/td&gt;
&lt;td&gt;最適な&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;controversial&lt;/td&gt;
&lt;td&gt;賛否両論ある&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;fasting&lt;/td&gt;
&lt;td&gt;断食&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;overcommit&lt;/td&gt;
&lt;td&gt;仕事などを引き受けすぎる&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;</content></entry><entry><title>Googleの研究者が教える、良い機械学習プロダクトを実装するための43のルール</title><link href="http://englishforhackers.com/rules-of-machine-learning-best-practices-for-ml-engineering.html" rel="alternate"></link><published>2018-03-25T00:00:00-04:00</published><updated>2018-03-25T00:00:00-04:00</updated><author><name>Masato Hagiwara</name></author><id>tag:englishforhackers.com,2018-03-25:/rules-of-machine-learning-best-practices-for-ml-engineering.html</id><summary type="html">&lt;p&gt;Google のリサーチ・サイエンティストである Martin Zinkevich 氏によって書かれた、機械学習を使った良いプロダクトを開発するためのコツを集めた記事。エンジニアが良い機械学習プロダクトを作るには、機械学習の専門知識が無いことに苦心するのではなく、得意なエンジニアリングの技術を活かすことが重要、というのが主な趣旨です。&lt;/p&gt;
&lt;p&gt;紹介記事：&lt;a href="http://martin.zinkevich.org/rules_of_ml/rules_of_ml.pdf"&gt;Rules of Machine Learning: Best Practices for ML Engineering&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;はじめに&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ほとんどの問題はエンジニアリングに関する問題である&lt;/li&gt;
&lt;li&gt;性能向上は、良い機械学習のアルゴリズムではなく、良い素性によってもたらされる&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;機械学習の前に&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ルール1. 本当に必要になるまで機械学習を使わない&lt;/li&gt;
&lt;li&gt;ルール2. まず指標を設計、実装する&lt;/li&gt;
&lt;li&gt;ルール3. ヒューリスティックが複雑になりすぎる前に、機械学習に移行する&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;フェーズI: 最初のパイプライン&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ルール4. 最初のモデルはシンプルに。インフラをまず整える&lt;/li&gt;
&lt;li&gt;ルール5. インフラを機械学習とは独立にテストする&lt;ul&gt;
&lt;li&gt;素性は正しく計算できているか。モデルは訓練環境とテスト環境で同じ値を返すか。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ルール6. パイプラインをコピーする場合は、欠損データに気をつける&lt;/li&gt;
&lt;li&gt;ルール7. ヒューリスティックを素性に変換するか、外部的に扱う …&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;p&gt;Google のリサーチ・サイエンティストである Martin Zinkevich 氏によって書かれた、機械学習を使った良いプロダクトを開発するためのコツを集めた記事。エンジニアが良い機械学習プロダクトを作るには、機械学習の専門知識が無いことに苦心するのではなく、得意なエンジニアリングの技術を活かすことが重要、というのが主な趣旨です。&lt;/p&gt;
&lt;p&gt;紹介記事：&lt;a href="http://martin.zinkevich.org/rules_of_ml/rules_of_ml.pdf"&gt;Rules of Machine Learning: Best Practices for ML Engineering&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;はじめに&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ほとんどの問題はエンジニアリングに関する問題である&lt;/li&gt;
&lt;li&gt;性能向上は、良い機械学習のアルゴリズムではなく、良い素性によってもたらされる&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;機械学習の前に&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ルール1. 本当に必要になるまで機械学習を使わない&lt;/li&gt;
&lt;li&gt;ルール2. まず指標を設計、実装する&lt;/li&gt;
&lt;li&gt;ルール3. ヒューリスティックが複雑になりすぎる前に、機械学習に移行する&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;フェーズI: 最初のパイプライン&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ルール4. 最初のモデルはシンプルに。インフラをまず整える&lt;/li&gt;
&lt;li&gt;ルール5. インフラを機械学習とは独立にテストする&lt;ul&gt;
&lt;li&gt;素性は正しく計算できているか。モデルは訓練環境とテスト環境で同じ値を返すか。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ルール6. パイプラインをコピーする場合は、欠損データに気をつける&lt;/li&gt;
&lt;li&gt;ルール7. ヒューリスティックを素性に変換するか、外部的に扱う&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;モニタリング&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ルール8. モデルの即時性に関する要求を把握する&lt;ul&gt;
&lt;li&gt;例：広告やランキングに関するモデルはすぐに古くなる&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ルール9. モデルをデプロイする前に問題を検出する&lt;/li&gt;
&lt;li&gt;ルール10. 「無症状の故障」に気をつける&lt;ul&gt;
&lt;li&gt;例：素性を計算するための統計が古い場合、性能が除々に低下する&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ルール11. 素性グループにオーナーを割り当て、ドキュメントを整える&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;最初の目的関数&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;指標：システムが報告する何らかの数値&lt;/li&gt;
&lt;li&gt;目的関数：機械学習が最適化しようとしている指標&lt;/li&gt;
&lt;li&gt;ルール12. 何を目的関数とするか考えすぎない&lt;/li&gt;
&lt;li&gt;ルール13. 単純で、観察可能な、真の目的関数に帰着可能な指標を選ぶ&lt;ul&gt;
&lt;li&gt;良い例：リンクのクリック率　悪い例：アクティブユーザーの数&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ルール14. 解釈可能なモデルから始め、デバッグを簡単にする&lt;ul&gt;
&lt;li&gt;例：線形・ロジスティック・ポワソン回帰&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ルール15. スパムフィルタとランキングをポリシー層で分ける&lt;ul&gt;
&lt;li&gt;ポリシー層：機械学習に、（単純な）ロジックを追加する層&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;フェーズII: 素性エンジニアリング&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ルール16. ローンチして、改善する&lt;/li&gt;
&lt;li&gt;ルール17. 学習された素性ではなく、直接観測・報告可能な素性からはじめる&lt;/li&gt;
&lt;li&gt;ルール18. 複数の状況に対して一般化できる素性を試す&lt;/li&gt;
&lt;li&gt;ルール19. 可能なら、非常に特定的な素性を使う&lt;/li&gt;
&lt;li&gt;ルール20. 人間に解釈可能な方法で素性を変換・結合する&lt;ul&gt;
&lt;li&gt;例1. 離散化 → 年齢から年齢層に変換&lt;/li&gt;
&lt;li&gt;例2. 直積 → 素性どうしのデカルト積　性別ｘ国&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ルール21. 線形モデルで学習可能な素性の数は、データ量に比例&lt;ul&gt;
&lt;li&gt;例：1000インスタンス→10数個の素性&lt;/li&gt;
&lt;li&gt;例：1000万インスタンス→10万素性&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ルール22. 使用されていない素性は削除する&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;人間による分析&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ルール23. あなたは典型的なエンドユーザーではない&lt;ul&gt;
&lt;li&gt;必ずユーザーテストか、実ユーザーで実験する&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ルール24. モデルの「差分」をまず測定する&lt;/li&gt;
&lt;li&gt;ルール25. 予測性能より、実用的な性能を考えてモデルを選ぶ&lt;/li&gt;
&lt;li&gt;ルール26. 誤りパターンを良く観察し、新しい素性を作る&lt;/li&gt;
&lt;li&gt;ルール27. 望まない振る舞いに対しては、定量化してから最適化する&lt;/li&gt;
&lt;li&gt;ルール28. 短期的な振る舞いが同じでも、長期的な振る舞いが同じだとは限らない&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;訓練時と提供時の歪み&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;提供時：モデルの訓練が終わって、その予測をサービス上で実際に使っている時&lt;/li&gt;
&lt;li&gt;ルール29. 訓練時と提供時を同じにするには、提供時に素性をログに出力することで解決&lt;/li&gt;
&lt;li&gt;ルール30. 訓練時にデータを適当に取捨選択するのではなく、重要度サンプリングを使う&lt;/li&gt;
&lt;li&gt;ルール31. テーブルをジョインしている場合は、データが変化する&lt;/li&gt;
&lt;li&gt;ルール32. 訓練パイプラインと提供時のパイプライン間で、なるべくコードを再利用する&lt;/li&gt;
&lt;li&gt;ルール33. モデルを訓練した時に使ったデータよりも新しいデータでテストする&lt;/li&gt;
&lt;li&gt;ルール34. 性能を少し犠牲にしても、綺麗な訓練データを作る&lt;ul&gt;
&lt;li&gt;例：スパムフィルタで、1% を held-out にして必ずユーザーに提示&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ルール35. ランキング問題に内在する歪みに注意する&lt;/li&gt;
&lt;li&gt;ルール36. 位置素性に関するフィードバック・ループに注意&lt;/li&gt;
&lt;li&gt;ルール37. 訓練・提供時の歪みを測定する&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;フェーズIII: 鈍化する成長、最適化、複雑なモデル&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ルール38. もし目的関数が合っていないのなら、新しい機能に時間を割かない&lt;/li&gt;
&lt;li&gt;ルール39. ローンチするかの決定は、プロダクトの長期的な目標に合わせる&lt;ul&gt;
&lt;li&gt;ローンチの決定が簡単なのは、全ての指標がよくなった時だけ&lt;/li&gt;
&lt;li&gt;自分が最適化できる指標だけ最大化して満足しない&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ルール40. アンサンブルモデルは、シンプルに&lt;/li&gt;
&lt;li&gt;ルール41. 性能向上が頭打ちになったら、質的に異なる情報源を探す&lt;/li&gt;
&lt;li&gt;ルール42. コンテンツの人気度と、検索結果の多様性・パーソナリゼーション・関連性には関連があると思わない&lt;/li&gt;
&lt;li&gt;ルール43. ユーザーの友人関係・振る舞いはサービス間で似ているかもしれないが、興味はそうでもない&lt;ul&gt;
&lt;li&gt;パーソナリゼーション素性を異なるプロダクトに持って行ってうまく行ったことがない&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;用語&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;英語&lt;/th&gt;
&lt;th&gt;日本語&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;heuristics&lt;/td&gt;
&lt;td&gt;ヒューリスティック (答えを導くための割と簡単な方法)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;fancy&lt;/td&gt;
&lt;td&gt;(手法、モデルなどが) 凝った&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;sanity check&lt;/td&gt;
&lt;td&gt;正しさのチェック&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;feature column&lt;/td&gt;
&lt;td&gt;(Google 独自用語) 素性グループ&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;proxy&lt;/td&gt;
&lt;td&gt;代替物 (ここでは、真の目的関数ではないが、それに十分近い指標)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;serve&lt;/td&gt;
&lt;td&gt;提供する (訓練した機械学習モデル・システムの予測を実際にプロダクトで使うこと)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;</content></entry><entry><title>GAN (敵対的生成ネットワーク) チュートリアル @ NIPS 2016 まとめ - Ian Goodfellow</title><link href="http://englishforhackers.com/nips-2016-generative-adversarial-networks-tutorial-ian-goodfellow.html" rel="alternate"></link><published>2018-02-28T00:00:00-05:00</published><updated>2018-02-28T00:00:00-05:00</updated><author><name>Masato Hagiwara</name></author><id>tag:englishforhackers.com,2018-02-28:/nips-2016-generative-adversarial-networks-tutorial-ian-goodfellow.html</id><summary type="html">&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=AJVyzd0rqdc"&gt;Ian Goodfellow による、GAN (Generative Adversarial Network; 敵対的生成ネットワーク) のチュートリアル。&lt;/a&gt;
Goodfellow 氏は GAN のそもそもの生みの親であり、&lt;a href="http://www.deeplearningbook.org/"&gt;教科書「Deep Learning」&lt;/a&gt;の著者としても有名。
２時間と、とても長い盛りだくさんのチュートリアルだが、分かりやすく、具体例やコツなどの満載なので、とても参考になる。&lt;/p&gt;
&lt;p&gt;&lt;img alt="GAN (敵対的生成ネットワーク) チュートリアル @ NIPS 2016" src="images/nips-2016-gan-tutorial.png"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;生成モデリング&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;訓練データから確率分布の表現を得る&lt;ul&gt;
&lt;li&gt;密度推定&lt;/li&gt;
&lt;li&gt;サンプル生成&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;なぜ生成モデルを研究するか&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;これまでの機械学習　１つの入力に対して１つの出力&lt;/li&gt;
&lt;li&gt;高次元確率分布は重要な対象&lt;/li&gt;
&lt;li&gt;強化学習の未来、計画をシミュレーションできる&lt;/li&gt;
&lt;li&gt;欠損値の扱い、半教師あり学習&lt;/li&gt;
&lt;li&gt;マルチモーダル出力&lt;ul&gt;
&lt;li&gt;例：ビデオの次のフレームを予測&lt;ul&gt;
&lt;li&gt;多くの可能性があるので、ぼやけた画像になってしまう&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;画像超解像技術&lt;/li&gt;
&lt;li&gt;iGAN → 人間が線画を書くだけで、写真クオリティの画像を生成&lt;/li&gt;
&lt;li&gt;画像→画像翻訳　条件付きGAN&lt;ul&gt;
&lt;li&gt;線画→写真、航空写真→地図 …&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=AJVyzd0rqdc"&gt;Ian Goodfellow による、GAN (Generative Adversarial Network; 敵対的生成ネットワーク) のチュートリアル。&lt;/a&gt;
Goodfellow 氏は GAN のそもそもの生みの親であり、&lt;a href="http://www.deeplearningbook.org/"&gt;教科書「Deep Learning」&lt;/a&gt;の著者としても有名。
２時間と、とても長い盛りだくさんのチュートリアルだが、分かりやすく、具体例やコツなどの満載なので、とても参考になる。&lt;/p&gt;
&lt;p&gt;&lt;img alt="GAN (敵対的生成ネットワーク) チュートリアル @ NIPS 2016" src="images/nips-2016-gan-tutorial.png"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;生成モデリング&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;訓練データから確率分布の表現を得る&lt;ul&gt;
&lt;li&gt;密度推定&lt;/li&gt;
&lt;li&gt;サンプル生成&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;なぜ生成モデルを研究するか&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;これまでの機械学習　１つの入力に対して１つの出力&lt;/li&gt;
&lt;li&gt;高次元確率分布は重要な対象&lt;/li&gt;
&lt;li&gt;強化学習の未来、計画をシミュレーションできる&lt;/li&gt;
&lt;li&gt;欠損値の扱い、半教師あり学習&lt;/li&gt;
&lt;li&gt;マルチモーダル出力&lt;ul&gt;
&lt;li&gt;例：ビデオの次のフレームを予測&lt;ul&gt;
&lt;li&gt;多くの可能性があるので、ぼやけた画像になってしまう&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;画像超解像技術&lt;/li&gt;
&lt;li&gt;iGAN → 人間が線画を書くだけで、写真クオリティの画像を生成&lt;/li&gt;
&lt;li&gt;画像→画像翻訳　条件付きGAN&lt;ul&gt;
&lt;li&gt;線画→写真、航空写真→地図&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;よりリアルな生成タスク (画像、音声)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;生成モデルの比較&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;最尤推定&lt;ul&gt;
&lt;li&gt;E[log p(x|Θ)] を最大化するΘを求める&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;他のほとんどのモデルは、どう最尤推定するかによって分類できる&lt;/li&gt;
&lt;li&gt;尤度関数が明示的かどうか (p を 評価できるか)&lt;ul&gt;
&lt;li&gt;密度が計算可能&lt;ul&gt;
&lt;li&gt;観測可能な Belief Nets&lt;ul&gt;
&lt;li&gt;連鎖律を使って p_model を明示的に&lt;/li&gt;
&lt;li&gt;PixelCNN&lt;ul&gt;
&lt;li&gt;サンプル生成が遅い、並列化できない、潜在的なコードで制御できない&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;WaveNet&lt;ul&gt;
&lt;li&gt;高クオリティ&lt;/li&gt;
&lt;li&gt;サンプル生成が遅い。1秒の音声を生成するために2分かかる&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="http://proceedings.mlr.press/v15/larochelle11a/larochelle11a.pdf"&gt;NADE&lt;/a&gt;, &lt;a href="https://arxiv.org/pdf/1502.03509.pdf"&gt;MADE&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PixelRNN&lt;/li&gt;
&lt;li&gt;非線形 ICA&lt;ul&gt;
&lt;li&gt;非線型変換を使って別の空間にマップ&lt;/li&gt;
&lt;li&gt;ヤコビ行列（の行列式）が存在する必要。入力と同じ次元が必要&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;密度を近似&lt;ul&gt;
&lt;li&gt;Variational autoencoder&lt;ul&gt;
&lt;li&gt;変分近似&lt;/li&gt;
&lt;li&gt;分布 q が完全の時だけ、元の分布と漸近一貫性がある&lt;/li&gt;
&lt;li&gt;高い尤度を達成可能だが、サンプルの質が低い&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;マルコフ連鎖 (ボルツマンマシン)&lt;ul&gt;
&lt;li&gt;エネルギー関数&lt;/li&gt;
&lt;li&gt;モンテカルロ法　異なるモードを扱えない　高次元でうまく行かない&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;密度関数が暗示的&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;マルコフ連鎖 &lt;a href="https://arxiv.org/pdf/1503.05571.pdf"&gt;GSN (Generative Stochastic Network)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;GAN の要件&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;潜在コードを使う&lt;/li&gt;
&lt;li&gt;漸近的に一貫性がある&lt;ul&gt;
&lt;li&gt;訓練データが無限にあれば、真の分布を再現できる&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;マルコフ連鎖を使わない&lt;/li&gt;
&lt;li&gt;高クオリティのサンプル&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;GAN の仕組み&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2つの「敵対的」なモデル&lt;ol&gt;
&lt;li&gt;「生成モデル」G&lt;ul&gt;
&lt;li&gt;実際にサンプルを生成&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;「識別モデル」D&lt;ul&gt;
&lt;li&gt;訓練した後は不要&lt;/li&gt;
&lt;li&gt;サンプルを見て、それが本物か偽物かを判断&lt;/li&gt;
&lt;li&gt;普通は、深層ニューラルネットで実装&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;訓練&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;訓練データのサンプル x に対して1を出力&lt;/li&gt;
&lt;li&gt;ノイズ z からサンプル x ~ G(z) を生成&lt;/li&gt;
&lt;li&gt;D は、D(G(z)) を 0 にするように学習&lt;/li&gt;
&lt;li&gt;G は、D(G(z)) を 1 にするように学習&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;例え：偽札業者と警察&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;偽札業者は警察に見破られないように学習&lt;/li&gt;
&lt;li&gt;警察は、偽札を見破るように（ただし、本物を偽札と判断しないように）学習&lt;/li&gt;
&lt;li&gt;結果として、偽札がどんどん本物に近づく&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;生成モデル&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;z (潜在変数) → x (観測変数)&lt;/li&gt;
&lt;li&gt;G(z; Θ)&lt;/li&gt;
&lt;li&gt;微分可能であること以外に、特に条件が無い&lt;/li&gt;
&lt;li&gt;訓練データを再現するためには、z の次元 &amp;gt; x の次元&lt;ul&gt;
&lt;li&gt;x 内の多用体だけを学習しないように&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;訓練の過程&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SGD 的な最適化アルゴリズム (Adam)を、以下の２つのミニバッチに対して走らせる&lt;ol&gt;
&lt;li&gt;訓練データのミニバッチ&lt;/li&gt;
&lt;li&gt;生成サンプルのミニバッチ&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;損失関数&lt;ul&gt;
&lt;li&gt;生成モデルのコストは識別モデルの逆&lt;/li&gt;
&lt;li&gt;単一の値を、識別モデルは最大化しようとし、生成モデルは最小化しようとする → Minimax Game&lt;/li&gt;
&lt;li&gt;第1項：log D(x) の最大化, 第2項：log D(G(z)) の最小化&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;問題：D(x) の解は？&lt;ul&gt;
&lt;li&gt;D(x) = p_data(x) + (p_data(x) + p_model(x))&lt;/li&gt;
&lt;li&gt;教師あり学習を使い、この比を求める&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;質問：生成モデルがいつも同じ、リアルな画像を生成しないようにするにはどうすれば？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Minimax ゲームがうまく動いていれば、D はそのような画像を偽物だと却下するはず&lt;/li&gt;
&lt;li&gt;実際には、最適化が鞍点に収束することは稀　あまり多様性の無い画像ばかりが生成され、失敗することも&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;質問：GAN と VAE は似ている。どういう場合に使い分ければ？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;尤度を最大化することだけを考えれば、VAE&lt;/li&gt;
&lt;li&gt;リアルなサンプルを生成するためには、GAN&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;質問：データからどうサンプルする？一様分布？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;単純な方法：x と z 両方とも、一様にサンプリング&lt;/li&gt;
&lt;li&gt;Importance sampling が使えるかも&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;質問：生成されたサンプルに不自然な部分は現れないのか&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;現れる。Convolutional 生成モデルを使うと、チェッカー模様のようなものが現れる。&lt;/li&gt;
&lt;li&gt;MNISTの例：低レイヤーのパターンを見ると、高周波数成分を検出するフーリエ基底に似ている。それよりも、筆画などを検出・生成するモデルを学習したほうが効率的&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;質問：Negative sampling (例：word2vec) との関係は？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ボルツマンマシンの negative phase&lt;/li&gt;
&lt;li&gt;GAN は、ほとんどが「negative」のフェーズ。例えるなら、大理石から像を彫る時、基本的にずっと石を削り続ける&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;訓練のバリエーション&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;各プレイヤーが、独立の損失関数&lt;/li&gt;
&lt;li&gt;識別モデルの損失は、Minimax バージョンと同じ。生成モデルは、log D(G(z)) を最大化&lt;/li&gt;
&lt;li&gt;識別モデルが非常に賢くなったとき、生成モデルの勾配が消える&lt;/li&gt;
&lt;li&gt;この損失関数をデフォルトで使うべき&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;DCGAN (deep convolutional GAN) アーキテクチャ&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;どうやって高解像度の画像を生成するか&lt;/li&gt;
&lt;li&gt;1 より大きい stride を使う&lt;/li&gt;
&lt;li&gt;バッチ正規化を使う（最後のレイヤー以外）&lt;/li&gt;
&lt;li&gt;リアルな寝室の画像&lt;/li&gt;
&lt;li&gt;人の顔&lt;ul&gt;
&lt;li&gt;潜在空間内での演算&lt;/li&gt;
&lt;li&gt;例：メガネをした男性の顔 ー 男性の顔 ＋ 女性の顔 ＝ メガネをした女性の顔&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;言語モデルの (king - man + woman = queen) に似ている&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ダイバージェンスは重要か？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;最尤推定 → KLダイバージェンス D_KL(p|q) の最小化 モードが平均化されてしまう&lt;/li&gt;
&lt;li&gt;D_KL(q|p) → モデルが一つのモードを選ぶ （サンプルが無いところに確率値を与えない）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;教科学習としてとらえる&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;生成モデルが報酬をもとに学習&lt;/li&gt;
&lt;li&gt;ただし、「環境」は固定ではなく識別モデル&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;生成モデルの損失関数の比較　&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;"non-saturating heuristics" → 識別モデルの値が 0 (偽物を正しく偽物と判断している状態)でも勾配が0ではない&lt;/li&gt;
&lt;li&gt;最尤推定コスト → 識別モデルの値が 1 に近づくにつれ、指数関数的に大きくなる → ミニバッチ内でもっともリアルなサンプルに大きく影響されてしまう&lt;/li&gt;
&lt;li&gt;GAN のサンプルがリアルなのは、損失関数というよりは、教師あり学習を使った近似戦略によるところが大きい&lt;/li&gt;
&lt;li&gt;VAE の場合は、同心円ガウシアン分布で近似　GAN は、そのような制約がない（共分散行列が対角でなくても良い）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;NCE (noise contrastive estimation), MLE との比較&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;NCE は、D として、明示的な（パラメータ化された）近似を学習する&lt;/li&gt;
&lt;li&gt;ノイズはガウス分布から生成（生成モデルを学習しない）&lt;/li&gt;
&lt;li&gt;質問 (Jürgen Schmidhuber): predictability minimization (1992) との関係は？同じアイデアでは？&lt;ul&gt;
&lt;li&gt;NIPS の論文に書いたのでそちらを参照。&lt;/li&gt;
&lt;li&gt;公共の場でこのように対決するのはあまり望まない&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;NCE は、predictability minimization よりも GAN に似てる (笑)&lt;/li&gt;
&lt;li&gt;NCE は、学習が識別モデルで起こる。GAN は生成モデルで起こる&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;GAN を使う上でのコツ&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;ラベルは役に立つ&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;条件付き GAN p(x) を学習するより p(y|x) を学習した方が良いサンプルが作れる&lt;/li&gt;
&lt;li&gt;p(x, y) を学習するだけでも、 p(x) よりも良い&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;片側ラベルスムージング&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;データのクロスエントロピーを計算するときに、ソフトな値 (0.9) を出力&lt;/li&gt;
&lt;li&gt;「間違い確率」をモデル化　データの中に誤り・ノイズが混入&lt;/li&gt;
&lt;li&gt;負ラベルに対してスムージングしないこと！&lt;/li&gt;
&lt;li&gt;正規化として使える&lt;/li&gt;
&lt;li&gt;Weight decay と違って、精度を犠牲にしない（単に確信度を下げるだけ）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;バッチ正規化&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;素性のバッチ内の平均を引き、標準偏差で割る&lt;/li&gt;
&lt;li&gt;バッチ内での相関を引き起こす&lt;ul&gt;
&lt;li&gt;参照バッチ → この参照バッチにおける平均と標準偏差をいつも使う&lt;/li&gt;
&lt;li&gt;ただし、参照バッチに対して過学習する&lt;ul&gt;
&lt;li&gt;→ 仮想バッチ 参照バッチに正規化したいサンプル x を加えたバッチを「参照」として使う&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;G と D のバランス&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;普通は、識別モデルが勝つ&lt;/li&gt;
&lt;li&gt;識別モデルモデルが強すぎて生成モデルが学習しない時&lt;ul&gt;
&lt;li&gt;誤：識別モデルを弱くする&lt;/li&gt;
&lt;li&gt;正：スムージングや、non-saturating heuristics を使う&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;識別モデルをより頻繁に更新する（いつも上手くいくとは限らない）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;研究のフロンティア&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;非収束問題&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GAN は、収束の十分条件を満たさない&lt;/li&gt;
&lt;li&gt;収束そのものが目的ではない&lt;/li&gt;
&lt;li&gt;密度関数の凸性を使い、関数を直接最適化できれば、GAN の収束性を保証できる&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;質問：ビデオに対する GAN (VGAN) 重複したサンプルを生成しないような保証があるか？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;起きないという保証はない&lt;/li&gt;
&lt;li&gt;生成モデルは一度も訓練データを見ることがない。識別モデルの勾配を通じて学習するだけ&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;質問：生の入力より、何らかの疎な表現 (基底や辞書) を学習した方が良いのでは？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;深層学習としては、素性エンジニアリングはなるべく避けたい&lt;/li&gt;
&lt;li&gt;出力層の pre-training&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;質問：GAN を使って訓練データを増やし、他のタスクの入力とする？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;自分の前のインターンが似たことをやっていた&lt;/li&gt;
&lt;li&gt;GAN を使って合成データを生成し、そこから分類器を学習&lt;/li&gt;
&lt;li&gt;本当のテストセットに適用&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;モード崩壊&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;生成モデルが、単一のモードしか生成しなくなってしまう&lt;/li&gt;
&lt;li&gt;逆 KL 損失関数のせいではない&lt;/li&gt;
&lt;li&gt;出力サンプルに多様性が無くなる。キャプションからの画像生成等では通常問題ない&lt;/li&gt;
&lt;li&gt;ミニバッチ素性&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;サンプルの問題&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;畳み込み（個数に関する制約がない） → 頭が２つある犬&lt;/li&gt;
&lt;li&gt;長距離の依存性に関する問題　→　目のサイズが違う、足のない犬&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Unrolled GAN&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;識別モデルの k ステップを遡って、誤差逆伝播&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;評価&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;サンプルの良さを定量化する方法が無い&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;離散的な出力&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;微分可能の条件　→　単語や文字を出力できない&lt;/li&gt;
&lt;li&gt;REINFORCE&lt;/li&gt;
&lt;li&gt;埋め込みに変換するモデルを学習し、GAN を学習&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;教師あり識別モデル&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;本物・偽物　の代わり→　本物の犬・本物の猫・偽物に分類&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;コードを解釈可能にする&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;強化学習とのつながり&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ゲーム、セキュリティへの適用&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;質問：損失関数が微分可能でない時は？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;上の「離散的な出力」を参照&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;質問：モードを捉えることが得意。画像サイズが大きくなった場合、モードが指数関数的に増える&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ただ、大きなモデルは指数関数的に多くのモードを捉えられる。&lt;/li&gt;
&lt;li&gt;例：解像度を上げると、動物の毛皮が見えるが、画像全体に渡って繰り返されるだけ&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;GAN と他のモデルとの組み合わせ&lt;ul&gt;
&lt;li&gt;PPGN の紹介&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;まとめ&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;教師あり学習を使い、扱いにくい損失関数を近似&lt;/li&gt;
&lt;li&gt;最尤推定に使われるものも含む、多くの損失関数を近似できる&lt;/li&gt;
&lt;li&gt;重要な研究課題：どうやってナッシュ均衡を探すか&lt;/li&gt;
&lt;li&gt;高画質画像生成の重要な要素&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;英語&lt;/th&gt;
&lt;th&gt;日本語&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;mode&lt;/td&gt;
&lt;td&gt;最頻値（ここでは、単にサンプルの密度が高い部分）&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;contrast&lt;/td&gt;
&lt;td&gt;対比する&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;tractable&lt;/td&gt;
&lt;td&gt;扱いやすい&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;semi-supervised&lt;/td&gt;
&lt;td&gt;半教師あり&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;chain rule&lt;/td&gt;
&lt;td&gt;連鎖律&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;asymptotically&lt;/td&gt;
&lt;td&gt;漸近的に&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;differentiable&lt;/td&gt;
&lt;td&gt;微分可能な&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;counterfeit&lt;/td&gt;
&lt;td&gt;偽物、偽造&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;manifold&lt;/td&gt;
&lt;td&gt;多様体&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;artifact&lt;/td&gt;
&lt;td&gt;人為的な結果、不自然さ&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;advocate&lt;/td&gt;
&lt;td&gt;提唱する&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;probability mass&lt;/td&gt;
&lt;td&gt;確率質量&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;saturate&lt;/td&gt;
&lt;td&gt;飽和する（値がそれ以上変化しないこと）&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;equilibrium&lt;/td&gt;
&lt;td&gt;均衡&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;oscillation&lt;/td&gt;
&lt;td&gt;振動&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;workaround&lt;/td&gt;
&lt;td&gt;回避方法&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;</content></entry><entry><title>ランディ・パウシュ氏による「最後の」タイムマネジメント講義 まとめ</title><link href="http://englishforhackers.com/time-management-by-randy-pausch.html" rel="alternate"></link><published>2018-02-15T00:00:00-05:00</published><updated>2018-02-15T00:00:00-05:00</updated><author><name>Masato Hagiwara</name></author><id>tag:englishforhackers.com,2018-02-15:/time-management-by-randy-pausch.html</id><summary type="html">&lt;p&gt;膵臓がんで余命３〜６ヶ月であることを医師から宣告された後の&lt;a href="https://www.youtube.com/watch?v=ji5_MqicxSo"&gt;「最後の授業」で有名なランディ・パウシュ教授&lt;/a&gt;が、バージニア大学で行った「タイムマネジメント」の講義。&lt;/p&gt;
&lt;p&gt;一つ一つのテクニック等は、他の本などにも載っているたりするものが多いですが、ランディ・パウシュ教授が、残された限られた時間の中で執り行った講義なので、説得力があります。
話も上手く面白くので、一見の価値ありです。&lt;/p&gt;
&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/oTugjssqOT0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen&gt;&lt;/iframe&gt;

&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;はじめに&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;バージニア大学&lt;/li&gt;
&lt;li&gt;膵臓がんで余命３〜６ヶ月であることを医師から宣告される（３ヶ月前）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;あらすじ&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;実用的なトーク&lt;/li&gt;
&lt;li&gt;ゴールをどう設定するか&lt;/li&gt;
&lt;li&gt;時間の無駄遣いをどう防ぐか&lt;/li&gt;
&lt;li&gt;上司・指導教官とどう接するか&lt;/li&gt;
&lt;li&gt;人にどう委譲するか&lt;/li&gt;
&lt;li&gt;生産性を上げるツールやテクニック&lt;/li&gt;
&lt;li&gt;ストレスと先延ばし&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;時間とお金&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;アメリカ社会は、お金に比べ、時間をうまく使うのが苦手&lt;/li&gt;
&lt;li&gt;自分の１時間の価値は？会社員なら、自分を雇うために年収の倍程度のお金がかかる&lt;/li&gt;
&lt;li&gt;時間を、お金と同じ物と見なして使うこと&lt;/li&gt;
&lt;li&gt;時間は二度と取り戻せない。お金は再度稼げる&lt;/li&gt;
&lt;li&gt;「時間の飢饉」食料を空輸してもアフリカの食糧問題は解決しない。根本的・システム的な改革が必要&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;本の推薦&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.amazon.com/Minute-Manager-Kenneth-Blanchard-Ph-D/dp/074350917X"&gt;"The One Minute …&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;p&gt;膵臓がんで余命３〜６ヶ月であることを医師から宣告された後の&lt;a href="https://www.youtube.com/watch?v=ji5_MqicxSo"&gt;「最後の授業」で有名なランディ・パウシュ教授&lt;/a&gt;が、バージニア大学で行った「タイムマネジメント」の講義。&lt;/p&gt;
&lt;p&gt;一つ一つのテクニック等は、他の本などにも載っているたりするものが多いですが、ランディ・パウシュ教授が、残された限られた時間の中で執り行った講義なので、説得力があります。
話も上手く面白くので、一見の価値ありです。&lt;/p&gt;
&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/oTugjssqOT0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen&gt;&lt;/iframe&gt;

&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;はじめに&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;バージニア大学&lt;/li&gt;
&lt;li&gt;膵臓がんで余命３〜６ヶ月であることを医師から宣告される（３ヶ月前）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;あらすじ&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;実用的なトーク&lt;/li&gt;
&lt;li&gt;ゴールをどう設定するか&lt;/li&gt;
&lt;li&gt;時間の無駄遣いをどう防ぐか&lt;/li&gt;
&lt;li&gt;上司・指導教官とどう接するか&lt;/li&gt;
&lt;li&gt;人にどう委譲するか&lt;/li&gt;
&lt;li&gt;生産性を上げるツールやテクニック&lt;/li&gt;
&lt;li&gt;ストレスと先延ばし&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;時間とお金&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;アメリカ社会は、お金に比べ、時間をうまく使うのが苦手&lt;/li&gt;
&lt;li&gt;自分の１時間の価値は？会社員なら、自分を雇うために年収の倍程度のお金がかかる&lt;/li&gt;
&lt;li&gt;時間を、お金と同じ物と見なして使うこと&lt;/li&gt;
&lt;li&gt;時間は二度と取り戻せない。お金は再度稼げる&lt;/li&gt;
&lt;li&gt;「時間の飢饉」食料を空輸してもアフリカの食糧問題は解決しない。根本的・システム的な改革が必要&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;本の推薦&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.amazon.com/Minute-Manager-Kenneth-Blanchard-Ph-D/dp/074350917X"&gt;"The One Minute Manager"&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.amazon.com/dp/B00GOZV3TM/"&gt;"The Seven Habits of Highly Effective People"&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ゴール、優先度&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;オフィスで仕事をする人は１日２時間を浪費する&lt;/li&gt;
&lt;li&gt;「なぜそのタスクをするか」「そのタスクをしなかったら何が起こるか」&lt;/li&gt;
&lt;li&gt;８０：２０ ルール → 人生のたった２０％のことが、８０％のアウトプットに寄与している&lt;/li&gt;
&lt;li&gt;Lou Holtz の「人生で成し遂げたい１０７のリスト」&lt;/li&gt;
&lt;li&gt;Walt Disney 「夢に描くことができれば、それを達成できる」はじめのディスニーランドを366日で完成させた&lt;/li&gt;
&lt;li&gt;計画を立てることの重要性　後から変えてもよい&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;To-do リスト&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;小さなステップに分解する&lt;ul&gt;
&lt;li&gt;部屋を片付ける → ベッドを整える　服を片付ける  etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;一番嫌なタスクから始める&lt;ul&gt;
&lt;li&gt;複数のカエルを食べるなら、一番大きいものから&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;重要度・緊急度マトリックス（重要！）&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;「重要＋緊急でない」タスクを「重要でない＋緊急」タスクより優先&lt;/li&gt;
&lt;li&gt;タスクが「緊急」になる前に片付けられる&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ツール&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;机を綺麗に！　書類を一つずつ片付ける　各書類には一度しか触れない&lt;/li&gt;
&lt;li&gt;Email にも同じ原則を適用。受信箱は To-do リストではない&lt;/li&gt;
&lt;li&gt;整理システム　書類をアルファベット順に格納&lt;/li&gt;
&lt;li&gt;マルチモニターにしない理由はない　低コストで大きな生産性の向上&lt;/li&gt;
&lt;li&gt;ツールを使ってスケジュールを管理　自分で覚えようとしない&lt;/li&gt;
&lt;li&gt;電話&lt;ul&gt;
&lt;li&gt;ハンズフリー電話　保留中に別のことができる&lt;/li&gt;
&lt;li&gt;時間を無駄にしやすい　立ちながら電話する&lt;/li&gt;
&lt;li&gt;通話の最初に目的を明らかにする&lt;/li&gt;
&lt;li&gt;次にやりたいことを机の上に置いておく&lt;/li&gt;
&lt;li&gt;セールスの電話　自分が話している間に切る&lt;/li&gt;
&lt;li&gt;ランチの前 (11:50) に電話する → 10分で終わる&lt;/li&gt;
&lt;li&gt;ヘッドセット　運動・通勤しながら電話ができる！&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;小さな「ありがとう」の手紙を書く&lt;/li&gt;
&lt;li&gt;書類の「ゴミ箱」　コンピューターのように、ゴミ箱から書類を取り出せるようにしておく&lt;/li&gt;
&lt;li&gt;オフィスを、自分に心地よく、訪問客に対しては心地よくなくする&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;スケジュール&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;時間を見つけるのではなく、作る&lt;/li&gt;
&lt;li&gt;「機会費用」を意識　あまり重要でないことをする代償　そのことをしたことそのものではなく、他のことに時間を使えないこと&lt;/li&gt;
&lt;li&gt;断る&lt;/li&gt;
&lt;li&gt;「引っ越しパーティー」　荷物の量を見積もり、人数が足りている場合だけ行く&lt;/li&gt;
&lt;li&gt;自分の時間を何が何でも守る&lt;/li&gt;
&lt;li&gt;スイッチングコストを意識する　５回割り込みされると１時間を浪費&lt;/li&gt;
&lt;li&gt;「時間日記」　一日のそれぞれの時点で、自分が何をしていたかを記録する&lt;/li&gt;
&lt;li&gt;特に学生へ：「ニセ授業」をスケジュールに入れる　授業の間に時間が出来た場合は、ニセ授業をスケジュールに入れ、図書館へ行く。&lt;/li&gt;
&lt;li&gt;既婚・子供がいる大学院生の方が早く卒業することも（生産性を意識するから）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;先延ばし&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;「怠けている」のが先延ばしにする唯一の理由ではない&lt;/li&gt;
&lt;li&gt;締め切り直前になって何かをするのは非常に効率が悪い (Fedex が儲かる理由)&lt;/li&gt;
&lt;li&gt;「ニセ締め切り」を設定して、それを守る&lt;/li&gt;
&lt;li&gt;「お願いする」ことの重要性　尻込みする前にまずは聞いてみる&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;委譲&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;正しい委譲の仕方：権力 (予算、リソース) と責任を同時に与える&lt;/li&gt;
&lt;li&gt;間違った委譲の仕方：決定権を与えず仕事を押し付けるだけ。&lt;/li&gt;
&lt;li&gt;汚い仕事 (例：掃除など) は自分でやる。&lt;/li&gt;
&lt;li&gt;仕事が達成できた・できなかった場合の報酬と罰を明らかにする&lt;/li&gt;
&lt;li&gt;メールなど、書面に残す&lt;/li&gt;
&lt;li&gt;良い行いを褒める&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ミーティング&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;電話・コンピュータを使わない。アジェンダを用意&lt;/li&gt;
&lt;li&gt;決定事項とやるべきこと（誰が、いつまで）を書き、メールで参加者に送る&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;テクノロジー&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;何かを早くするだけではなく、人間の方法を根本的に変えてしまう時に使う&lt;/li&gt;
&lt;li&gt;メールを削除するな！&lt;/li&gt;
&lt;li&gt;何かを頼みたければ、５人に同時送信しない。１人に直接お願いする&lt;/li&gt;
&lt;li&gt;48時間以内に返信が無ければ、ずっと無いと思え　→　しつこくお願いしろ&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;他のアドバイス&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;上司&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;次のミーティングまでに何をしなければいけないか&lt;/li&gt;
&lt;li&gt;上司の他、誰に頼ればよいか&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;休暇&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;他の誰かに委譲するか、休暇から帰るまで待ってもらう&lt;/li&gt;
&lt;li&gt;メールをチェックしていたら休暇とは言えない&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;テレビ&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;平均１週間のうち28時間をテレビを見て過ごす&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;約束&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;絶対守る&lt;/li&gt;
&lt;li&gt;守れそうにない場合は、あらかじめ連絡し、再度調整してもらう&lt;/li&gt;
&lt;li&gt;ほとんどのものは、合格・不合格ではない。「火曜日にできそうにありません。木曜でも良いですか」&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;30日後に、この講義をもう一度見る&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;30日前と、何が変わったか&lt;/li&gt;
&lt;li&gt;変わったのなら、愛する人と時間をもっと過ごそう&lt;/li&gt;
&lt;li&gt;時間が思ったほど無かったと気づいてからでは、もう遅い&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;英語&lt;/th&gt;
&lt;th&gt;日本語&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;pancreatic cancer&lt;/td&gt;
&lt;td&gt;膵臓がん&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;procrastination&lt;/td&gt;
&lt;td&gt;先延ばしにすること&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;equate&lt;/td&gt;
&lt;td&gt;同一視する&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;platitude&lt;/td&gt;
&lt;td&gt;陳腐なきれい事&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;famine&lt;/td&gt;
&lt;td&gt;飢饉&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;compulsive&lt;/td&gt;
&lt;td&gt;脅迫的な、衝動的な&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;note (v.)&lt;/td&gt;
&lt;td&gt;気づく、〜に注意する&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;dissonant&lt;/td&gt;
&lt;td&gt;調和していない&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;delegation&lt;/td&gt;
&lt;td&gt;委譲&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;toofer&lt;/td&gt;
&lt;td&gt;一石二鳥&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;idiosyncratic&lt;/td&gt;
&lt;td&gt;変わった、多種多用な&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;</content></entry><entry><title>「深層学習と自然言語処理」オックスフォード大/DeepMind 講義まとめ（用語集付き）</title><link href="http://englishforhackers.com/oxford-cs-deepnlp-2017-summary.html" rel="alternate"></link><published>2018-02-08T00:00:00-05:00</published><updated>2018-02-08T00:00:00-05:00</updated><author><name>Masato Hagiwara</name></author><id>tag:englishforhackers.com,2018-02-08:/oxford-cs-deepnlp-2017-summary.html</id><summary type="html">&lt;p&gt;&lt;img alt="「深層学習と自然言語処理」オックスフォード大/DeepMind 講義" src="images/oxford-deepnlp-lectures.png"&gt;&lt;/p&gt;
&lt;p&gt;オックスフォード大の「深層学習と自然言語処理」(&lt;a href="https://github.com/oxford-cs-deepnlp-2017/lectures"&gt;Oxford Deep NLP 2017 course&lt;/a&gt;)の講義メモです。&lt;/p&gt;
&lt;p&gt;講義ビデオ、スライド、講義の詳細等については、&lt;a href="https://github.com/oxford-cs-deepnlp-2017/lectures"&gt;講義の公式ページ&lt;/a&gt;を参照してください。&lt;/p&gt;
&lt;h2&gt;目次&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#lecture1a"&gt;講義1a: 導入 (Phil Blunsom)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#lecture1b"&gt;講義1b: 深層ニューラルネットは友だち (Wang Ling)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#lecture2a"&gt;講義2a: 単語レベルの意味 (Ed Grefenstette)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#lecture2b"&gt;講義2b: 実習の概要 (Chris Dyer)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#lecture3"&gt;講義3: 言語モデルとRNN パート1 (Phil Blunsom)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#lecture4"&gt;講義4: 言語モデルとRNN パート2 (Phil Blunsom)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#lecture5"&gt;講義5: テキスト分類 (Karl Moritz Hermann)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#lecture7"&gt;講義6: Nvidia の GPU を使った深層学習 …&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;p&gt;&lt;img alt="「深層学習と自然言語処理」オックスフォード大/DeepMind 講義" src="images/oxford-deepnlp-lectures.png"&gt;&lt;/p&gt;
&lt;p&gt;オックスフォード大の「深層学習と自然言語処理」(&lt;a href="https://github.com/oxford-cs-deepnlp-2017/lectures"&gt;Oxford Deep NLP 2017 course&lt;/a&gt;)の講義メモです。&lt;/p&gt;
&lt;p&gt;講義ビデオ、スライド、講義の詳細等については、&lt;a href="https://github.com/oxford-cs-deepnlp-2017/lectures"&gt;講義の公式ページ&lt;/a&gt;を参照してください。&lt;/p&gt;
&lt;h2&gt;目次&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#lecture1a"&gt;講義1a: 導入 (Phil Blunsom)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#lecture1b"&gt;講義1b: 深層ニューラルネットは友だち (Wang Ling)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#lecture2a"&gt;講義2a: 単語レベルの意味 (Ed Grefenstette)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#lecture2b"&gt;講義2b: 実習の概要 (Chris Dyer)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#lecture3"&gt;講義3: 言語モデルとRNN パート1 (Phil Blunsom)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#lecture4"&gt;講義4: 言語モデルとRNN パート2 (Phil Blunsom)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#lecture5"&gt;講義5: テキスト分類 (Karl Moritz Hermann)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#lecture7"&gt;講義6: Nvidia の GPU を使った深層学習 (eremy Appleyard)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#lecture7"&gt;講義7: 条件付き言語モデリング (Chris Dyer)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#lecture8"&gt;講義8: アテンションを使った言語生成 (Chris Dyer)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#lecture9"&gt;講義9: 音声認識 (Andrew Senior)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#lecture10"&gt;講義10: 音声合成 (Andrew Senior)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#lecture11"&gt;講義11: 質問応答 (Karl Moritz Hermann)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#lecture12"&gt;講義12: 記憶 (Ed Grefenstette)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#lecture13"&gt;講義13: ニューラルネットにおける言語知識 (Chris Dyer)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr/&gt;

&lt;p&gt;&lt;a name="lecture1a"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;講義1a: 導入&lt;/h2&gt;
&lt;p&gt;講師：Phil Blunsom (オックスフォード大 / DeepMind)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;はじめに&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;AI (人工知能) といえば自然言語&lt;/li&gt;
&lt;li&gt;言語は、コミュニケーションだけではなく、概念を表現するために使われる&lt;/li&gt;
&lt;li&gt;どうやって人間が言語を学習するかはまだよく分かっていない&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;教科書&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;最新の研究を扱うため、教科書を使わない。ただし、必要に応じて以下を参考：&lt;ul&gt;
&lt;li&gt;深層学習: Goodfellow, Bengio, Courville: &lt;a href="http://www.deeplearningbook.org/"&gt;Deep Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;機械学習: &lt;a href="https://mitpress.mit.edu/books/machine-learning-0"&gt;Murphy 本&lt;/a&gt; と &lt;a href="http://www.springer.com/us/book/9780387310732"&gt;Bishop 本&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;前提知識&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;数学 (線形代数, 微分積分, 確率)&lt;/li&gt;
&lt;li&gt;機械学習 (評価; 過学習, 一般化, 正則化; ニューラルネットワークの基礎)&lt;/li&gt;
&lt;li&gt;プログラミング (言語やフレームワークは問わない)&lt;/li&gt;
&lt;li&gt;自然言語処理 (NLP) や計算機言語学 (CL) の包括的なコースでははない&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;タスク&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;言語理解&lt;ul&gt;
&lt;li&gt;CNN のニュース記事を与え、質問に答える。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;変換 (transduction) タスク。系列から系列への変換。&lt;ul&gt;
&lt;li&gt;音声認識：音声 → テキスト&lt;/li&gt;
&lt;li&gt;機械翻訳：テキスト(言語X) → テキスト(言語Y)&lt;/li&gt;
&lt;li&gt;音声合成：テキスト → 音声&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;画像理解&lt;ul&gt;
&lt;li&gt;「この男性の視力は 2.0 か？」&lt;ul&gt;
&lt;li&gt;視力とメガネに関する知識を持っていないければいけない&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;言語構造&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;多義性: "I saw her duck" (注: "duck"は「アヒル」という意味と「身をかがめる」という意味がある)&lt;/li&gt;
&lt;li&gt;慣用句: "kick the bucket" →　「死ぬ」&lt;/li&gt;
&lt;li&gt;照応: ボールは箱の中に入らなかった。それは[大きすぎる/小さすぎる]からだ。&lt;ul&gt;
&lt;li&gt;「それ」は箱か、ボールか。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;英語&lt;/th&gt;
&lt;th&gt;日本語&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;compelling&lt;/td&gt;
&lt;td&gt;人を引きつける&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;prerequisite&lt;/td&gt;
&lt;td&gt;前提知識&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;comprehensive&lt;/td&gt;
&lt;td&gt;包括的な&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;20/20 vision&lt;/td&gt;
&lt;td&gt;（日本での）視力 1.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;duck (a.)&lt;/td&gt;
&lt;td&gt;アヒル&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;duck (v.)&lt;/td&gt;
&lt;td&gt;しゃがむ&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;kick the bucket&lt;/td&gt;
&lt;td&gt;死ぬ&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr/&gt;

&lt;p&gt;&lt;a name="lecture1b"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;講義1b: 深層ニューラルネットは友だち&lt;/h2&gt;
&lt;p&gt;講師：Wang Ling (DeepMind)&lt;/p&gt;
&lt;p&gt;&lt;em&gt;パート1&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;数字&lt;/li&gt;
&lt;li&gt;変数&lt;/li&gt;
&lt;li&gt;演算子&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;関数 (入力, 出力)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;翻訳, 囲碁の手の推定, 画像分類もある種の関数&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;関数をどう推定するか&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;パラメータを使ってモデル化し、入力と出力からパラメータを推定。どう推定するか？&lt;/li&gt;
&lt;li&gt;パラメータの「仮説」を立て、そのパラメータを使った出力と実際の出力とを比べる。&lt;/li&gt;
&lt;li&gt;仮説の「良さ」  →  損失関数によって定義  (例: 二乗損失)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;最適化&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;損失関数を最小化する&lt;/li&gt;
&lt;li&gt;単純な最適化：損失関数が下がる方向に、現在のパラメータを 1 ずつ動かしていく&lt;/li&gt;
&lt;li&gt;最適解を見つけられるとは限らない。「検索問題」と呼ばれる。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;最適化の改善&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ステップサイズを小さくする (例: 1 の代わりに 0.1 ずつ動かす)&lt;/li&gt;
&lt;li&gt;ステップサイズを少しずつ小さくしていく&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;このアプローチを画像分類に適用すると？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;画像全体が入力。1ピクセル = 1変数&lt;/li&gt;
&lt;li&gt;モデルが大きすぎる&lt;/li&gt;
&lt;li&gt;サンプル数が多すぎる&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;勾配！&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;パラメータを動かすベクトルを仮定し、移動した距離あたりの損失の減少幅を仮定 → limit → 損失関数の微分!&lt;/li&gt;
&lt;li&gt;コスト関数自体の代わりに、微分を計算すれば良い&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;最急降下法&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;現在のパラメータから、r = 損失関数の微分（単位距離あたりの損失減少）を計算&lt;/li&gt;
&lt;li&gt;r * a (学習率)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;パート2 - 深層学習入門&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;現実のモデルは非線形&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;例：２つの線形モデルの組み合わせ　あるしきい値 (例: x = 6) から線形モデルが切り替わる&lt;/li&gt;
&lt;li&gt;一つの線形モデルでは学習が足りない (underfitting)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;モデルの組み合わせ&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;シグモイド関数 をモデル重みに使う y = (w1 x + b1) s1 + (w2 x + b2) s2&lt;/li&gt;
&lt;li&gt;s = σ(wx + b)&lt;/li&gt;
&lt;li&gt;wを大きくするとほぼステップ関数になる&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;多層パーセプトロン&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;例：３つの線形モデルの組み合わせは？「not s1 and not s3」をどう表現する？&lt;/li&gt;
&lt;li&gt;最初のレイヤーは境界条件を学習&lt;/li&gt;
&lt;li&gt;次のレイヤーは範囲 (論理和や論理積) を学習、etc.&lt;/li&gt;
&lt;li&gt;３層を使うと XOR を学習できる → 十分の層とパラメータがあれば、任意の関数を近似できる&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;未学習 (underfitting)：モデルの表現力が（データの複雑さに対して）低すぎる&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;過学習 (overfitting)：モデルの表現力が高すぎる&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;データ量が増える → 過学習のリスクが下がる&lt;/li&gt;
&lt;li&gt;正則化 → モデルの複雑さに対するペナルティ → 過学習のリスクが下がる&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;離散値の扱い方&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ルックアップ・テーブルを使う&lt;/li&gt;
&lt;li&gt;連続値の集合で、離散値を表現 → 埋め込み(embedding)&lt;/li&gt;
&lt;li&gt;出力 logit の集合を softmax で確率分布に変換&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr/&gt;

&lt;p&gt;&lt;a name="lecture2a"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;講義2a: 単語レベルの意味&lt;/h2&gt;
&lt;p&gt;講師: Ed Grefenstette (DeepMind)&lt;/p&gt;
&lt;p&gt;単語をどう表現するか？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;自然言語のテキスト：離散的な記号の系列&lt;/li&gt;
&lt;li&gt;単純な表現：one-hot ベクトル&lt;/li&gt;
&lt;li&gt;単語ベクトル&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;問題：スパース、単語同士が直交、意味を扱えない&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;分布類似度&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;単語の意味を文脈によって表現　密なベクトル&lt;/li&gt;
&lt;li&gt;頻度ベース、予測ベース、タスクベースの３つの推定手法&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;頻度ベースの手法&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;文脈（単語の左右の w 単語）における他の単語の出現頻度を数える -&amp;gt; ベクトル化&lt;/li&gt;
&lt;li&gt;単語同士の類似度をコサイン類似度によって計算  ベクトルの長さに依存しない&lt;/li&gt;
&lt;li&gt;素性の不平等さ  関連があるから頻度が高いのか、ただ単にその単語の頻度が高いのか  様々な正規化手法(例：TF-IDF, PMI)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ニューラル埋め込みモデル&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;単語-素性の行列を考える&lt;/li&gt;
&lt;li&gt;one-hot ベクトルがあれば、掛け合わせると埋め込みが求まる&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ニューラル埋め込み&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;単語 t に対して、コーパス内の文脈における素性（他の単語） c(t) の出現を数える&lt;/li&gt;
&lt;li&gt;スコア関数を定義&lt;/li&gt;
&lt;li&gt;損失関数を、スコアのコーパス全体の和と定義&lt;/li&gt;
&lt;li&gt;損失関数を最小化&lt;/li&gt;
&lt;li&gt;E が埋め込み&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;良いスコア関数とは？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;単語を埋め込みで表現（当然！）&lt;/li&gt;
&lt;li&gt;t が c(t) によってどのぐらいうまく説明できるかを表現できる&lt;/li&gt;
&lt;li&gt;ある単語が、（その他の単語よりも） 文脈をうまく表現できる&lt;/li&gt;
&lt;li&gt;微分可能&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;C&amp;amp;W モデル (Collobert et al. 2011)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;埋め込み → 畳み込み → 多層パーセプトロン → スコア&lt;/li&gt;
&lt;li&gt;正しい単語のスコア - 間違った（無作為に選んだ単語で置き換えた分）スコアのヒンジ損失を最大化するように学習&lt;/li&gt;
&lt;li&gt;結果として、文脈の表現に関する情報も埋め込みにエンコーディングされるようになる&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;CBoW (Mikolov et al. 2013)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;文脈から単語を予測&lt;/li&gt;
&lt;li&gt;文脈の単語を埋め込み表現、和を計算し、投影 → softmax&lt;/li&gt;
&lt;li&gt;線形で速い 以前は、計算量の高い softmax の代わりに negative sampling&lt;/li&gt;
&lt;li&gt;最近は softmax をそのまま計算&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Skip-gram (Mikolov et al. 2013)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;単語から文脈を予測&lt;/li&gt;
&lt;li&gt;単語の埋め込み → 投影 → softmax → 文脈単語の尤度&lt;/li&gt;
&lt;li&gt;速い！&lt;/li&gt;
&lt;li&gt;深層学習ではない！&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;両手法の比較&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;頻度ベース vs ニューラル埋め込みモデル → 同じ考えを共有&lt;/li&gt;
&lt;li&gt;word2vec == PMI (点相互情報量) 行列の分解 (Levy and Goldberg, 2014)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ニューラル手法の長所&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;実装・学習が容易&lt;/li&gt;
&lt;li&gt;高い並列度&lt;/li&gt;
&lt;li&gt;他の離散的な概念も使える (係り受けや品詞など)&lt;/li&gt;
&lt;li&gt;画像などの連続値文脈にも使える&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;内部的な評価&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;WordSim-333, SimLex-999, アナロジー (有名な例：queen = king - man + woman), 可視化&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;外部的な評価&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;他のタスクの性能を上げるために使う&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;タスクベース&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;埋め込みをニューラルネットの入力として使う&lt;/li&gt;
&lt;li&gt;埋め込み（素性）もニューラルネットのパラメータの一部&lt;/li&gt;
&lt;li&gt;素性表現の学習&lt;/li&gt;
&lt;li&gt;単純な例&lt;ul&gt;
&lt;li&gt;文/文書分類 → Bag of Vectors （埋め込みベクトルの和） → 投影 → softmax&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;タスク依存の表現を学習（感情推定 → 肯定的・否定的な単語）&lt;/li&gt;
&lt;li&gt;単語の意味は、タスクに根付いている（タスクが意味を決める）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;二言語素性 (Herman &amp;amp; Blunsom 2014)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;英語の文 e_i とドイツ語の文 g_i の類似度を最大化させる&lt;/li&gt;
&lt;li&gt;単純な和のかわりに、隣接する単語の間に非線形性を導入 (tanh)&lt;/li&gt;
&lt;li&gt;損失: 差を最小化  0 に縮退しないように、対訳と非対訳との間の差を最大化&lt;/li&gt;
&lt;li&gt;直感：対訳文は、高レベルの意味を共有する&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;まとめ&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;タスクに特有の情報を学習できる  ただし、これが一般的な「意味」を学習している保証はない&lt;ul&gt;
&lt;li&gt;マルチタスク目的関数である程度軽減できる。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;予備学習 (pre-trainig) して固定&lt;ul&gt;
&lt;li&gt;転移学習の一形態 タスク固有の訓練データが少なかったり、語彙のカバー率が小さかったりする時に有用&lt;/li&gt;
&lt;li&gt;タスク固有の訓練データが大きいときには、一般性を犠牲にしても、埋め込みを学習するほうが良い&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;英語&lt;/th&gt;
&lt;th&gt;日本語&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;literature&lt;/td&gt;
&lt;td&gt;過去の文献&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;corrupt&lt;/td&gt;
&lt;td&gt;(データをわざと) 破損させる&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;intrinsic&lt;/td&gt;
&lt;td&gt;内部的な&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;extrinsic&lt;/td&gt;
&lt;td&gt;外部的な&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;salient&lt;/td&gt;
&lt;td&gt;重要な&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr/&gt;

&lt;p&gt;&lt;a name="lecture2b"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;講義2b: 実習の概要&lt;/h2&gt;
&lt;p&gt;講師：Chris Dyer (DeepMind/CMU)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;２種類の実習&lt;ol&gt;
&lt;li&gt;入門実習&lt;/li&gt;
&lt;li&gt;本実習&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;実習1&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;言語の「感知」と「表現」&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;未知語&lt;/li&gt;
&lt;li&gt;トークナイズ ("New York City" は１トークンか？３トークンか)&lt;/li&gt;
&lt;li&gt;大文字・小文字&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;コーパス&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ニュース記事 vs twitter&lt;/li&gt;
&lt;li&gt;Heap's Law: コーパスのサイズが増えるにしたがって語彙のサイズも増える。&lt;/li&gt;
&lt;li&gt;twitter は α が小さい（急激に）増える＋シングルトン（一度しか出現しない単語）が 70%&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;実習1では、表現学習を扱う&lt;/p&gt;
&lt;p&gt;&lt;em&gt;本実習&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;テキスト分類 (e.g., スパムフィルタ)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;自然言語生成 (NLG)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;言語モデリング&lt;/li&gt;
&lt;li&gt;タイポ修正&lt;/li&gt;
&lt;li&gt;条件付き言語モデリング&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;自然言語理解&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;翻訳、要約、チャットボット (+NLG)&lt;/li&gt;
&lt;li&gt;指示理解&lt;/li&gt;
&lt;li&gt;質問応答&lt;/li&gt;
&lt;li&gt;対話インターフェース&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;解析&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;トピックモデリング&lt;/li&gt;
&lt;li&gt;言語解析（例： 形態素解析、構文解析）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;データセット&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;単一のデータセット TED Talks を使う&lt;/li&gt;
&lt;li&gt;単一のデータセットを色々な問題に変換するスキル → 機械学習では重要！&lt;/li&gt;
&lt;li&gt;トピックラベル、タイトル、要約、ビデオ、ビデオとのアラインメント、翻訳&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;入門実習 -&amp;gt; TED Talks から単語埋め込みを学習&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;本実習&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;TED のラベルを予測&lt;/li&gt;
&lt;li&gt;TED のラベルからトークを生成&lt;/li&gt;
&lt;li&gt;TED のトーク翻訳器&lt;/li&gt;
&lt;li&gt;TED のトークから要約を生成&lt;/li&gt;
&lt;li&gt;ある部分を話すのにかかる時間を推定&lt;/li&gt;
&lt;li&gt;聴衆が笑ったかどうかを予測&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ツールキット&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;自動微分 → 速度が重要＋間違いやすい&lt;/li&gt;
&lt;li&gt;静的 (TensorFlow, Theano)  pros: 計算グラフの最適化 cons: 演算が限られている&lt;/li&gt;
&lt;li&gt;動的 (DyNet, PyTorch)  pros: 何でも書ける  cons: 最適化がしにくい&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;英語&lt;/th&gt;
&lt;th&gt;日本語&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;practical&lt;/td&gt;
&lt;td&gt;実習&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;percept&lt;/td&gt;
&lt;td&gt;知覚&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;derivative&lt;/td&gt;
&lt;td&gt;微分&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr/&gt;

&lt;p&gt;&lt;a name="lecture3"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;講義3: 言語モデルとRNN パート1&lt;/h2&gt;
&lt;p&gt;講師：Phil Blunsom&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;言語モデルとは&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;単語の系列に確率を与える&lt;/li&gt;
&lt;li&gt;根本的な問題&lt;ul&gt;
&lt;li&gt;翻訳 → 構文や語順の解消&lt;/li&gt;
&lt;li&gt;音声認識 → 単語チョイスの曖昧性解消&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;歴史&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;戦時の暗号理論 ドイツの Enigma 暗号の解読 → 「ドイツ語らしい」発話&lt;/li&gt;
&lt;li&gt;多くの自然言語処理タスクは、(条件付き)言語モデリングに帰着できる&lt;ul&gt;
&lt;li&gt;例: 翻訳、質問応答、対話&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;言語モデルの基本&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;連鎖律 (Chain rule) を使って、同時分布を条件付き分布の積に分解&lt;/li&gt;
&lt;li&gt;→ 過去の履歴から、次の単語を予測する問題に変換&lt;/li&gt;
&lt;li&gt;大量のデータを簡単に取得できる&lt;/li&gt;
&lt;li&gt;自然言語を理解すること同様に難しい&lt;ul&gt;
&lt;li&gt;例：P(| There she built a ) → 非常に多くの可能性  難しい&lt;/li&gt;
&lt;li&gt;例: P(| Alice went to the beach. There she built a ) → "sand castle" "boat" etc.&lt;/li&gt;
&lt;li&gt;there → the beach, she → Alice の照応関係を理解　かつ、セマンティクスを理解&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;評価&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;クロスエントロピー&lt;ul&gt;
&lt;li&gt;テキストを言語モデルでエンコードする時に必要なビット数&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Perplexity&lt;ul&gt;
&lt;li&gt;2 のクロスエントロピー乗&lt;/li&gt;
&lt;li&gt;各単語を見たときのモデルの「驚き度合い」&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;時系列予測問題&lt;ul&gt;
&lt;li&gt;訓練データとは別のテストデータを使う&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;データ&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;少なくとも 10億単語は必要&lt;/li&gt;
&lt;li&gt;PTB (Penn Treebank)&lt;ul&gt;
&lt;li&gt;小さい&lt;/li&gt;
&lt;li&gt;加工され過ぎ&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Billon Word Corpus&lt;ul&gt;
&lt;li&gt;文をランダムに入れ替えて訓練・テストセットを作成&lt;/li&gt;
&lt;li&gt;「未来と過去を分離」「記事を分離」の２つの原則に反している&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;上の２つとも欠陥があるので本当は使うべきではない&lt;/li&gt;
&lt;li&gt;WikiText datasets&lt;ul&gt;
&lt;li&gt;オススメ&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Nグラムモデル&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;マルコフ仮定&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;最後の k - 1 個だけ見て次を予測。k次マルコフモデル&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;多項分布を求めるのが簡単　スケーラブル&lt;/li&gt;
&lt;li&gt;例：トライグラム P(w3 | w1, w2) = count(w1, w2, w3) / count(w1, w2)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;バックオフ&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;最尤推定が良いとは限らない&lt;/li&gt;
&lt;li&gt;"Oxford Primm's eater" → コーパス中におそらく一度も現れない&lt;/li&gt;
&lt;li&gt;バイグラム確率と補間する&lt;/li&gt;
&lt;li&gt;単純な手法の一つ：線形補間バックオフ&lt;/li&gt;
&lt;li&gt;超巨大データがあれば、割と単純な手法でもうまくいく&lt;/li&gt;
&lt;li&gt;最も一般的な手法 Kneser-Ney&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;言語モデルが難しい理由&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ロングテール&lt;/li&gt;
&lt;li&gt;どんなにコーパスを大きくしても未知語がある&lt;/li&gt;
&lt;li&gt;ルールベースの人工知能がうまく行かない理由でもある&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;長所&lt;ul&gt;
&lt;li&gt;速い&lt;/li&gt;
&lt;li&gt;評価が定数時間&lt;/li&gt;
&lt;li&gt;言語の実際の分布にマッチ&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;短所&lt;ul&gt;
&lt;li&gt;長距離の依存関係を扱えない&lt;/li&gt;
&lt;li&gt;dog/cat などの意味の似た分布や形態論を扱えない&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ニューラルNグラムモデル&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;トライグラムの場合&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;wn-2, wn-1 (one-hot ベクトル)&lt;/li&gt;
&lt;li&gt;直前２単語を入力として、wn の分布を softmax で出力する前向きニューラルネット&lt;/li&gt;
&lt;li&gt;出力の softmax 層は巨大！（入力は one-hot vector なのでそれほどでもない）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;サンプリング&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;直前２単語を入力、出力層の確率をもとに次の単語をサンプル、...&lt;/li&gt;
&lt;li&gt;デコーディング(復号化)の基本&lt;/li&gt;
&lt;li&gt;確率をもとに最大のものを選ぶ&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;学習&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;実際の単語と、その単語の確率との間で損失関数(log probability)を定義&lt;/li&gt;
&lt;li&gt;逆伝搬&lt;/li&gt;
&lt;li&gt;タイムステップごとに展開 (unrolling)&lt;ul&gt;
&lt;li&gt;逆伝播は木構造になる -&amp;gt; 複数CPU/GPU やクラスタ上で分散可能&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;長所&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;未知Nグラムへの一般化&lt;/li&gt;
&lt;li&gt;埋め込みにより、類義語をうまく扱える&lt;/li&gt;
&lt;li&gt;Nグラムモデルより省メモリー（線形素性の場合）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;短所&lt;ul&gt;
&lt;li&gt;nの値に従いパラメータ数が増加&lt;/li&gt;
&lt;li&gt;長距離の依存を扱えない&lt;/li&gt;
&lt;li&gt;言語の実際の分布を保証しない&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;RNN 再帰型ニューラルネット&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;無限の履歴&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;直前の隠れ層 -&amp;gt; 次の隠れ層へのリンク&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;逆伝播&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;展開すると DAG(有向非巡回グラフ) になる&lt;/li&gt;
&lt;li&gt;通常通り誤差逆伝播できる (BPTT; Back Propagation Through Time) 時間軸上の逆伝播&lt;/li&gt;
&lt;li&gt;ただし、各単語や隠れ層は独立に計算できない&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Truncated Back Propagation Through Time&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;逆伝搬を途中で分断する (前向き伝播の時は分断しない)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ミニバッチ化&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;BPTT を使うと、シークエンスがばらばらの長さ（ミニバッチを最長の長さに合わせなければならない）&lt;/li&gt;
&lt;li&gt;TBPTT を使うと、各シークエンスが一定の長さに収まる (GPUで高速化しやすい)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;長所&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;長距離の依存を扱える (翻訳等をするためには必須)&lt;/li&gt;
&lt;li&gt;履歴を隠れ層に圧縮, 依存の長さに応じてパラメータ数が増えない&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;短所&lt;ul&gt;
&lt;li&gt;学習が難しい&lt;/li&gt;
&lt;li&gt;隠れ層のサイズの2乗に従ってメモリが増える&lt;/li&gt;
&lt;li&gt;言語の実際の分布を保証しない&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;バイアス・バリアンスのトレードオフ&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;例：文を覚えて数えるだけの言語モデル → 低バイアス、高バリアンス&lt;/li&gt;
&lt;li&gt;Nグラムモデル：バイアス有り、低バリアンス&lt;/li&gt;
&lt;li&gt;RNN: バイアスを減らす&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;英語&lt;/th&gt;
&lt;th&gt;日本語&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;utterance&lt;/td&gt;
&lt;td&gt;発話&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;inflate&lt;/td&gt;
&lt;td&gt;(数字を)実情以上に大きくする&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;flawed&lt;/td&gt;
&lt;td&gt;欠陥がある&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;power law&lt;/td&gt;
&lt;td&gt;べき乗則&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;amenable&lt;/td&gt;
&lt;td&gt;〜に適している&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;feed&lt;/td&gt;
&lt;td&gt;(ニューラルネットに、実データを)与える&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;unroll&lt;/td&gt;
&lt;td&gt;(RNNを)展開する&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;truncate&lt;/td&gt;
&lt;td&gt;(余分なものを)切り取る&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;esoteric&lt;/td&gt;
&lt;td&gt;難解な&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;strawman&lt;/td&gt;
&lt;td&gt;たたき台&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr/&gt;

&lt;p&gt;&lt;a name="lecture4"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;講義4: 言語モデルとRNN パート2&lt;/h2&gt;
&lt;p&gt;講師：Phil Blunsom&lt;/p&gt;
&lt;p&gt;RNN が Nグラム言語モデルよりも性能が良いのだとしたら、何か長距離の依存関係を捉えられているはずだ → しかし、本当に学習できているか？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;消える（爆発する）勾配問題&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;時間軸上をさかのぼる勾配に何が起きているか？&lt;/li&gt;
&lt;li&gt;zn = 非線形関数の中身&lt;/li&gt;
&lt;li&gt;Vh -&amp;gt; zn を直前の隠れ層 hn-1で偏微分&lt;/li&gt;
&lt;li&gt;誤差関数の h1 偏微分 -&amp;gt; Vh を何回も掛け合わせる&lt;/li&gt;
&lt;li&gt;Vh のスペクトル半径 (固有値) は多くの場合、小さい -&amp;gt; 距離にしたがって勾配が指数関数的に小さくなる&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;解決策&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;２階微分使う (例：LBFGS) (スケールしない)&lt;/li&gt;
&lt;li&gt;勾配が消えないような初期化をする&lt;/li&gt;
&lt;li&gt;根本的な解決策：アーキテクチャを変えてしまうこと！&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;LSTM (Long Short Term Memory)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;セル状態 cn (記憶) を導入&lt;/li&gt;
&lt;li&gt;LSTM のキー：現在のセル状態 = f * 直前のセル状態 + 何か&lt;ul&gt;
&lt;li&gt;注：RNN のような掛け算ではなく、足し算&lt;/li&gt;
&lt;li&gt;ここに非線型性を入れないのが重要&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;何か -&amp;gt; i (入力ゲート) * tanh(入力; 直前の隠れ層)&lt;/li&gt;
&lt;li&gt;f -&amp;gt; 忘却 (forget)&lt;/li&gt;
&lt;li&gt;実装：多くの線形変換をひとまとめにできる&lt;/li&gt;
&lt;li&gt;変種：i のかわりに (1 -f)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Gated Recurrent Unit (GRU)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;セル状態が無く、h だけ&lt;/li&gt;
&lt;li&gt;"ゲート付き和セル (Gated additive cells)" は (初期化などを工夫しなくても)うまくいく&lt;/li&gt;
&lt;li&gt;機械翻訳、音声認識、音声合成はだいたい LSTM&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;深い RNN に基づく言語モデル&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;縦方向に長くする -&amp;gt; 時刻 t で複数の隠れ層 (記憶が増える)&lt;/li&gt;
&lt;li&gt;横方向に長くする -&amp;gt; Recurrent Highway Network&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;スケーリング&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ニューラル言語モデルの計算量 -&amp;gt; 語彙のサイズに大きな影響を受ける&lt;/li&gt;
&lt;li&gt;特に最後の softmax 層&lt;/li&gt;
&lt;li&gt;short-list → 高頻度語だけニューラルモデルを使う。他はn-gram -&amp;gt; ニューラルモデルの利点を殺してしまう&lt;/li&gt;
&lt;li&gt;Batch local short-list -&amp;gt; バッチ内の語彙だけを使う。乱暴な近似＋不安定&lt;/li&gt;
&lt;li&gt;勾配を近似 -&amp;gt; softmax を exp で置き換え、分母を違うパラメータで置き換える&lt;/li&gt;
&lt;li&gt;NCE (Noise Contrastive Estimation)&lt;ul&gt;
&lt;li&gt;データが本当の分布から来ているかノイズのある分布から来ているかの二値分類器&lt;/li&gt;
&lt;li&gt;訓練時間を削減&lt;/li&gt;
&lt;li&gt;テスト時には通常の softmax を計算するので、速くならない&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Important Sampling (IS)&lt;ul&gt;
&lt;li&gt;本当の単語とノイズのあるサンプルの間の多値分類器&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;要素に分解 (factorization)&lt;ul&gt;
&lt;li&gt;Brown クラスタリング等を使ってクラスに分類&lt;/li&gt;
&lt;li&gt;クラスの分布とクラス内の分布の２つの softmax に分解&lt;/li&gt;
&lt;li&gt;多層化 二分木 (バイナリコードを各単語に付与)&lt;ul&gt;
&lt;li&gt;短所：二分木を作るのが難しい  GPU で速くしにくい&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;サブワードモデル&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;単語のかわりに文字レベル&lt;/li&gt;
&lt;li&gt;中国語などは単語の概念がそもそも曖昧&lt;/li&gt;
&lt;li&gt;softmax が速い, 未知語が無い (例：人名) かわりに、依存の距離が長くなる&lt;/li&gt;
&lt;li&gt;形態学的な単語内の構造を扱える 例：disunited, disinherited, disinterested&lt;/li&gt;
&lt;li&gt;perplexity では単語レベルにはまだ敵わないが、言語モデルの未来&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;正規化&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Dropout&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;0/1 のビットマスクをサンプル、隠れユニットに乗算&lt;/li&gt;
&lt;li&gt;リカレントな接続 (例：隠れ層間) に適用しても効果的ではない&lt;ul&gt;
&lt;li&gt;理由: 時間軸上でこれを繰り返すと、ある時間が経過すると全ての隠れ状態がマスクされてしまう&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;リカレントではない接続（例：入力→隠れ層) だけに Dropout を適用&lt;/li&gt;
&lt;li&gt;よく採られる方法：過学習するほどネットワークを大きくし、強く正規化をかける&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Bayesian Dropout&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;リカレントな接続間で共有される Dropout マスクを使う&lt;/li&gt;
&lt;li&gt;文ごとに違う重みを使う&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;英語&lt;/th&gt;
&lt;th&gt;日本語&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;compelling&lt;/td&gt;
&lt;td&gt;魅力的な&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Finnish&lt;/td&gt;
&lt;td&gt;フィンランド語 (形態論が特に複雑なことで有名)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Turkish&lt;/td&gt;
&lt;td&gt;トルコ語&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;hone in on&lt;/td&gt;
&lt;td&gt;..に焦点をあわせる&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;modus operandi&lt;/td&gt;
&lt;td&gt;決まったやり方&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr/&gt;

&lt;p&gt;&lt;a name="lecture5"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;講義5: テキスト分類&lt;/h2&gt;
&lt;p&gt;講師：Karl Moritz Hermann (DeepMind)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;テキスト分類とは？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;スパム分類&lt;/li&gt;
&lt;li&gt;記事のトピック&lt;/li&gt;
&lt;li&gt;ツイートのハッシュタグ予測&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;分類の種類&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;二値 (true/false)&lt;/li&gt;
&lt;li&gt;多値&lt;/li&gt;
&lt;li&gt;多ラベル&lt;/li&gt;
&lt;li&gt;クラスタリング&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;分類の方法&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;手動（正確だが遅い、高い）&lt;/li&gt;
&lt;li&gt;ルールベース（正確だが、ルールを人手で書く必要がある）&lt;/li&gt;
&lt;li&gt;統計ベース（自動で高速だが、訓練データが必要）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;統計的テキスト分類&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P(c|d) (c ... クラス、d ... テキスト/文書)&lt;/li&gt;
&lt;li&gt;表現  テキスト -&amp;gt; d&lt;/li&gt;
&lt;li&gt;分類  P(c|d)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;表現&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;BoW (bag-of-words)&lt;/li&gt;
&lt;li&gt;手動で作った素性&lt;/li&gt;
&lt;li&gt;素性学習&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;生成 vs 識別モデル&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;生成モデル P(c, d) 潜在変数と観測変数の同時分布に確率を付与&lt;ul&gt;
&lt;li&gt;Nグラム, 隠れマルコフモデル, 確率的文脈自由文法, etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;識別モデル P(c | d) データが与えられた時の潜在変数の分布に確率を付与&lt;ul&gt;
&lt;li&gt;ロジスティック回帰&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ナイーブベイズ&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ベイズの法則&lt;/li&gt;
&lt;li&gt;P(c | d) 比例 P(c) P(d|c) -&amp;gt; 文書を単語に分解 P(c|t_i)&lt;/li&gt;
&lt;li&gt;ラベル付きの訓練データの統計を取るだけ&lt;/li&gt;
&lt;li&gt;「ナイーブ」-&amp;gt; 全ての単語は独立　文書の確率を単語の確率の積で近似　実際はけっこううまくいく&lt;/li&gt;
&lt;li&gt;MAP (maximize a posteriori)&lt;/li&gt;
&lt;li&gt;大量の小さい確率の積はトリッキー → 対数空間で計算&lt;/li&gt;
&lt;li&gt;確率ゼロ　→ スムージング&lt;/li&gt;
&lt;li&gt;長所：シンプル、解釈可能、速い&lt;/li&gt;
&lt;li&gt;短所：独立仮定、文・文書の構造を考慮してない、ゼロ確率&lt;/li&gt;
&lt;li&gt;ナイーブベイズは生成モデル！&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;素性表現&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;二値・多値・連続値&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ロジスティック回帰&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ロジスティック：ロジスティック関数を使う　回帰：素性と重みの積で表現&lt;/li&gt;
&lt;li&gt;二値のケース  logit = バイアス＋重み*素性、P(true|d) = ロジスティック関数(logit)&lt;/li&gt;
&lt;li&gt;多値のケース  logit → softmax&lt;/li&gt;
&lt;li&gt;softmax → ロジスティック関数の多値への拡張&lt;/li&gt;
&lt;li&gt;分類だけではなく確率も学習&lt;/li&gt;
&lt;li&gt;学習：対数確率を最大化  βに関する微分は凸関数　ただし閉じた解は存在しない&lt;/li&gt;
&lt;li&gt;長所：シンプル、解釈可能、素性間の独立を仮定しない　短所：学習が（ナイーブベイズより）難しい　手法をデザインする必要　汎化しない可能性&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;RNN (再帰型ニューラルネットワーク)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;hi は、i までの入力と、i - 1 までの h に依存&lt;/li&gt;
&lt;li&gt;i までのテキストの情報を含んでいる&lt;/li&gt;
&lt;li&gt;テキストの表現そのもの！&lt;/li&gt;
&lt;li&gt;テキスト全体を読ませた後の h を取り出して素性にすればいい&lt;/li&gt;
&lt;li&gt;h が必要な情報を含んでいることをどのように保証するか？&lt;/li&gt;
&lt;li&gt;損失関数：基本的には MLP (多層パーセプトロン) と同じ → クロスエントロピー&lt;/li&gt;
&lt;li&gt;多クラス分類&lt;ul&gt;
&lt;li&gt;クロスエントロピーは、ラベルが1つの場合&lt;/li&gt;
&lt;li&gt;方法1: 複数の２値分類器を訓練&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;複数の目的関数&lt;ul&gt;
&lt;li&gt;言語モデルの目的関数と文書分類を同時に最適化する&lt;/li&gt;
&lt;li&gt;あらかじめ学習した単語の埋め込み表現を使うことも&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;双方向RNN&lt;ul&gt;
&lt;li&gt;前向きRNNの最後の隠れ状態＋後ろ向きRNNの最後の隠れ状態&lt;/li&gt;
&lt;li&gt;ただしテキスト生成には使えない&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;生成モデルにも識別モデルにもなる&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;非系列型ニューラルネット&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;再帰型ネット&lt;ul&gt;
&lt;li&gt;構文の形に潜在状態を構成&lt;/li&gt;
&lt;li&gt;自己複合器 (autoencoder) の損失関数を導入&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;畳み込みネット&lt;ul&gt;
&lt;li&gt;畳み込み フィルタを適用&lt;/li&gt;
&lt;li&gt;Subsample (maxなど)一部の画素だけを残す&lt;/li&gt;
&lt;li&gt;単語 x 埋め込み  の行列を元画像と見なす&lt;/li&gt;
&lt;li&gt;利点：速い、BOW で十分、行列（小さい窓）を使うので、構造を少し使える&lt;/li&gt;
&lt;li&gt;欠点：逐次的ではない、可変長のテキストに対する生成モデルは少し難しい&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;英語&lt;/th&gt;
&lt;th&gt;日本語&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;plagiarism&lt;/td&gt;
&lt;td&gt;剽窃&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;interpretable&lt;/td&gt;
&lt;td&gt;解釈可能な&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;reconstruction&lt;/td&gt;
&lt;td&gt;再現&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr/&gt;

&lt;p&gt;&lt;a name="lecture6"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;講義6: Nvidia の GPU を使った深層学習&lt;/h2&gt;
&lt;p&gt;講師：Jeremy Appleyard (Nvidia)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;なぜ性能が重要か&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;訓練時：より多くのアーキテクチャを実験できる&lt;/li&gt;
&lt;li&gt;プロダクション：ユーザーにより速く結果を提示できる&lt;/li&gt;
&lt;li&gt;全てが自動ではない　機械学習の研究者も知っているべきこと&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ハードウェア&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CPU → 遅延がなるべく小さいように最適化　大きなキャッシュ&lt;/li&gt;
&lt;li&gt;GPU → 並列度が非常に高い。数万の演算を同時に実行　すぐにデータが帰ってこない&lt;ul&gt;
&lt;li&gt;CPU より10倍以上高いスループット (gflops)&lt;/li&gt;
&lt;li&gt;メモリー帯域も同じ傾向&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ルーフライン・モデル&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;演算強度 (arithmetic intensity) = flop / バイト → x軸&lt;/li&gt;
&lt;li&gt;演算性能 flop/s → y軸&lt;/li&gt;
&lt;li&gt;グラフにすると、屋根のような形になる → ルーフライン&lt;/li&gt;
&lt;li&gt;例：行列乗算　演算強度が高い&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;RNN (LSTM)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;多くの行列乗算&lt;/li&gt;
&lt;li&gt;ミニバッチ化  (収束が良くなる、ハードウェア上で高速化)&lt;/li&gt;
&lt;li&gt;行列乗算の右側 (w_t, h_t-1) は共通 → ４つの乗算を一つにまとめることが可能&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;行列x行列乗算 (GEMM - BLAS の関数名)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;計算省略&lt;/li&gt;
&lt;li&gt;LSTM: flops / バイトの比は 2HB:3B+4H → O(n) だが、H と B の値に大きく依存&lt;/li&gt;
&lt;li&gt;プロダクション：バッチサイズは 1 であることが多い&lt;/li&gt;
&lt;li&gt;ルーフライン・モデル (バッチのサイズ 対 GFLOP/s)&lt;/li&gt;
&lt;li&gt;バッチサイズ = 32 〜 64 あたりに「角」がある&lt;/li&gt;
&lt;li&gt;実測値と理論値はよく一致している（角のあたり以外）&lt;/li&gt;
&lt;li&gt;ミニバッチ化は非常に大切！&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ネットワークレベルの最適化&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;どうやって結果を変えずに高速化するか&lt;ul&gt;
&lt;li&gt;メモリー転送を減らす&lt;/li&gt;
&lt;li&gt;オーバーヘッドを減らす&lt;/li&gt;
&lt;li&gt;並列度を上げる&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;最適化1 (メモリー転送)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;行列 A (固定) をメモリにロードする時間を削減&lt;/li&gt;
&lt;li&gt;入力 w_t はお互いに独立&lt;/li&gt;
&lt;li&gt;W*[w_t; h_(t-1)] を、w に依存する部分と h に依存する部分の和に分解&lt;/li&gt;
&lt;li&gt;w をグループ化  バッチサイズを増やすのと同等&lt;/li&gt;
&lt;li&gt;永続RNNs → リカレント行列を、チップ上のメモリに保持しておく高度テクニック&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;最適化2&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;オーバーヘッド&lt;/li&gt;
&lt;li&gt;要素ごとの積　各演算ごとにカーネルを起動&lt;/li&gt;
&lt;li&gt;演算ごとにカーネルを起動しなければいけない理由はない  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;最適化3&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;並列度を上げる&lt;/li&gt;
&lt;li&gt;多層RNN&lt;/li&gt;
&lt;li&gt;単純な方法：１層目をすべて計算、次に２層目をすべて計算、etc.&lt;/li&gt;
&lt;li&gt;代わりに、放射状に互いに依存しないセルを同時に計算する&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;cuDNN&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LSTM などの標準的な高速化が提供されている&lt;/li&gt;
&lt;li&gt;BLAS, FFT, 乱数生成 などのライブラリも&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;おわりに&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;性能を意識することは大事&lt;/li&gt;
&lt;li&gt;ソフトウェアとハードウェアの選択、両方が影響&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;英語&lt;/th&gt;
&lt;th&gt;日本語&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;bound&lt;/td&gt;
&lt;td&gt;上限&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;intensity&lt;/td&gt;
&lt;td&gt;強度&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;pointwise&lt;/td&gt;
&lt;td&gt;(本講義では)要素ごとの (element-wise と同じ)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;back-to-back&lt;/td&gt;
&lt;td&gt;隣り合わせに&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr/&gt;

&lt;p&gt;&lt;a name="lecture7"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;講義7: 条件付き言語モデリング&lt;/h2&gt;
&lt;p&gt;講師：Chris Dyer (DeepMind / カーネギーメロン大)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;条件「無し」の言語モデル&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ある語彙上の文字列に対して確率を付与&lt;/li&gt;
&lt;li&gt;過去の履歴から次の単語を予測する問題に単純化&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;条件付き言語モデル&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ある文脈条件 x のもと、言語を生成&lt;ul&gt;
&lt;li&gt;x: 著者、w: その著者のテキスト&lt;/li&gt;
&lt;li&gt;x: フランス語の文, w: 翻訳された英語の文 (確率が訳に立つ！)&lt;/li&gt;
&lt;li&gt;x: 画像, w: 画像のキャプション&lt;/li&gt;
&lt;li&gt;x: 音声, w: 音声の書き起こし&lt;/li&gt;
&lt;li&gt;x: 文書＋質問, w: 応答&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;訓練データは入力, 出力のペア&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;タスクによって、利用できるデータの量が大きく異る&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;翻訳、要約、キャプション、音声認識等は比較的大規模なデータ&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;アルゴリズム&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;最も確率の高い w を探すのは困難&lt;/li&gt;
&lt;li&gt;ビームサーチ&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;評価&lt;ul&gt;
&lt;li&gt;クロスエントロピー, パープレキシティ (実装：普通、解釈：難しい)&lt;/li&gt;
&lt;li&gt;タスク依存の評価  例：翻訳のBLEU（実装：簡単、解釈：普通） ←オススメ&lt;/li&gt;
&lt;li&gt;人手評価（実装：難しい、解釈：簡単）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;エンコーダー・デコーダーモデル&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;入力 x を固定長ベクトル c にどうエンコード（符号化）するか → 問題依存&lt;/li&gt;
&lt;li&gt;デコード時に、どうやって c を条件として使うか → あまり問題依存ではない&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Kalchbrenner and Blunsom 2013&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;デコーダー：古典的な RNN ＋ 原文をエンコードしたもの s&lt;/li&gt;
&lt;li&gt;エンコーダー：単純なモデル：単語埋め込みの和&lt;ul&gt;
&lt;li&gt;利点：速い　データ量が少なくて済む&lt;/li&gt;
&lt;li&gt;欠点: 語順を考慮しない  非合成的な語 ("hot dog") を扱えない&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;もう少し賢いモデル：Convolutional sentence model (CSM)&lt;ul&gt;
&lt;li&gt;重ね合わせた畳み込み層&lt;/li&gt;
&lt;li&gt;利点：局所的な相互作用　構文的なものを捉えられる&lt;/li&gt;
&lt;li&gt;欠点：長さの異なる文をどう扱いか&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Sutskever et al. 2014&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;さらに単純な構造　エンコーダー・デコーダー両方 LSTM&lt;/li&gt;
&lt;li&gt;エンコーディングは (c_l, h_l) (注：c → セル状態，h → 隠れ状態)&lt;/li&gt;
&lt;li&gt;利点：LSTM が長距離の依存関係を覚えられる  欠点：隠れ状態が多くの情報を覚えないといけない&lt;/li&gt;
&lt;li&gt;工夫&lt;ul&gt;
&lt;li&gt;エンコーダとデコーダ間で文を与える順序を逆にする&lt;/li&gt;
&lt;li&gt;アンサンブル  (Softmax の前に、複数個のモデルの出力を平均する)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;デコーディング&lt;ul&gt;
&lt;li&gt;arg max P(w | x) の arg max を正確に計算するのは難しい&lt;/li&gt;
&lt;li&gt;貪欲法で代替 直前の単語が正しいと仮定して次の単語に移る&lt;/li&gt;
&lt;li&gt;ビームサーチ：上位 b 個の仮定を保持しながらデコード&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;画像キャプション生成&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ニューラルネットは全てがベクトル → 複数のモダリティ (様式) に対応するのが簡単&lt;/li&gt;
&lt;li&gt;ImageNet の訓練済みレイヤーを画像のエンベディングとして使用&lt;/li&gt;
&lt;li&gt;Kiros et al. (2013)&lt;ul&gt;
&lt;li&gt;上述の K&amp;amp;B 2013 に類似&lt;/li&gt;
&lt;li&gt;隠れ状態の更新の際に、エンコードされた画像を足し合わせるだけ&lt;/li&gt;
&lt;li&gt;乗算的言語モデル テンソル r_(i, j, w) → 分解して低ランク近似&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;質問&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;単語をどう翻訳するのは文脈依存ではないか？&lt;ul&gt;
&lt;li&gt;Yes! だが、現在よく使われているテストセットでは文はかなり独立&lt;/li&gt;
&lt;li&gt;会話文のようなより良いテストデータでは効果があるかも&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;英語&lt;/th&gt;
&lt;th&gt;日本語&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;unconditional&lt;/td&gt;
&lt;td&gt;無条件の/条件の無い&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;conditional&lt;/td&gt;
&lt;td&gt;条件付きの&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;transcription&lt;/td&gt;
&lt;td&gt;(音声の) 書き起こし&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;intractable&lt;/td&gt;
&lt;td&gt;計算量的に困難&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;modulo&lt;/td&gt;
&lt;td&gt;〜を除いて&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;modality&lt;/td&gt;
&lt;td&gt;様式 (例：画像 vs テキスト)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;architecture&lt;/td&gt;
&lt;td&gt;アーキテクチャ (ニューラルネットの構造)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;compositional&lt;/td&gt;
&lt;td&gt;(意味が) 合成的な&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;draconian&lt;/td&gt;
&lt;td&gt;極めて厳しい&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr/&gt;

&lt;p&gt;&lt;a name="lecture8"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;講義8: アテンションを使った言語生成&lt;/h2&gt;
&lt;p&gt;講師：Chris Dyer (DeepMind / カーネギーメロン大)&lt;/p&gt;
&lt;p&gt;復習：条件付き言語モデル&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;問題点&lt;ul&gt;
&lt;li&gt;ベクトルによる条件付け → 文を固定長のベクトルに圧縮&lt;/li&gt;
&lt;li&gt;勾配を非常に長い距離、伝播させる必要がある  LSTM も忘れる&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;機械翻訳におけるアテンション&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;解法&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;原言語の文を行列で表現 → 容量問題を解決&lt;/li&gt;
&lt;li&gt;対象言語の文を行列から生成 → 伝播問題を解決&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;文の行列表現&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;行列の列の数 = 単語の数&lt;/li&gt;
&lt;li&gt;単純なモデル：単語ベクトルの連結（単純すぎて誰も論文に書いていない）&lt;/li&gt;
&lt;li&gt;畳み込みネット：Gehring et al. (2016)  K &amp;amp; B (2013) に似ている&lt;/li&gt;
&lt;li&gt;双方向RNN: Bahdanau et al. (2015) により有名&lt;ul&gt;
&lt;li&gt;前向き＋後ろ向き → 連結 →　行列 (2n x w)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;2017年の現状&lt;ul&gt;
&lt;li&gt;体系的に比較した研究はほとんど無い&lt;/li&gt;
&lt;li&gt;畳み込みネットは興味深く、あまり研究されてない&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;行列からの生成 (Bahdanau et al. 2015)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;生成時に、RNN は２つの情報を使う&lt;ul&gt;
&lt;li&gt;直前に生成した単語のベクトル表現&lt;/li&gt;
&lt;li&gt;入力行列の「ビュー」&lt;ul&gt;
&lt;li&gt;時間ごとに、入力行列の違う部分から情報を取り出す&lt;/li&gt;
&lt;li&gt;重み a_t (長さ = |f|) → アテンション&lt;/li&gt;
&lt;li&gt;各（生成側の）単語が（入力側の）単語にどう対応しているか解釈可能&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;どう a_t を計算するか&lt;ul&gt;
&lt;li&gt;各時刻 t に、期待される入力ベクトル r_t = V s_(t-1) を計算 (V は学習可能パラメータ)&lt;/li&gt;
&lt;li&gt;→ これを、F の各列との内積を計算 → Softmax して a_t を得る&lt;/li&gt;
&lt;li&gt;(Bahdanau et al. 2015) → 内積を MLP で置き換え&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;BLEU +11!&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;モデルの変種&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Early binding (早期結合)&lt;/li&gt;
&lt;li&gt;Late binding (晩期結合？) 現在の潜在状態と、アテンション・ベクトルを考慮して、単語を生成&lt;ul&gt;
&lt;li&gt;遅すぎる？ アテンションが間接的にしか潜在状態に寄与してない&lt;/li&gt;
&lt;li&gt;訓練時に、潜在状態とアテンションの計算を並列化できる&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;まとめ&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;アテンションは畳み込みネットの「プーリング」に似てる&lt;/li&gt;
&lt;li&gt;アテンションを可視化すると単語のアラインメントが観察できる&lt;/li&gt;
&lt;li&gt;勾配について&lt;ul&gt;
&lt;li&gt;デコーダーが間違えた場合、アテンションの強い語に勾配が強く伝播する&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;翻訳とアテンション&lt;ul&gt;
&lt;li&gt;人間が翻訳する時、文を記憶するわけではない → 必要に応じて原文を参照する&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;画像キャプション生成におけるアテンション&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Vinyals et al. 2014&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sutskever のモデルと同じ&lt;/li&gt;
&lt;li&gt;ただし、画像のエンコーダーは畳み込みネット&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;アテンションは役に立つか？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Yes!&lt;/li&gt;
&lt;li&gt;畳み込みネットの各知覚野を畳み込んだベクトル → アノテーション・ベクトル a&lt;/li&gt;
&lt;li&gt;アテンションの重みを、Bahdanau et al. 2014 の方法で計算&lt;/li&gt;
&lt;li&gt;Stochastic hard attention (Xu et al. 2015)&lt;ul&gt;
&lt;li&gt;ソフトな分布ではなく、知覚野を一つに決めてサンプル&lt;/li&gt;
&lt;li&gt;Jensen の不等式を使い単純化&lt;/li&gt;
&lt;li&gt;MCMC を使いサンプリング&lt;/li&gt;
&lt;li&gt;教科学習の REINFORCE&lt;/li&gt;
&lt;li&gt;アテンションの重み → 単語を生成した時にどこに注目していたかを可視化できる&lt;/li&gt;
&lt;li&gt;BLEU を使って画像キャプションを評価するのは、機械翻訳に比べて難しい&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;英語&lt;/th&gt;
&lt;th&gt;日本語&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;vehemently&lt;/td&gt;
&lt;td&gt;猛烈に&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Vulgar Latin&lt;/td&gt;
&lt;td&gt;俗ラテン語&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;receptive field&lt;/td&gt;
&lt;td&gt;知覚野&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;inequality&lt;/td&gt;
&lt;td&gt;不等式&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr/&gt;

&lt;p&gt;&lt;a name="lecture9"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;講義9: 音声認識&lt;/h2&gt;
&lt;p&gt;講師：Andrew Senior (DeepMind)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;音声認識&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ASR 自動音声認識  音声の波形→テキスト&lt;/li&gt;
&lt;li&gt;TTS テキスト読み上げ  テキスト→音声の波形&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;関連する問題&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;自発発話 vs 読み上げ, 大規模語彙, 雑音のある環境, 低資源, 訛り, etc.&lt;/li&gt;
&lt;li&gt;TTS&lt;/li&gt;
&lt;li&gt;話者特定&lt;/li&gt;
&lt;li&gt;音声強調&lt;/li&gt;
&lt;li&gt;音源分離&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;音声&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;気圧の変化の波&lt;/li&gt;
&lt;li&gt;声帯 → 声道による変調 → 調音（母音） ＋ 摩擦や閉鎖（子音）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;音声の表現&lt;ul&gt;
&lt;li&gt;人間の音声は ~85 Hz - 8 kHz&lt;/li&gt;
&lt;li&gt;解像度 (bits per sample) 1 bit でも理解可能&lt;/li&gt;
&lt;li&gt;より低次元のデータ → 高速フーリエ解析 (FFT) して周波数帯ごとのエネルギーに変換&lt;ul&gt;
&lt;li&gt;音声の問題を画像認識の問題に変換！&lt;/li&gt;
&lt;li&gt;ただし x軸（時間）は可変&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;FFT もまだ次元が多すぎる&lt;ul&gt;
&lt;li&gt;メル尺度 (人間の聴力特性に合わせた非線型スケール) に変換した離散ウインドウを使ってダウンサンプリング&lt;/li&gt;
&lt;li&gt;40次元程度&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;MFCC (メル周波数ケプストラム係数)&lt;ul&gt;
&lt;li&gt;メル尺度のフィルタバンクから得られた値を離散コサイン変換 (主成分分析に類似)&lt;/li&gt;
&lt;li&gt;連続するフレーム間で積み重ね&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;音声認識の歴史&lt;ul&gt;
&lt;li&gt;1960年代：Dynamic Time Warping (テンプレートを伸縮してマッチング)&lt;/li&gt;
&lt;li&gt;1970年代：隠れマルコフモデル&lt;/li&gt;
&lt;li&gt;1995-：ガウス混合モデルが主流&lt;/li&gt;
&lt;li&gt;2006-：ニューラルネットワーク&lt;/li&gt;
&lt;li&gt;2012-：RNN&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;コミュニケーションとしての音声&lt;ul&gt;
&lt;li&gt;音素(phoneme) - 単語・意味を区別する最小の単位  表記：IPA/X-SAMPA&lt;/li&gt;
&lt;li&gt;韻律(prosody) - リズム、アクセント、イントネーションなど。認識する研究は多いが、あまり使われない&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;データセット&lt;ul&gt;
&lt;li&gt;TIMIT (小さい、音素境界を人手で付与)&lt;/li&gt;
&lt;li&gt;Wall Street Journal 読み上げ&lt;/li&gt;
&lt;li&gt;...&lt;/li&gt;
&lt;li&gt;Google voice search&lt;ul&gt;
&lt;li&gt;実際のユーザーの発話&lt;/li&gt;
&lt;li&gt;２年間だけ保存、その後破棄&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;DeepSpeech&lt;ul&gt;
&lt;li&gt;発話者がヘッドフォンで雑音を聞きながら発話 → 発話に影響&lt;/li&gt;
&lt;li&gt;その上に雑音を付与&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;確率的音声認識&lt;ul&gt;
&lt;li&gt;入力 o (observation; 観察) から、 最も尤もらしい単語系列 w を求める。&lt;/li&gt;
&lt;li&gt;HMM 状態＝音素、出力＝MFCCなどのベクトル&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;音の単位&lt;ul&gt;
&lt;li&gt;文脈非依存のHMM 状態 (語頭・語中・語末の３状態を別々にモデル化)&lt;/li&gt;
&lt;li&gt;文脈依存のHMM 状態 (例: "cat" の /k/ と "cone" の /k/ が違う)&lt;/li&gt;
&lt;li&gt;diphone (音素のバイグラム)&lt;/li&gt;
&lt;li&gt;音節&lt;/li&gt;
&lt;li&gt;単語全体 (YouTube 音声認識の論文)&lt;/li&gt;
&lt;li&gt;graphemes (文字) - 単語→発音の辞書を持たなくても良い&lt;ul&gt;
&lt;li&gt;英語では普通  単語&amp;lt;-&amp;gt;発音の対応を(音声認識の副作用として)学習  (例："ough")&lt;/li&gt;
&lt;li&gt;他の言語（イタリア語・トルコ語）では綴りが音に一致&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;文脈依存の音素クラスタリング&lt;ul&gt;
&lt;li&gt;音素のトライグラムを考えると、3 x 42^3 の組み合わせ&lt;/li&gt;
&lt;li&gt;このほとんどは起こらない → クラスタリング&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;音声認識の基本式&lt;ul&gt;
&lt;li&gt;w^ = arg max P(w | o) = arg max P(o | w)P(w)&lt;/li&gt;
&lt;li&gt;P(o|w) ... 音響モデルスコア, P(w) ... 言語モデルスコア&lt;/li&gt;
&lt;li&gt;言語モデル chain rule を使った n-gram  デコーディング時に最ランキング&lt;/li&gt;
&lt;li&gt;n-gram 言語モデルと LSTM等の識別的言語モデルを組み合わせると良い結果&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;変換としての音声認識&lt;ul&gt;
&lt;li&gt;音声→フレーム→状態→音素→単語→文→意味&lt;/li&gt;
&lt;li&gt;重み付き有限状態トランスデューサー(WFST) (音素→単語への変換)&lt;/li&gt;
&lt;li&gt;単語→文へ変換する WFST と合成&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ガウス混合モデル (音響モデル)&lt;ul&gt;
&lt;li&gt;1990〜2010 の主流モデル&lt;/li&gt;
&lt;li&gt;複数のガウス分布の重み付き和。各分布の平均と分散(対角成分のみ)を学習&lt;/li&gt;
&lt;li&gt;EMアルゴリズムによって学習 M:強制アラインメント, E:パラメータの推定&lt;/li&gt;
&lt;li&gt;とても並列化しやすいが、データを効率的に利用できない (1フレーム→1音素)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;強制アラインメント&lt;ul&gt;
&lt;li&gt;ビタビアルゴリズムを使い、訓練データにおいて、素性と音素状態を最尤アラインメント&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;デコーディング&lt;ul&gt;
&lt;li&gt;認識時は、行列のかわりにグラフになる（語の間の空白、複数の可能性、etc.）&lt;/li&gt;
&lt;li&gt;ビームサーチ (スコアの高い top-n 経路だけを残す)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ニューラルネットワークを用いた音声認識&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;素性を計算 or 確率を計算&lt;/li&gt;
&lt;li&gt;素性を計算&lt;ul&gt;
&lt;li&gt;通常の前向きネットワーク、ボトルネック層の値を素性と使う&lt;/li&gt;
&lt;li&gt;元のガウス混合モデルと合わせて（連結して）使う&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ハイブリッド・ネット&lt;ul&gt;
&lt;li&gt;音素の分類器としてNNを学習&lt;/li&gt;
&lt;li&gt;P(o | c) を GMM ではなくNNでモデル化&lt;/li&gt;
&lt;li&gt;言語モデルと音響モデルを重みで調整すると良い結果&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;畳み込み式ネット (CNN)&lt;ul&gt;
&lt;li&gt;歴史は長い&lt;/li&gt;
&lt;li&gt;WaveNet （音声合成）でも使われる&lt;/li&gt;
&lt;li&gt;時間軸上の pooling は良くない（時間情報を捨ててしまう）周波数の領域では OK&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;繰り返し型ネット (RNN)&lt;ul&gt;
&lt;li&gt;RNN, LSTM, ..&lt;/li&gt;
&lt;li&gt;CLDNN (Sainath et al., 2015a) - CNN + LSTM + DNN&lt;/li&gt;
&lt;li&gt;GRU (DeepSpeech)&lt;/li&gt;
&lt;li&gt;双方向モデルで性能は上がるが、遅延が生じる （発話の終わりまで待たないといけない）&lt;/li&gt;
&lt;li&gt;テクニック：発話が終わったかどうか確信の無い段階で、Web検索を開始 → 低遅延を実現&lt;/li&gt;
&lt;li&gt;Switchboard (大規模・電話・自発発話コーパス)で人間に匹敵 (Xiong et al., 2016)&lt;ul&gt;
&lt;li&gt;BLSTMのアンサンブル&lt;/li&gt;
&lt;li&gt;i-vector で話者を正規化&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;CTC (Connectionist Temporal Classification) → テクニックの集合&lt;ul&gt;
&lt;li&gt;音素の間に無音シンボルを挿入&lt;/li&gt;
&lt;li&gt;継続的アラインメント&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Sequence discriminative training&lt;ul&gt;
&lt;li&gt;Cross entropy は、正しいクラスの確率を最大化する&lt;/li&gt;
&lt;li&gt;本当に最小化したいのは WER (単語誤り率) → これと近い、微分可能な損失関数を使う&lt;/li&gt;
&lt;li&gt;訓練時にデコーディング(言語モデルを含む)をし、間違えた部分と正解との差を増やす&lt;/li&gt;
&lt;li&gt;WER を 15％削減&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;seq2seq&lt;ul&gt;
&lt;li&gt;基本的な seq2seq は音声認識に向いてない　発話は長すぎる＋機械翻訳等と異なり、単調 (monotone)&lt;/li&gt;
&lt;li&gt;アテンションは向いてる  Attention + seq2seq (Chorowski et al. 2015)&lt;/li&gt;
&lt;li&gt;Listen, Attend, Spell (Chen et al., 2015)&lt;/li&gt;
&lt;li&gt;別々に学習した言語モデルを統合するのが難しい&lt;/li&gt;
&lt;li&gt;Watch Listen, Attend, Spell (Chung et al., 2016) 音声認識＋ビデオから読唇  音声だけよりも良い。ビデオだけでも音声認識できる (WER = 15%)&lt;/li&gt;
&lt;li&gt;Neural transducer - アテンションは系列全体を見なければならない。チャンク毎に認識することで解決&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;英語&lt;/th&gt;
&lt;th&gt;日本語&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;peculiarity&lt;/td&gt;
&lt;td&gt;特異な点&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;nominal&lt;/td&gt;
&lt;td&gt;名目上の&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;babble&lt;/td&gt;
&lt;td&gt;ガヤガヤという話し声&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Zulu&lt;/td&gt;
&lt;td&gt;ズールー語&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;prosody&lt;/td&gt;
&lt;td&gt;韻律&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;acoustic&lt;/td&gt;
&lt;td&gt;音響的な&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;vocal tract&lt;/td&gt;
&lt;td&gt;声道&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;arbitrarily&lt;/td&gt;
&lt;td&gt;任意に&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;perceptive&lt;/td&gt;
&lt;td&gt;知覚的な&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Polish [pouliʃ]&lt;/td&gt;
&lt;td&gt;ポーランド(人の/語)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;polish [pɑliʃ]&lt;/td&gt;
&lt;td&gt;磨く&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;lexicon&lt;/td&gt;
&lt;td&gt;辞書&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;precursor&lt;/td&gt;
&lt;td&gt;先駆け&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;monotonic&lt;/td&gt;
&lt;td&gt;単調&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr/&gt;

&lt;p&gt;&lt;a name="lecture10"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;講義10: 音声合成&lt;/h2&gt;
&lt;p&gt;講師：Andrew Senior (DeepMind)&lt;/p&gt;
&lt;p&gt;音声認識（続き）&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;End-to-end モデル&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;生の音声データからテキストを出力するモデルを直接学習する&lt;/li&gt;
&lt;li&gt;音素の代わりに文字/単語を出力&lt;/li&gt;
&lt;li&gt;素性の計算を単純化　(MCFF/log-Mel などの人手で作られた素性に頼らない)&lt;/li&gt;
&lt;li&gt;依存関係の距離が長くなる&lt;/li&gt;
&lt;li&gt;Clockwork RNN (Koutnik et al, 2014) 周期の異なる階層的な複数のRNN&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;フィルタの学習&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;特性周波数のピークを低い周波数から順にプロット&lt;/li&gt;
&lt;li&gt;より低い周波数帯をカバーするフィルタが多い&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;雑音のある環境下での音声認識&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ノイズを人工的に合成&lt;/li&gt;
&lt;li&gt;Google では、YouTube のビデオからスピーチ以外の部分を抽出して合成&lt;/li&gt;
&lt;li&gt;部屋シミュレーター&lt;/li&gt;
&lt;li&gt;denoiser (ノイズ除去器) をマルチタスク的に学習 → 文字書き起こしの無い音声データも使える&lt;/li&gt;
&lt;li&gt;雑音のある環境下では、人間の話し方も変わる&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;複数マイクの音声認識&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;研究レベルでの歴史は長いが、スマホ、Amazon Echo などの複数マイクデバイスが普及するにつれて重要に&lt;/li&gt;
&lt;li&gt;ビームフォーミング (Beamforming) マイクの指向性を高める&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;音声合成&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;音声合成とは&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;テキストから音声波形&lt;/li&gt;
&lt;li&gt;音声生成の過程&lt;ul&gt;
&lt;li&gt;声帯・声道 → 口で変調&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;音声生成の流れ&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;テキスト解析&lt;ul&gt;
&lt;li&gt;例：文分割、単語分割、品詞解析、テキスト標準化, etc.&lt;/li&gt;
&lt;li&gt;例：429 は four-hundred-twenty-nine か four-twenty-nine か&lt;/li&gt;
&lt;li&gt;離散 → 離散的 (NLP)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;音声生成&lt;ul&gt;
&lt;li&gt;離散 → 連続的&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;音声生成&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ルールに基づいたフォルマント合成&lt;/li&gt;
&lt;li&gt;サンプルに基づいた連結合成 → 途切れ途切れの音、不自然&lt;/li&gt;
&lt;li&gt;フレーズ合成 (例：駅のアナウンス) → 「次の列車は」＋地名＋「行きです。」&lt;/li&gt;
&lt;li&gt;モデルに基づいた生成的合成&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;連結合成&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;多様性のあるデータベースを作成&lt;/li&gt;
&lt;li&gt;diphones をカバー&lt;/li&gt;
&lt;li&gt;簡単な音声認識システムを使い、強制アラインメント → diphone の境界を特定&lt;/li&gt;
&lt;li&gt;コスト（サンプルと望む出力の距離）を最小化&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;データベースの作成&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;スタジオ録音&lt;/li&gt;
&lt;li&gt;一貫した環境&lt;/li&gt;
&lt;li&gt;背景雑音無し&lt;/li&gt;
&lt;li&gt;プロの単一話者から大量のオーディオを録音&lt;/li&gt;
&lt;li&gt;読み上げ音声（自発発話ではない）&lt;/li&gt;
&lt;li&gt;データセット&lt;ul&gt;
&lt;li&gt;VCKT (Voice Cloning Tool Kit)&lt;ul&gt;
&lt;li&gt;退行性疾患患者が、あらかじめ自分の声を録音→話せなくなってからも自分の声で音声合成&lt;/li&gt;
&lt;li&gt;汎用的なモデルを訓練し、自分の声に適応&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Merlin&lt;ul&gt;
&lt;li&gt;オープンソースの音声合成システム&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;音声合成の評価&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;音声認識は簡単 → 単語誤り率&lt;/li&gt;
&lt;li&gt;音声合成 → 主観的&lt;/li&gt;
&lt;li&gt;客観的な指標 (例: 駅のアナウンス)：聞いて分かるか？今ああまり意味をなさない&lt;/li&gt;
&lt;li&gt;Mean Opinion Scale (0 から 5の尺度)&lt;/li&gt;
&lt;li&gt;A/B 選好テスト どちらがより良いか&lt;/li&gt;
&lt;li&gt;客観的な指標&lt;ul&gt;
&lt;li&gt;PESQ&lt;/li&gt;
&lt;li&gt;ロバストな MOS&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Blizzard Competition&lt;ul&gt;
&lt;li&gt;音声合成のコンペティション&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;TTSの確率的な定式化&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;p(x | w, X, W)&lt;ul&gt;
&lt;li&gt;X: 音声波形, W: 文字書き起こし, w: 入力テキスト, x: 出力波形&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;補助変数&lt;ul&gt;
&lt;li&gt;o: 音響素性, l: 言語素性, λ: モデル&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;近似1：点推定&lt;/li&gt;
&lt;li&gt;近似2: ステップ毎の最大化&lt;/li&gt;
&lt;li&gt;言語的素性&lt;ul&gt;
&lt;li&gt;文 (長さ)、句 (抑揚)、単語 (品詞)、音節 (強調、声調)、音素 (有声/無声)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;持続時間モデル&lt;ul&gt;
&lt;li&gt;各音素がどのぐらいの時間継続するかを別にモデル化&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Vocoder&lt;ul&gt;
&lt;li&gt;voice decoder/encoder 声の合成&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;生成的音響モデル&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;HMM&lt;ul&gt;
&lt;li&gt;アラインメント・モデルと同様&lt;/li&gt;
&lt;li&gt;各状態に対して、出力ベクトルの平均・分散を計算、生成の際に利用する&lt;/li&gt;
&lt;li&gt;多くの情報が平均されているので、くぐもった声になる&lt;/li&gt;
&lt;li&gt;問題：&lt;ul&gt;
&lt;li&gt;スムーズではない&lt;/li&gt;
&lt;li&gt;高次元の音響素性を扱いにくい&lt;/li&gt;
&lt;li&gt;データの断片化&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ニューラルネット&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ガウシアン分布の代わりにニューラルネットを使用&lt;/li&gt;
&lt;li&gt;フレーム間で固定でなくても良い → よりスムーズな転移&lt;/li&gt;
&lt;li&gt;隠れ層にリカレント接続を入れると性能が向上&lt;/li&gt;
&lt;li&gt;高次元の素性をモデル化可能（生スペクトルさえ）&lt;/li&gt;
&lt;li&gt;現在では、研究＋製品の主流&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;End-to-End システム&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Audo-encoder を使い、低次元の音響素性を学習 → 良い結果&lt;/li&gt;
&lt;li&gt;Source-filter モデルと音響モデルを同時に最適化&lt;/li&gt;
&lt;li&gt;WaveNet&lt;ul&gt;
&lt;li&gt;生音声の生成モデル&lt;/li&gt;
&lt;li&gt;Pixel RNN, Pixel CNN のモデルに類似&lt;/li&gt;
&lt;li&gt;自己回帰モデル&lt;/li&gt;
&lt;li&gt;畳み込みネットでモデル化 (Casual dilated convolution) 長距離の依存関係を捉えられる&lt;/li&gt;
&lt;li&gt;出力に softmax (回帰ではなく分類)&lt;ul&gt;
&lt;li&gt;単にサンプリングすると品質が落ちるので、μ-lawアルゴリズムを最初に適用&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;"Dan Jurafsky" 「今では、音声合成は言語モデルと同じ問題だ」&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ベイジアン End-to-End&lt;ul&gt;
&lt;li&gt;積分をアンサンブルで近似&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;WaveNet ではじめて連結方式を超えた&lt;ul&gt;
&lt;li&gt;自然さやモデルの柔軟性では生成モデルのほうが連結方式よりも上&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;課題&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;文脈依存性 → 答えになる単語を強調&lt;/li&gt;
&lt;li&gt;音声合成と音声認識を一つのシステムとして訓練&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;英語&lt;/th&gt;
&lt;th&gt;日本語&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;filterbank&lt;/td&gt;
&lt;td&gt;フィルタバンク (フィルタの集合)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;modulate&lt;/td&gt;
&lt;td&gt;変調する&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;fricative&lt;/td&gt;
&lt;td&gt;摩擦音&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;click&lt;/td&gt;
&lt;td&gt;パチパチ音&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;degenerative&lt;/td&gt;
&lt;td&gt;退行性&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;intelligible&lt;/td&gt;
&lt;td&gt;理解できる&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;muffled&lt;/td&gt;
&lt;td&gt;音がくぐもった&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Houston&lt;/td&gt;
&lt;td&gt;ハウストン　(ニューヨークの通り)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Houston&lt;/td&gt;
&lt;td&gt;ヒューストン (テキサスの都市)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr/&gt;

&lt;p&gt;&lt;a name="lecture11"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;講義11: 質問応答&lt;/h2&gt;
&lt;p&gt;講師：Karl Moritz Hermann (DeepMind)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;なぜ質問応答が重要か&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;質問応答は AI 完全&lt;ul&gt;
&lt;li&gt;QA が解ければ他の問題も解ける&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;多くの応用 (検索、対話、情報抽出, ...)&lt;/li&gt;
&lt;li&gt;最近の良い結果 (例：IBM Watson Jeopardy!)&lt;/li&gt;
&lt;li&gt;多くの課題（比較的容易なものも含め）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;質問応答は３種類のデータに依存&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;質問&lt;/li&gt;
&lt;li&gt;文脈/ソース(出典)&lt;/li&gt;
&lt;li&gt;応答 (これ自体が質問であることも)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;質問の分類&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;5W1H&lt;/li&gt;
&lt;li&gt;質問の主語&lt;/li&gt;
&lt;li&gt;予測される応答の種類&lt;/li&gt;
&lt;li&gt;応答を引き出す出典の種類&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;「応答」にまず注目。QAシステムを作る際には&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;応答はどういった形式か&lt;/li&gt;
&lt;li&gt;どこから応答を引き出してくるか&lt;/li&gt;
&lt;li&gt;訓練データはどういった形式か&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;をまず考えるのが有用。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;質問応答の種類&lt;ul&gt;
&lt;li&gt;読解理解&lt;/li&gt;
&lt;li&gt;意味解析&lt;/li&gt;
&lt;li&gt;画像質問応答&lt;/li&gt;
&lt;li&gt;情報検索&lt;/li&gt;
&lt;li&gt;図書参照&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;意味解析 (Semantic Parsing)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;自然言語を意味の形式表現に変換 → 論理表現を使ってデータベースを検索&lt;/li&gt;
&lt;li&gt;知識ベース&lt;ul&gt;
&lt;li&gt;３つ組で知識を格納 (関係, エンティティ1, エンティティ2)&lt;/li&gt;
&lt;li&gt;自由で利用可能な知識ベース (FreeBase, WikiData, etc.)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;知識ベースは簡単に利用できるが、訓練データを入手するのは大変&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;言語を論理表現に変換するための訓練を経た人しかできない (Amazon Mechanical Turk が使えない)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;深層学習によるアプローチ&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;機械翻訳と同じモデル（系列変換）&lt;/li&gt;
&lt;li&gt;問題点：訓練データが少ない、目的言語（論理表現）が複雑、固有名詞や数字の扱いが難しい&lt;/li&gt;
&lt;li&gt;解決策：論理表現に頼らない。論理表現を潜在的なものとして扱い、質問→応答を直接学習&lt;/li&gt;
&lt;li&gt;改善手法：アテンションを使う、目的言語側での制約を使う、半教師あり学習を使う&lt;/li&gt;
&lt;li&gt;複数のソースからの生成&lt;ul&gt;
&lt;li&gt;"Pointer Networks" の利用&lt;/li&gt;
&lt;li&gt;データベースを参照&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;読解理解&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;新聞記事＋質問（穴埋め形式） → 応答&lt;/li&gt;
&lt;li&gt;大規模コーパス (CNN/DailyMail, CBT, SQuAD)&lt;ul&gt;
&lt;li&gt;前提：訓練時には出典を見ない、答えは出典の中に単語かフレーズの形で含まれる&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;固有名詞と OOV を匿名のマーカーに置換&lt;ul&gt;
&lt;li&gt;語彙サイズの削減、訓練したモデルの一般化&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ニューラルモデル：P(a|q, d) をモデル化&lt;/li&gt;
&lt;li&gt;d → 双方向 LSTM, q → 双方向 LSTM, 合成&lt;/li&gt;
&lt;li&gt;アテンションを利用した読解理解 → 時刻 t ごとの文書dの表現を求め、クエリ表現と合成&lt;/li&gt;
&lt;li&gt;Attention Sum Reader: 応答は文書の中に含まれているという事実を使い、答えが位置 i にある確率をそのままモデル化&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;応答文選択&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;質問に対し、応答として使える文をコーパス（ウェブ全体）選ぶ&lt;/li&gt;
&lt;li&gt;データセット：TREC QA Track, MS MARCO&lt;/li&gt;
&lt;li&gt;ニューラルモデル：応答候補 a クエリ q に対して、sigmoid(q^T M a + b) を計算&lt;/li&gt;
&lt;li&gt;評価：精度、MRR (平均逆順位)、BLEU&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;画像QA&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;データ・セット (VisualQA, VQA 2.0, COCO-QA)&lt;/li&gt;
&lt;li&gt;ニューラルモデル：質問→何らかのエンコーダ, 画像→畳み込みネット&lt;/li&gt;
&lt;li&gt;「盲目モデル」（画像を見ない）でもそこそこ上手く行く&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;読解理解とタスクが似ている → アテンションのような同様のテクニックが使える&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;まとめ：QAシステムのつくりかた&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;タスクは何か&lt;/li&gt;
&lt;li&gt;質問、応答、文脈はどんなものか&lt;/li&gt;
&lt;li&gt;データはどこから来るか&lt;/li&gt;
&lt;li&gt;データを補充できるか&lt;/li&gt;
&lt;li&gt;質問と文脈をどうエンコードするか&lt;/li&gt;
&lt;li&gt;質問と文脈をどう組み合わせるか&lt;/li&gt;
&lt;li&gt;答えをどう予測・生成するか&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;英語&lt;/th&gt;
&lt;th&gt;日本語&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;low hanging fruit&lt;/td&gt;
&lt;td&gt;簡単に解決できる問題&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;MET office&lt;/td&gt;
&lt;td&gt;イギリス気象庁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;factoid&lt;/td&gt;
&lt;td&gt;(イギリス英語) 疑似事実&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;defunct&lt;/td&gt;
&lt;td&gt;機能していない&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;grounded&lt;/td&gt;
&lt;td&gt;基底的 (真偽値が求められる)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;extrapolate&lt;/td&gt;
&lt;td&gt;外挿する&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;anonymize&lt;/td&gt;
&lt;td&gt;匿名化する&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;garbage in, garbage out&lt;/td&gt;
&lt;td&gt;質の低い入力からは質の低い出力しか生まれないこと&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr/&gt;

&lt;p&gt;&lt;a name="lecture12"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;講義12: 記憶&lt;/h2&gt;
&lt;p&gt;講師: Ed Grefenstette (DeepMind)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;RNN 復習&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;RNN 入力→出力 + 隠れ状態 ht -&amp;gt; ht+1 に更新&lt;/li&gt;
&lt;li&gt;長距離の依存を扱える&lt;/li&gt;
&lt;li&gt;多くのNLPタスクが、変換タスクとして捉えられる (構文解析、翻訳、計算)&lt;ul&gt;
&lt;li&gt;Learning to Execute → Python プログラムを１文字ずつ読み込み、実行結果を１文字ずつ出力&lt;ul&gt;
&lt;li&gt;注：評価時に、「正しい」系列を入力して次の文字を予測。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;計算の階層&lt;ul&gt;
&lt;li&gt;有限状態機械（正規言語） → プッシュダウン・オートマトン (文脈自由言語) → チューリングマシン (計算可能な関数)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;変換モデルのボトルネック&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;容量が可変ではない&lt;ul&gt;
&lt;li&gt;原文の全ての情報を隠れ状態に保持しないといけない&lt;/li&gt;
&lt;li&gt;対象言語のモデリングに大部分の時間がかかる&lt;/li&gt;
&lt;li&gt;エンコーダーに伝わる勾配が小さい&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;RNNの限界&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;チューリングマシン → RNNに変換化　ただし学習可能とは限らない&lt;/li&gt;
&lt;li&gt;単純な RNN は、チューリングマシンを学習できない&lt;/li&gt;
&lt;li&gt;RNN は、有限状態機械の近似&lt;/li&gt;
&lt;li&gt;RNN の状態は、コントローラーと記憶両方の役割&lt;/li&gt;
&lt;li&gt;長距離の依存関係は、より大容量の記憶が必要&lt;/li&gt;
&lt;li&gt;有限状態機械は、そもそも限界がある&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;RNN 再考&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;APIの視点から考える（前の状態＋入力 → 次の状態＋出力） vanilla RNN も LSTM も同じ&lt;/li&gt;
&lt;li&gt;コントローラーと記憶を分ける&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;アテンション&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;あるデータを表現するベクトルの配列&lt;/li&gt;
&lt;li&gt;入出力ロジックを制御するコントローラー&lt;/li&gt;
&lt;li&gt;各時刻で記憶を読む&lt;/li&gt;
&lt;li&gt;記憶に勾配を蓄積&lt;/li&gt;
&lt;li&gt;Early Fusion vs Late Fusion → 記憶から読み込んだデータを入力に統合するか、出力に統合するか&lt;/li&gt;
&lt;li&gt;エンコーダー・デコーダーモデルのためのROM&lt;ul&gt;
&lt;li&gt;エンコーダーに勾配が別の経路で伝わる → ボトルネックの回避&lt;/li&gt;
&lt;li&gt;ソフトアラインメントの学習&lt;/li&gt;
&lt;li&gt;長い系列の中で情報を見つけやすい&lt;/li&gt;
&lt;li&gt;記憶が固定&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;テキスト含意&lt;ul&gt;
&lt;li&gt;Premise (前提) と Hypothesis (仮説) → 矛盾/中立/含意&lt;/li&gt;
&lt;li&gt;単純なモデル：前提＋仮説に RNN + アテンションを適用&lt;/li&gt;
&lt;li&gt;読解理解&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;レジスタ機械&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;コントローラーが RAM に影響を及ぼす&lt;/li&gt;
&lt;li&gt;コントローラーが（記憶アクセス用の）キーを生成 → アテンションのように使う&lt;/li&gt;
&lt;li&gt;チューリングマシンとの関係：一般的なアルゴリズムを学習するのは難しいが、特定の条件下では可能 (e.g., Graves et al. 2014)&lt;/li&gt;
&lt;li&gt;複雑な推論には、RNN+アテンション以上に複雑で表現力の高いものが必要&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ニューラル・プッシュダウン・オートマトン&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ニューラル・スタック&lt;/li&gt;
&lt;li&gt;コントローラーが、push/pop動作 + データを決める&lt;/li&gt;
&lt;li&gt;連続値スタック  各ベクトル（データ）に確信度を付与。push/pop は確信度を加算/減算&lt;/li&gt;
&lt;li&gt;人工タスク：値のコピー、反転&lt;/li&gt;
&lt;li&gt;言語タスク：SVO から SOV への変換  性の無い言語から有る言語への変換&lt;/li&gt;
&lt;li&gt;LSTM でも収束するが、学習が遅い（正規言語の近似を学習している）&lt;/li&gt;
&lt;li&gt;Stack, Queue, DeQueue, それぞれ得意な問題が違う&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;英語&lt;/th&gt;
&lt;th&gt;日本語&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;esoteric&lt;/td&gt;
&lt;td&gt;難解な&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;with a grain of salt&lt;/td&gt;
&lt;td&gt;話半分に&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;hierarchy&lt;/td&gt;
&lt;td&gt;階層&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;premise&lt;/td&gt;
&lt;td&gt;前提&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;hypothesis&lt;/td&gt;
&lt;td&gt;仮説&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr/&gt;

&lt;p&gt;&lt;a name="lecture13"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;講義13: ニューラルネットにおける言語知識&lt;/h2&gt;
&lt;p&gt;講師: Chris Dyer&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;言語学とは&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;言語でどう意味を表現するか&lt;/li&gt;
&lt;li&gt;脳がどう言語を処理・生成するか&lt;/li&gt;
&lt;li&gt;人間の言語にはどういうものが可能か&lt;/li&gt;
&lt;li&gt;人間の子供は、少ないデータからどう言語を学ぶか&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;文の階層関係&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;not と anybody の間にNPI (negative polarity item; 否定極性項目) の関係&lt;/li&gt;
&lt;li&gt;"not" は anybody よりも先に来ないといけない (木の親)&lt;/li&gt;
&lt;li&gt;仮説: 子供が言語を簡単に学ぶのは、構造的にあきらかに意味をなさない仮説を考えていないから&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;RNN&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;RNN は非常に強力なモデル&lt;/li&gt;
&lt;li&gt;チューリング完全&lt;/li&gt;
&lt;li&gt;どういった帰納的バイアスがあるか&lt;/li&gt;
&lt;li&gt;どういった過程を置くか&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;RNN の&lt;a href="https://en.wikipedia.org/wiki/Inductive_bias"&gt;帰納的バイアス&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;難しい問題&lt;/li&gt;
&lt;li&gt;系列的新近性を優先&lt;/li&gt;
&lt;li&gt;証拠：勾配、系列の反転などの実験、アテンション&lt;/li&gt;
&lt;li&gt;チョムスキー「人間の言語を効率的に学ぶためには、系列的新近性は良いバイアスではない」&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;言語はどう意味を表現するか&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;"This film is hardly a treat."&lt;/li&gt;
&lt;li&gt;否定 "hardly" が入る&lt;/li&gt;
&lt;li&gt;Bag-of-words モデルでは難しい&lt;/li&gt;
&lt;li&gt;系列的 RNN でもおそらくうまくいく&lt;/li&gt;
&lt;li&gt;合成の原則：表現の意味は、個別の構成要素の意味とそれを合成する規則から成る&lt;/li&gt;
&lt;li&gt;統語論の木表現 → 構文木&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;再帰的ニューラルネットワーク&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;構文木の各節点に隠れ状態  h = tanh(W[l;r] + b) として再帰的に構成する&lt;/li&gt;
&lt;li&gt;節点の種類(動詞, 名詞, etc.)によって合成ルールを変える → W を変える&lt;/li&gt;
&lt;li&gt;映画のレビューを構文解析し、各節点について極性（ポジティブ、ネガティブ）をアノテート&lt;/li&gt;
&lt;li&gt;否定によって、極性がまったく変わってしまうことも&lt;/li&gt;
&lt;li&gt;精度を見ると、Bigram + ナイーブベイズより少し良いだけ&lt;ul&gt;
&lt;li&gt;ただし、"not good" "not terrible" などの否定を伴う表現で非常に高精度&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;多くの拡張&lt;ul&gt;
&lt;li&gt;セルの定義、樹状LSTM、N個の子節点、プログラミング言語への応用&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;リカレント（繰り返し型）との比較&lt;ul&gt;
&lt;li&gt;利点：構文に従った意味の表現　良い帰納的バイアス　勾配の逆伝播距離が短い　中間節点へのアノテーション&lt;/li&gt;
&lt;li&gt;欠点：構文木が必要　右分岐した時勾配の伝播距離が長い　バッチ計算しにくい&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;構文解析&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;RNN 文法&lt;/li&gt;
&lt;li&gt;RNN を使ってシンボル＋制御シンボルを生成&lt;/li&gt;
&lt;li&gt;木のトップダウン、左→右の表現を生成 (木のS式表現)&lt;/li&gt;
&lt;li&gt;スタックにこれまでに生成されたシンボルを保持&lt;/li&gt;
&lt;li&gt;次のアクションの確率をどう求めるか&lt;ul&gt;
&lt;li&gt;長さに上限が無い → RNN&lt;/li&gt;
&lt;li&gt;部分木の複雑さに上限が無い → 再帰的ニューラルネット&lt;/li&gt;
&lt;li&gt;状態をあまり更新しない → stack RNN&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;スタックRNN&lt;ul&gt;
&lt;li&gt;PUSH と POP の２つの演算&lt;/li&gt;
&lt;li&gt;PUSH した時に前にあった要素との間に接続を作る → 接続が木構造と同じになる&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;系列的新近性よりも構文の新近性を重視&lt;/li&gt;
&lt;li&gt;パラメータ推定&lt;ul&gt;
&lt;li&gt;生成モデル p(x, y), 文 x と構文木 y&lt;/li&gt;
&lt;li&gt;識別モデル GEN のかわりに、SHIFT 操作を使う&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;解析&lt;ul&gt;
&lt;li&gt;ビームサーチを使う&lt;/li&gt;
&lt;li&gt;条件付きの言語モデル　アクション列の確率をモデリングしている&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;結果&lt;ul&gt;
&lt;li&gt;生成モデルの方が良い。従来手法より高い F値&lt;/li&gt;
&lt;li&gt;言語モデルとして使うと LSTM+dropout より良い&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;単語の表現&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;任意性&lt;ul&gt;
&lt;li&gt;car - c + b = bar&lt;/li&gt;
&lt;li&gt;cat - b + b = bat   (同じ演算なのに結果が全く違う)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;文字に意味はあるか？&lt;ul&gt;
&lt;li&gt;cool   cooool   coooooool  ← 意味が予測できる&lt;/li&gt;
&lt;li&gt;cat + s = cats,  bat + s = bats ← 規則的&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;単語を構造のあるオブジェクトとして見る&lt;ul&gt;
&lt;li&gt;形態素解析し、形態素ごとのベクトルを合成&lt;/li&gt;
&lt;li&gt;
&lt;ol&gt;
&lt;li&gt;元のベクトル、2. 形態素ベクトルの合成、3. 文字ベクトルの合成、を連結&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;複数の生成モードを混合し、単語を生成&lt;/li&gt;
&lt;li&gt;トルコ語とフィンランド語での言語モデリング&lt;ul&gt;
&lt;li&gt;モードが増えるほど、性能が向上&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ニューラルネットの言語概念の解析&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/pdf/1611.01368.pdf"&gt;Lizen, Dupoux, Goldberg 2017&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;言語モデリングの代わりに、数の一致を予測&lt;/li&gt;
&lt;li&gt;Wikipedia の文を自動でアノテーション&lt;/li&gt;
&lt;li&gt;結果&lt;ul&gt;
&lt;li&gt;途中にはさまる名詞が無い場合 → 距離が14まで基本的にエラー率は 0&lt;/li&gt;
&lt;li&gt;途中にはさまる名詞の数に影響される (エラー率５%ほど)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;他の実験&lt;ul&gt;
&lt;li&gt;文法的か、非文法的か&lt;/li&gt;
&lt;li&gt;言語モデリングを目的関数とした場合、うまくいかない。構文を一般的に学習しているわけではない&lt;/li&gt;
&lt;li&gt;文法チェッカーを作る場合は注意！（文法を直接学習させたほうが良い）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;まとめ&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;言語学の利点：より良いモデルが作れる　モデルがきちんと動いているか調べられる&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;英語&lt;/th&gt;
&lt;th&gt;日本語&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;empirical&lt;/td&gt;
&lt;td&gt;経験的な&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;constituent&lt;/td&gt;
&lt;td&gt;構成要素&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;parse&lt;/td&gt;
&lt;td&gt;構文木, 構文解析結果&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;idiosyncratic&lt;/td&gt;
&lt;td&gt;独特の&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;confound&lt;/td&gt;
&lt;td&gt;交絡&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;</content></entry><entry><title>英語で読む「Google のソフトウェアエンジニアリング」</title><link href="http://englishforhackers.com/read-software-engineering-at-google-in-english.html" rel="alternate"></link><published>2018-01-01T00:00:00-05:00</published><updated>2018-01-01T00:00:00-05:00</updated><author><name>Masato Hagiwara</name></author><id>tag:englishforhackers.com,2018-01-01:/read-software-engineering-at-google-in-english.html</id><summary type="html">&lt;p&gt;Google にて10年以上ソフトウェアエンジニアとして働き、現在は最近では text-to-speech の研究に従事しているFergus Henderson氏による&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/pdf/1702.01715.pdf"&gt;Software Engineering at Google&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;という記事。Google のソフトウェアエンジニアリングのプロセスや文化が簡単にまとまっていて参考になる点も多いと思います。主要なポイントを簡単にまとめると：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;単一の巨大なリポジトリ。どのエンジニアでも（検索ランキングやセキュリティなどに関わる重要な部分を除き）自由にチェックアウトしてコードが読める。&lt;/li&gt;
&lt;li&gt;分散ビルドシステム Blaze (100〜1000台のサーバーで分散ビルド・テスト) キャッシュにより高速なコンパイルを実現。&lt;/li&gt;
&lt;li&gt;厳格なコードレビュー文化。メインのリポジトリにコミットするコードは必ずコードレビューを経なければいけない。&lt;/li&gt;
&lt;li&gt;ユニットテスト、負荷テスト。&lt;/li&gt;
&lt;li&gt;スタイルガイド、５つの公式言語(C++, Java, Python, Go, JavaScript)のサポート。&lt;/li&gt;
&lt;li&gt;頻繁なリリース（１，２週間ペース） リリースの自動化、ステージング環境における本番トラフィックを使った統合テスト。&lt;/li&gt;
&lt;li&gt;リリース承認プロセス・ツール。&lt;/li&gt;
&lt;li&gt;ポストモーテム：大きな障害が起きた時に書く文書。影響（クエリ数や経済的損失）、再発防止策などを書く。&lt;/li&gt;
&lt;li&gt;数年ごとにソフトウェアの書き直し。&lt;/li&gt;
&lt;li&gt;20 …&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;p&gt;Google にて10年以上ソフトウェアエンジニアとして働き、現在は最近では text-to-speech の研究に従事しているFergus Henderson氏による&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/pdf/1702.01715.pdf"&gt;Software Engineering at Google&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;という記事。Google のソフトウェアエンジニアリングのプロセスや文化が簡単にまとまっていて参考になる点も多いと思います。主要なポイントを簡単にまとめると：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;単一の巨大なリポジトリ。どのエンジニアでも（検索ランキングやセキュリティなどに関わる重要な部分を除き）自由にチェックアウトしてコードが読める。&lt;/li&gt;
&lt;li&gt;分散ビルドシステム Blaze (100〜1000台のサーバーで分散ビルド・テスト) キャッシュにより高速なコンパイルを実現。&lt;/li&gt;
&lt;li&gt;厳格なコードレビュー文化。メインのリポジトリにコミットするコードは必ずコードレビューを経なければいけない。&lt;/li&gt;
&lt;li&gt;ユニットテスト、負荷テスト。&lt;/li&gt;
&lt;li&gt;スタイルガイド、５つの公式言語(C++, Java, Python, Go, JavaScript)のサポート。&lt;/li&gt;
&lt;li&gt;頻繁なリリース（１，２週間ペース） リリースの自動化、ステージング環境における本番トラフィックを使った統合テスト。&lt;/li&gt;
&lt;li&gt;リリース承認プロセス・ツール。&lt;/li&gt;
&lt;li&gt;ポストモーテム：大きな障害が起きた時に書く文書。影響（クエリ数や経済的損失）、再発防止策などを書く。&lt;/li&gt;
&lt;li&gt;数年ごとにソフトウェアの書き直し。&lt;/li&gt;
&lt;li&gt;20%タイム。&lt;/li&gt;
&lt;li&gt;OKR (Objective Key Results)：年・四半期単位の具体的目標。目標は高く (平均して65%程度の達成率を目指すように) 設定する。&lt;/li&gt;
&lt;li&gt;役職 (Engineering Manager, Tech Lead) 技術キャリアと管理職キャリアの区別。研究者もエンジニアも研究して論文を書ける。&lt;/li&gt;
&lt;li&gt;"Codelabs" によるエンジニアの研修。メンターの割り当て。&lt;/li&gt;
&lt;li&gt;素晴らしい働きをした同僚に会社負担で $100 を贈れる「ピアボーナス」制度。金銭ボーナス無しの賞賛「kudos」制度。&lt;/li&gt;
&lt;li&gt;委員会による昇進審議。低い業績の場合は業績改善計画 (PIP) の実施。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;英語は割と平易で読みやすいと思います。&lt;/p&gt;
&lt;p&gt;Post-mortem, OKR, 20% time, Peer Bonus など、色々な用語が含まれていますが、私が働いている &lt;a href="https://www.duolingo.com/"&gt;Duolingo&lt;/a&gt; をはじめ、
他のアメリカのテック系の会社でも採用しているところも多いのではないでしょうか。テックニュース等で聞くことも多いので知っておいて損はないかと思います。&lt;/p&gt;
&lt;p&gt;以下、本記事を読むにあたってキーとなる英単語・フレーズとその日本語訳を取り上げます。
意味が難しくても、一般に記事や論文などにあまり出てこない表現や、単に辞書を引けばわかるようなものはあえて取り上げていません。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;英語&lt;/th&gt;
&lt;th&gt;日本語&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;practice&lt;/td&gt;
&lt;td&gt;(この場合は「実行」「やり方」などの意味に近い)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;feasible&lt;/td&gt;
&lt;td&gt;実現可能な&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;declarative&lt;/td&gt;
&lt;td&gt;宣言的な&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;as (someone) see fit&lt;/td&gt;
&lt;td&gt;自らの判断で&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;cherry-pick&lt;/td&gt;
&lt;td&gt;好きなものだけを選ぶ&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;post-mortem&lt;/td&gt;
&lt;td&gt;検死、システム障害後の検証&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;burn out&lt;/td&gt;
&lt;td&gt;燃え尽きる&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;appraisal&lt;/td&gt;
&lt;td&gt;査定&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;have the final say&lt;/td&gt;
&lt;td&gt;最終決定権がある&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;controversial&lt;/td&gt;
&lt;td&gt;賛否両論ある&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;kudos&lt;/td&gt;
&lt;td&gt;賞賛&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;termination&lt;/td&gt;
&lt;td&gt;解雇&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;</content></entry></feed>