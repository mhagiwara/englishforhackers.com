<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>エンジニア・研究者の英語学習 - Reinforcement Learning</title><link href="http://englishforhackers.com/" rel="alternate"></link><link href="http://englishforhackers.com/feeds/reinforcement-learning.atom.xml" rel="self"></link><id>http://englishforhackers.com/</id><updated>2018-09-05T00:00:00-04:00</updated><entry><title>モデルフリー制御 - Google DeepMind の David Silver 氏による強化学習コース 講義5</title><link href="http://englishforhackers.com/david-silver-reinforcement-learning-lecture5.html" rel="alternate"></link><published>2018-09-05T00:00:00-04:00</published><updated>2018-09-05T00:00:00-04:00</updated><author><name>Masato Hagiwara</name></author><id>tag:englishforhackers.com,2018-09-05:/david-silver-reinforcement-learning-lecture5.html</id><summary type="html">&lt;script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async&gt;&lt;/script&gt;

&lt;p&gt;「無料でアクセスできる最高の強化学習のコース」と名高い、Google DeepMind / University College London の David Silver 氏による強化学習のコース。&lt;a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html"&gt;こちらのページから、全ての講義スライドと講義ビデオが見られる&lt;/a&gt;。以下は、講義5 のメモです。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;はじめに&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;非常に多くの問題が、MDP としてモデル化できる&lt;/li&gt;
&lt;li&gt;On-policy (方策オン型)&lt;ul&gt;
&lt;li&gt;「行動しながら学ぶ」&lt;/li&gt;
&lt;li&gt;学習している方策と、サンプルを生成する方策が同じ&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Off-policy (方策オフ型)&lt;ul&gt;
&lt;li&gt;「他の人の行動から学ぶ」&lt;/li&gt;
&lt;li&gt;学習している方策と、サンプルを生成する方策が違う&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;方策オン型　MC 制御&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;復習: 方策反復：1) 方策評価 \( v_\pi \) の推定 と、2) 方策改善 (貪欲的方策改善) を繰り返す&lt;/li&gt;
&lt;li&gt;ここに、MC 法による方策評価を組み込むことはできるか？&lt;ul&gt;
&lt;li&gt;問題点：\( V …&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async&gt;&lt;/script&gt;

&lt;p&gt;「無料でアクセスできる最高の強化学習のコース」と名高い、Google DeepMind / University College London の David Silver 氏による強化学習のコース。&lt;a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html"&gt;こちらのページから、全ての講義スライドと講義ビデオが見られる&lt;/a&gt;。以下は、講義5 のメモです。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;はじめに&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;非常に多くの問題が、MDP としてモデル化できる&lt;/li&gt;
&lt;li&gt;On-policy (方策オン型)&lt;ul&gt;
&lt;li&gt;「行動しながら学ぶ」&lt;/li&gt;
&lt;li&gt;学習している方策と、サンプルを生成する方策が同じ&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Off-policy (方策オフ型)&lt;ul&gt;
&lt;li&gt;「他の人の行動から学ぶ」&lt;/li&gt;
&lt;li&gt;学習している方策と、サンプルを生成する方策が違う&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;方策オン型　MC 制御&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;復習: 方策反復：1) 方策評価 \( v_\pi \) の推定 と、2) 方策改善 (貪欲的方策改善) を繰り返す&lt;/li&gt;
&lt;li&gt;ここに、MC 法による方策評価を組み込むことはできるか？&lt;ul&gt;
&lt;li&gt;問題点：\( V(s) \) に従って貪欲に行動しようとしても、MDP の完全な情報が必要 → 解法: 代わりに \( Q(s, a) \) を使う&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;\( Q = q_\pi \) を MC で評価する&lt;ul&gt;
&lt;li&gt;問題点：探索問題。貪欲的に行動すると、必要な状態に到達することができない&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;探索&lt;ul&gt;
&lt;li&gt;ε-貪欲探索&lt;ul&gt;
&lt;li&gt;確率εでランダムな行動を（一様分布に従って）取る&lt;/li&gt;
&lt;li&gt;ε-貪欲探索に従う新しい方策 \( \pi' \) は、前の方策 \( \pi \) よりも良い&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;MC で方策を評価し、ε-貪欲探索をすると？&lt;ul&gt;
&lt;li&gt;最適方策 \( \pi_* \) に到達する。どのぐらいかかるか分からない&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;モンテカルロ制御&lt;ul&gt;
&lt;li&gt;エピソードの完了 → Q を更新&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;GLIE (Greedy in the Limit with Infinite Exploration) &lt;ul&gt;
&lt;li&gt;全ての状態-行動ペアを、無限回探索する　&lt;/li&gt;
&lt;li&gt;方策が貪欲方策に収束する&lt;/li&gt;
&lt;li&gt;GLIE モンテカルロ制御&lt;ul&gt;
&lt;li&gt;各状態-行動ペアについて、\( G_t \) の平均を \( Q(S_t, A_t) \) として保持&lt;/li&gt;
&lt;li&gt;新しいQ値に従い、ε-貪欲方策を更新&lt;/li&gt;
&lt;li&gt;最適な行動価値関数 \( q_*(s, a) \) に収束&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;方策オン型 TD 学習&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;アイデア：MC の代わりに TD を制御ループの時に使う&lt;ul&gt;
&lt;li&gt;TD を \( Q(S, A) \) の推定に使う&lt;/li&gt;
&lt;li&gt;Sarsa: なぜ Sarsa? (S, A) → R → S' → A'&lt;/li&gt;
&lt;li&gt;更新式: \( Q(S, A) \leftarrow Q(S, A) + \alpha( R + \gamma Q(S', A') - Q(S, A) ) \)&lt;/li&gt;
&lt;li&gt;方策改善には、ε貪欲探索を使う&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Sarsa は、\( Q(s, a) \to q_*(s, a) \) に収束する (条件付きだが、ほとんどの場合成り立つ)&lt;/li&gt;
&lt;li&gt;nステップ Sarsa&lt;ul&gt;
&lt;li&gt;nステップ先までの報酬を考慮。例: \( q_t^{(2)} = R_{t+1} + \gamma R_{t+2} + \gamma^2 Q(S_{t+2}) \)&lt;/li&gt;
&lt;li&gt;\( Q(S_t, A) \leftarrow Q(S_t, A) + \alpha (q_t^{(n)} - Q(S_t, A)) \)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Sarsa(λ) の前向き観点&lt;ul&gt;
&lt;li&gt;\( q^\lambda_t = (1 - \lambda) \sum_{n=1}^\infty \lambda^{n-1}q_t^{(n)} \)&lt;/li&gt;
&lt;li&gt;\( Q(S_t, A) \leftarrow Q(S_t, A) + \alpha (q^\lambda_t - Q(S_t, A)) \)&lt;/li&gt;
&lt;li&gt;問題点：時間軸上で先読みしている。エピソードの最後まで待ちたくない&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Sarsa(λ) の後ろ向き観点&lt;ul&gt;
&lt;li&gt;Eligibility Trace \( E_t(s, a) \) を定義 (TD(λ) と違い、状態と行動のペアに対して定義)&lt;/li&gt;
&lt;li&gt;\( \delta_t = R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \) &lt;/li&gt;
&lt;li&gt;\( Q(s, a) \leftarrow Q(s, a) + \alpha \delta_t E_t(s, a) \)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;方策オフ型&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;別の方策 \( \mu \) に従いながら、対象の方策 \( \pi \) を評価&lt;/li&gt;
&lt;li&gt;なぜこれが重要か&lt;ul&gt;
&lt;li&gt;人間や他のエージェントから学ぶ&lt;/li&gt;
&lt;li&gt;古い方策によって作られた経験から学習&lt;/li&gt;
&lt;li&gt;探索的な方策から、最適な方策を学習&lt;/li&gt;
&lt;li&gt;一つの方策から、複数の方策を学習&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;重要サンプリング&lt;ul&gt;
&lt;li&gt;異なる分布の期待値を予測する&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;方策オフ型のモンテカルロ法に重要サンプリングを適用する&lt;ul&gt;
&lt;li&gt;非常に大きい分散。ほとんど使えない。&lt;/li&gt;
&lt;li&gt;TD 学習を使うことが必須&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Q-Learning&lt;ul&gt;
&lt;li&gt;振る舞いを規定する方策 \( \mu \) によって取られた（実際の）行動 \( A_{t+1} \)&lt;/li&gt;
&lt;li&gt;学習中の方策によって取られた（仮想的な）行動 \( A' \)&lt;/li&gt;
&lt;li&gt;\( Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha (R_{t+1} + \gamma Q(S_{t+1}, A') - Q(S^t, A)) \)&lt;/li&gt;
&lt;li&gt;方策オフ型 Q-Learning → 学習したい方策が貪欲的な場合 (SarsaMax)&lt;ul&gt;
&lt;li&gt;最適価値関数に収束&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;</content></entry><entry><title>モデルフリー予測 - Google DeepMind の David Silver 氏による強化学習コース 講義4</title><link href="http://englishforhackers.com/david-silver-reinforcement-learning-lecture4.html" rel="alternate"></link><published>2018-09-04T00:00:00-04:00</published><updated>2018-09-04T00:00:00-04:00</updated><author><name>Masato Hagiwara</name></author><id>tag:englishforhackers.com,2018-09-04:/david-silver-reinforcement-learning-lecture4.html</id><summary type="html">&lt;script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async&gt;&lt;/script&gt;

&lt;p&gt;「無料でアクセスできる最高の強化学習のコース」と名高い、Google DeepMind / University College London の David Silver 氏による強化学習のコース。&lt;a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html"&gt;こちらのページから、全ての講義スライドと講義ビデオが見られる&lt;/a&gt;。以下は、講義4 のメモです。&lt;/p&gt;
&lt;p&gt;モデルフリー予測 = 未知の MDP の価値関数を推定する&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;モンテカルロ (MC) 学習&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;経験のエピソードから直接学習する&lt;/li&gt;
&lt;li&gt;エピソードが終了する必要あり&lt;/li&gt;
&lt;li&gt;方策 \( \pi \) の下で、経験のエピソード \( S_1, A_1, R_2, ..., S_k \sim \pi \) から \( v_\pi \) を学習&lt;/li&gt;
&lt;li&gt;復習： 利得 \( G_t = R_{t+1} + \gamma R_{t+2} + ... + \gamma …&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async&gt;&lt;/script&gt;

&lt;p&gt;「無料でアクセスできる最高の強化学習のコース」と名高い、Google DeepMind / University College London の David Silver 氏による強化学習のコース。&lt;a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html"&gt;こちらのページから、全ての講義スライドと講義ビデオが見られる&lt;/a&gt;。以下は、講義4 のメモです。&lt;/p&gt;
&lt;p&gt;モデルフリー予測 = 未知の MDP の価値関数を推定する&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;モンテカルロ (MC) 学習&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;経験のエピソードから直接学習する&lt;/li&gt;
&lt;li&gt;エピソードが終了する必要あり&lt;/li&gt;
&lt;li&gt;方策 \( \pi \) の下で、経験のエピソード \( S_1, A_1, R_2, ..., S_k \sim \pi \) から \( v_\pi \) を学習&lt;/li&gt;
&lt;li&gt;復習： 利得 \( G_t = R_{t+1} + \gamma R_{t+2} + ... + \gamma^{T-1}R_T \)&lt;/li&gt;
&lt;li&gt;復習： 価値関数 \( v_\pi(s) = E_\pi[G_t | S_t = s] \)&lt;/li&gt;
&lt;li&gt;モンテカルロ方策評価：各エピソードについて、状態 \( s \) を最初に訪問した時に&lt;ul&gt;
&lt;li&gt;カウンターと利得の合計を更新。&lt;/li&gt;
&lt;li&gt;\( V(s) \) → 利得の合計 / 訪問回数&lt;/li&gt;
&lt;li&gt;十分多くの \( N(s) \) を観察すると、\( V(s) \) は \( v_\pi(s) \) に収束&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;各訪問ごとに更新する場合&lt;ul&gt;
&lt;li&gt;訪問回数を、各訪問ごとに更新&lt;/li&gt;
&lt;li&gt;これでも、\( V(s) \to v_\pi(s) \)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;「差分」を使った系列の平均計算&lt;ul&gt;
&lt;li&gt;系列 \( x_1, x_2, ... \) を全て観測し終わらなくても、それまでの平均 \( \mu_k \) を計算することができる&lt;/li&gt;
&lt;li&gt;\( \mu_k = \mu_{k-1} + \frac{1}{k} (x_k - \mu_{k-1}) \) &lt;/li&gt;
&lt;li&gt;直感的な説明： 新しい値を観測した時、それまでの推定値と大きくことなる場合は、推定値を大きく更新する。&lt;/li&gt;
&lt;li&gt;\( \mu_{k-1} \) →.それまでの推定値。\( x_k - \mu_{k-1} \) → 新しい値を観測した時の「驚き」。\( \frac{1}{k} \) → 学習率&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;差分モンテカルロ更新&lt;ul&gt;
&lt;li&gt;エピソード毎に更新（全てのエピソードの「和」を保持しない）&lt;/li&gt;
&lt;li&gt;\( N(S_t) \leftarrow N(S_t) + 1 \) &lt;/li&gt;
&lt;li&gt;\( V(S_t) \leftarrow V(S_t) + \frac{1}{N(S_t)}(G_t - V(S_t)) \)&lt;/li&gt;
&lt;li&gt;注：エピソードが終わるまで待つ必要あり&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;時間差分 (Temporal Difference) 学習&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;経験のエピソードから直接学習する (モンテカルロ法と同じ)&lt;/li&gt;
&lt;li&gt;エピソードが終了してなくても良い → ブートストラップ法 (モンテカルロ法との差異)&lt;/li&gt;
&lt;li&gt;MC と TD の違い&lt;ul&gt;
&lt;li&gt;MC: 実際の利得 \( G_t \) を使って更新: \( V(S_t) \leftarrow V(S_t) + \alpha(G_t - V(S_t)) \)&lt;/li&gt;
&lt;li&gt;TD: 利得の予測値を使って更新: \( V(S_t) \leftarrow V(S_t) + \alpha(R_{t+1} + \gamma V(S_{t+1}) - V(S_t)) \)&lt;ul&gt;
&lt;li&gt;\( R_{t+1} + \gamma V(S_{t+1}) \) は TD ターゲットと呼ばれる&lt;/li&gt;
&lt;li&gt;\( \delta_t = R_{t+1} + \gamma V(S_{t+1}) - V(S_t) \) は TD 誤差と呼ばれる&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;TD の長所・欠点&lt;ul&gt;
&lt;li&gt;最終結果を見る前に学習できる。最終結果の無いエピソードでも学習できる&lt;/li&gt;
&lt;li&gt;偏り (Bias) と分散　(Variance) のトレードオフ&lt;ul&gt;
&lt;li&gt;真の TD ターゲット \( R_{t+1} + \gamma v_\pi(S_{t+1}) \) は \( v_\pi(S_t) \)の 不偏推定量&lt;/li&gt;
&lt;li&gt;TD ターケット \( R_{t+1} + \gamma V(S_{t+1}) \) は、偏りのある推定量　&lt;/li&gt;
&lt;li&gt;ただし、TD ターゲットは、利得よりも分散が小さい&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;MC は分散大、偏りゼロ。TD は分散小、偏りあり。&lt;/li&gt;
&lt;li&gt;TD(0) は \( v_\pi(s) \) に収束。初期値に敏感&lt;/li&gt;
&lt;li&gt;MC は、観察された利得との平均二乗誤差を最小化する解に収束する&lt;/li&gt;
&lt;li&gt;TD は、データに対して尤度最大の MDPを学習し、それを解く&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ブートストラップとサンプリング&lt;ul&gt;
&lt;li&gt;MC → ブートストラップ無し&lt;/li&gt;
&lt;li&gt;DP → ブートストラップ有り&lt;/li&gt;
&lt;li&gt;TD → ブートストラップ有り&lt;/li&gt;
&lt;li&gt;MC → サンプリング有り&lt;/li&gt;
&lt;li&gt;DP → サンプリング無し&lt;/li&gt;
&lt;li&gt;TD → サンプリング無し&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;TD(λ)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;TD ターゲットを計算するときに、nステップ先読みする&lt;ul&gt;
&lt;li&gt;例：\( G_t^{(2)}  = R_{t+1} + \gamma R_{t+2} + \gamma^2 V(S_{t+2}) \)&lt;/li&gt;
&lt;li&gt;n を大きくすると、MC 法に収束する&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;nステップ先読みを平均する&lt;ul&gt;
&lt;li&gt;例：\( \frac{1}{2} G_t^{(2)} + \frac{1}{2} G_t^{(4)} \)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;γ利得 → 全てのnステップ利得の幾何平均&lt;ul&gt;
&lt;li&gt;\( G_t^\lambda = (1-\lambda) \sum_{n=1}^\infty \lambda^{n-1} G_t^n \)&lt;/li&gt;
&lt;li&gt;なぜ幾何平均? 前の値を保持しなくて良いので、計算コストが低い。TD(0) と同じコストで計算できる。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;TD(λ) の「前向き」観点&lt;ul&gt;
&lt;li&gt;\( V(S_t) \leftarrow V(S_t) + \alpha (G^\lambda_t - V(S_t)) \)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;MC と同じように、エピソードが収束するまで待つ必要がある&lt;/li&gt;
&lt;li&gt;TD(λ) の「後ろ向き」観点&lt;ul&gt;
&lt;li&gt;Eligibility Trace&lt;ul&gt;
&lt;li&gt;最近性と、頻度を両方考慮する量&lt;/li&gt;
&lt;li&gt;\( E_0(s) = 0, E_t(s) = \gamma E_{t-1}(s) + {\mathbf 1}(S_t = s) \)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;\( V(s) \leftarrow V(s) + \alpha \delta_t E_t(s) \)&lt;/li&gt;
&lt;li&gt;\( \lambda = 0 \) の時 → TD(0) と等価&lt;/li&gt;
&lt;li&gt;\( \lambda = 1 \) の時 → 更新の合計は　MC と同じ&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;</content></entry><entry><title>動的計画法を使った計画 - Google DeepMind の David Silver 氏による強化学習コース 講義3</title><link href="http://englishforhackers.com/david-silver-reinforcement-learning-lecture3.html" rel="alternate"></link><published>2018-09-03T00:00:00-04:00</published><updated>2018-09-03T00:00:00-04:00</updated><author><name>Masato Hagiwara</name></author><id>tag:englishforhackers.com,2018-09-03:/david-silver-reinforcement-learning-lecture3.html</id><summary type="html">&lt;script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async&gt;&lt;/script&gt;

&lt;p&gt;「無料でアクセスできる最高の強化学習のコース」と名高い、Google DeepMind / University College London の David Silver 氏による強化学習のコース。&lt;a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html"&gt;こちらのページから、全ての講義スライドと講義ビデオが見られる&lt;/a&gt;。以下は、講義3 のメモです。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;はじめに&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;動的計画法&lt;ul&gt;
&lt;li&gt;「動的」: 逐次的、時間&lt;/li&gt;
&lt;li&gt;「計画」≒ 方策&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;動的計画法がいつ使えるか&lt;ul&gt;
&lt;li&gt;最適なサブ構造に分解し、そこから最適解が求められる場合&lt;ul&gt;
&lt;li&gt;例: グラフの最短経路問題&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;サブ問題がお互いに関係しており、何回も現れる場合 → キャッシュできる&lt;/li&gt;
&lt;li&gt;MDPはこの両方を満たす&lt;ul&gt;
&lt;li&gt;ベルマン方程式&lt;/li&gt;
&lt;li&gt;問題の再帰的な分解&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;例&lt;ul&gt;
&lt;li&gt;スケジュール&lt;/li&gt;
&lt;li&gt;文字列アルゴリズム&lt;/li&gt;
&lt;li&gt;グラフアルゴリズム&lt;/li&gt;
&lt;li&gt;グラフィカルアルゴリズム&lt;/li&gt;
&lt;li&gt;生物情報学&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;動的計画法を使った計画&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;MDP の情報が全て分かっている前提&lt;/li&gt;
&lt;li&gt;予測: MDP と方策 \( \pi \) が分かっている時に、価値関数 \( v_ …&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async&gt;&lt;/script&gt;

&lt;p&gt;「無料でアクセスできる最高の強化学習のコース」と名高い、Google DeepMind / University College London の David Silver 氏による強化学習のコース。&lt;a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html"&gt;こちらのページから、全ての講義スライドと講義ビデオが見られる&lt;/a&gt;。以下は、講義3 のメモです。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;はじめに&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;動的計画法&lt;ul&gt;
&lt;li&gt;「動的」: 逐次的、時間&lt;/li&gt;
&lt;li&gt;「計画」≒ 方策&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;動的計画法がいつ使えるか&lt;ul&gt;
&lt;li&gt;最適なサブ構造に分解し、そこから最適解が求められる場合&lt;ul&gt;
&lt;li&gt;例: グラフの最短経路問題&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;サブ問題がお互いに関係しており、何回も現れる場合 → キャッシュできる&lt;/li&gt;
&lt;li&gt;MDPはこの両方を満たす&lt;ul&gt;
&lt;li&gt;ベルマン方程式&lt;/li&gt;
&lt;li&gt;問題の再帰的な分解&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;例&lt;ul&gt;
&lt;li&gt;スケジュール&lt;/li&gt;
&lt;li&gt;文字列アルゴリズム&lt;/li&gt;
&lt;li&gt;グラフアルゴリズム&lt;/li&gt;
&lt;li&gt;グラフィカルアルゴリズム&lt;/li&gt;
&lt;li&gt;生物情報学&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;動的計画法を使った計画&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;MDP の情報が全て分かっている前提&lt;/li&gt;
&lt;li&gt;予測: MDP と方策 \( \pi \) が分かっている時に、価値関数 \( v_\pi \) を求める&lt;/li&gt;
&lt;li&gt;操作: MDP が分かっている時に、最適方策 \( \pi_* \) を求める&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;方策反復&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;MDP を解くための仕組みの一つ&lt;/li&gt;
&lt;li&gt;方策 \( \pi \) を評価する&lt;/li&gt;
&lt;li&gt;ベルマン方程式を逆向きに繰り返し適用&lt;/li&gt;
&lt;li&gt;任意の \( v_1 \) からスタート。ベルマン方程式を適用し、\( v_2 \) を得る。&lt;/li&gt;
&lt;li&gt;\( v_{k+1}(s) \) を計算するためには&lt;ul&gt;
&lt;li&gt;1ステップ先読みする。\( v_{k+1}(s) = \sum_{a \in A} \pi(a|s)( R^a_s + \gamma \sum_{s' \in S} P^a_{ss'} v_k(s') ) \)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;これを繰り返すと、\( v_* \) に収束する&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;方策をどう改善するか&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;方策 \( \pi \) が与えられた時、&lt;ul&gt;
&lt;li&gt;まず、方策 \( \pi \) を評価し、\( v_\pi(s) \) を得る&lt;/li&gt;
&lt;li&gt;\( v_\pi(s) \) に従い、貪欲に行動し、方策 \( \pi' \) を得る&lt;/li&gt;
&lt;li&gt;「格子世界」の例では、\( \pi' \) が最適方策 \( \pi_* \)&lt;/li&gt;
&lt;li&gt;一般的には、これを繰り返すと、最適方策に収束する&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;決定的な方策 \( \pi \) からスタート&lt;ul&gt;
&lt;li&gt;貪欲に行動することで、この方策を改善できる。 \( \pi'(s) = \arg\max_{a \in A} q_\pi(s, a) \)&lt;/li&gt;
&lt;li&gt;→ 価値関数も改善する&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;この繰り返し的な改善がストップする時 → ベルマン最適方程式を満たす → 方策は最適である \( v_\pi(s) = v_*(s) \)&lt;/li&gt;
&lt;li&gt;方策評価が収束するまで繰り返す必要があるか？ \( k \) 回繰り返せば十分。ただし \( k = 1 \) ではだめ。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;価値反復&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;MDP を解くためのもう一つの仕組み&lt;/li&gt;
&lt;li&gt;最適原則&lt;ul&gt;
&lt;li&gt;方策 \( \pi(a|s) \) は、以下の条件を満たす時、またその時に限って、最適価値関数 \( v_\pi(s) = v_*(s) \) を満たす。&lt;ul&gt;
&lt;li&gt;任意の状態 \( s' \) が \( s \) から到達可能&lt;/li&gt;
&lt;li&gt;状態 \( s' \) が、最適価値関数を満たす \( v_\pi(s') = v_*(s') \)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;もし、部分問題に対して最適解が分かっている時、\( v_*(s') \)&lt;ul&gt;
&lt;li&gt;１ステップ先読みする： \( v_*(s) \leftarrow \max_{a \in A} R^a_s + \gamma \sum_{s' \in S} P^a_{ss'}v_*(s') \)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;最適方策 \( \pi \) を探す&lt;ul&gt;
&lt;li&gt;\( v_1 \to v_2 \to ... \to v_* \)&lt;/li&gt;
&lt;li&gt;\( v_{k+1}(s) \) から \( v_k(s') \) を更新&lt;/li&gt;
&lt;li&gt;方策反復とは違い、方策を明示的に使わない&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;まとめ&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;予測 → ベルマン期待値方程式 - 反復方策評価&lt;/li&gt;
&lt;li&gt;操作 → ベルマン期待値方程式+貪欲的方策更新 - 方策反復&lt;/li&gt;
&lt;li&gt;操作 → ベルマン最適方程式 - 価値反復&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;拡張&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;非同期動的計画法&lt;ul&gt;
&lt;li&gt;他の状態の更新が終わるまで待たない。最適値に収束する&lt;/li&gt;
&lt;li&gt;In-place 動的計画法　&lt;ul&gt;
&lt;li&gt;価値関数の表を直接書き換える&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;優先度付き sweeping&lt;ul&gt;
&lt;li&gt;どの状態を次に更新するか優先度をつける&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;リアルタイム 動的計画法&lt;ul&gt;
&lt;li&gt;エージェントに関係のある状態だけ更新する&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ベルマン方程式は、状態数が多い時に効率が悪い&lt;ul&gt;
&lt;li&gt;サンプリング&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;</content></entry><entry><title>マルコフ決定過程 (MDP) - Google DeepMind の David Silver 氏による強化学習コース 講義2</title><link href="http://englishforhackers.com/david-silver-reinforcement-learning-lecture2.html" rel="alternate"></link><published>2018-09-02T00:00:00-04:00</published><updated>2018-09-02T00:00:00-04:00</updated><author><name>Masato Hagiwara</name></author><id>tag:englishforhackers.com,2018-09-02:/david-silver-reinforcement-learning-lecture2.html</id><summary type="html">&lt;script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async&gt;&lt;/script&gt;

&lt;p&gt;「無料でアクセスできる最高の強化学習のコース」と名高い、Google DeepMind / University College London の David Silver 氏による強化学習のコース。&lt;a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html"&gt;こちらのページから、全ての講義スライドと講義ビデオが見られる&lt;/a&gt;。以下は、講義2 のメモです。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;マルコフ決定過程 (MDP)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;環境が完全に観察可能&lt;/li&gt;
&lt;li&gt;状態が、過程を完全に規定する&lt;/li&gt;
&lt;li&gt;多くの強化学習問題が、MDP として定式化可能&lt;/li&gt;
&lt;li&gt;部分観測マルコフ決定過程 (POMDP) も、MDP に変換可能&lt;/li&gt;
&lt;li&gt;バンディットアルゴリズムも、状態が一つしかない MDP&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;マルコフ性&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;次に何が起こるかは、今の状態だけに依存&lt;/li&gt;
&lt;li&gt;Lecture 1 参照&lt;/li&gt;
&lt;li&gt;状態遷移確率 \( P_{ss'} = P[S_{t+1} = s' | S_t = s] \) 行列で表現可能&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;マルコフ過程 …&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async&gt;&lt;/script&gt;

&lt;p&gt;「無料でアクセスできる最高の強化学習のコース」と名高い、Google DeepMind / University College London の David Silver 氏による強化学習のコース。&lt;a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html"&gt;こちらのページから、全ての講義スライドと講義ビデオが見られる&lt;/a&gt;。以下は、講義2 のメモです。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;マルコフ決定過程 (MDP)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;環境が完全に観察可能&lt;/li&gt;
&lt;li&gt;状態が、過程を完全に規定する&lt;/li&gt;
&lt;li&gt;多くの強化学習問題が、MDP として定式化可能&lt;/li&gt;
&lt;li&gt;部分観測マルコフ決定過程 (POMDP) も、MDP に変換可能&lt;/li&gt;
&lt;li&gt;バンディットアルゴリズムも、状態が一つしかない MDP&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;マルコフ性&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;次に何が起こるかは、今の状態だけに依存&lt;/li&gt;
&lt;li&gt;Lecture 1 参照&lt;/li&gt;
&lt;li&gt;状態遷移確率 \( P_{ss'} = P[S_{t+1} = s' | S_t = s] \) 行列で表現可能&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;マルコフ過程&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;状態列 \( S_1, S_2, ... \) がマルコフ性を満たすとき → マルコフ過程&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;マルコフ報酬過程 (Markov Reward Process; MRP)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;R: 報酬関数 \( R_s = E[ R_{t+1} | S_t = s] \)&lt;/li&gt;
&lt;li&gt;利得 \( G_t = \sum_{k=0}^\infty \gamma^k R_{t+k+1} \)&lt;/li&gt;
&lt;li&gt;なぜ割引率 \( \gamma \) を使うか → 数学的に便利。報酬が発散するのを防ぐ。未来に行くほど不確定。直近の未来の報酬を優先。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;価値関数&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;状態 s に居るときの利得の期待値 \( v(s) = E[G_t | S_t = s] \)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ベルマン方程式&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;\( v(s) = E[R_{t+1} + \gamma v(S_{t+1}) | S_t = s] \)&lt;/li&gt;
&lt;li&gt;価値観数は、1) すぐ次の報酬、2) 次の状態の価値（＋割り引き）の２つに分解できる&lt;/li&gt;
&lt;li&gt;行列表現: \( v = R + \gamma Pv \)&lt;ul&gt;
&lt;li&gt;v: \( v = (v(1), ..., v(n))^T \)&lt;/li&gt;
&lt;li&gt;R: \( R = (R(1), ..., R(n))^T \)&lt;/li&gt;
&lt;li&gt;P: 状態 i から j への遷移確率行列&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;解析的に解ける: \( v = (I - \gamma P)^{-1}R \)&lt;ul&gt;
&lt;li&gt;小さい MDP にしか適用できない。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;マルコフ決定過程&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;マルコフ報酬過程 + 行動&lt;/li&gt;
&lt;li&gt;報酬 R: \( R^a_s = E[R_{t+1} | S_t = s, A_t = a] \)&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;方策&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;\( \pi(a | s) = P[A_t = a | S_t = s] \) → エージェントの振る舞いを完全に規定&lt;ul&gt;
&lt;li&gt;時間 \( t \) に依存しない&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;MDP と方策 \( \pi \) が与えられたとき、状態系列 \( S_1, S_2, ... \) はマルコフ過程 → マルコフ決定過程を「平ら」にする&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;価値関数&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;状態価値関数は、方策 \( \pi \) に依存： \( v_\pi(s) = E_\pi[G_t | S_t = s] \)&lt;/li&gt;
&lt;li&gt;行動価値関数: \( q(s, a) = E[G_t | S_t = s, A_t = a] \)&lt;/li&gt;
&lt;li&gt;ベルマン方程式を使って、直近の報酬と次の状態の価値に分解できる&lt;ul&gt;
&lt;li&gt;状態価値関数: \( v_\pi(s) = \sum_{a \in A} \pi(a|s) q_\pi(a, s) \)&lt;/li&gt;
&lt;li&gt;行動価値関数: \( q(s, a) = R^a_s + \gamma \sum_{s' \in S}P^a_{ss'} v_\pi(s') \)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ベルマン方程式の再帰適用: \( v_\pi(s) = \sum_{a \in A} \pi(a|s)( R^a_s + \gamma \sum_{s' \in S} P^a_{ss'} v_\pi(s') ) \)&lt;/li&gt;
&lt;li&gt;\( q_\pi \) にも同じことができる&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;最適状態価値関数&lt;ul&gt;
&lt;li&gt;全ての方策の中で、価値関数が最大となるもの: \( v_*(s) = \max_\pi v_\pi(s) \)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;最適行動価値関数&lt;ul&gt;
&lt;li&gt;\( q_{*}(s, a) = \max_\pi q_\pi(s, a) \) → これがあれば、MDP は「解けた」（各状態において、どう行動すべきかが分かる）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;最適方策&lt;ul&gt;
&lt;li&gt;ある方策が他の方策より良いとは？ \( \pi \ge \pi' if v_\pi(s) \ge v_\pi'(s), \forall s \)&lt;/li&gt;
&lt;li&gt;定理: 他のあらゆる方策よりも良い最適方策 \( \pi_* \) が存在する。複数存在する場合もある&lt;/li&gt;
&lt;li&gt;\( q_*(s, a) \) を最大化する行動を取ることで、最適方策が得られる&lt;/li&gt;
&lt;li&gt;\( v_* \) と \( q_* \) についても、上記のベルマン方程式が適用できる&lt;ul&gt;
&lt;li&gt;ただし、\( \sum_{a \in A} \) は \( \max_a \) に置き換わる&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;非線形 (max が入る) →　閉じた形での解は存在しない&lt;/li&gt;
&lt;li&gt;繰り返し&lt;ul&gt;
&lt;li&gt;価値反復 (Value Iteration)&lt;/li&gt;
&lt;li&gt;方策反復 (Policy Iteration)&lt;/li&gt;
&lt;li&gt;Q 学習&lt;/li&gt;
&lt;li&gt;Sarsa &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;MDP の拡張&lt;/li&gt;
&lt;li&gt;無限 / 連続 MDP&lt;/li&gt;
&lt;li&gt;部分観測マルコフ決定過程 (POMDP)&lt;/li&gt;
&lt;li&gt;割り引きの無い, 平均報酬 MDP&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;</content></entry><entry><title>強化学習入門 - Google DeepMind の David Silver 氏による強化学習コース 講義1</title><link href="http://englishforhackers.com/david-silver-reinforcement-learning-lecture1.html" rel="alternate"></link><published>2018-09-01T00:00:00-04:00</published><updated>2018-09-01T00:00:00-04:00</updated><author><name>Masato Hagiwara</name></author><id>tag:englishforhackers.com,2018-09-01:/david-silver-reinforcement-learning-lecture1.html</id><summary type="html">&lt;script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async&gt;&lt;/script&gt;

&lt;p&gt;「無料でアクセスできる最高の強化学習のコース」と名高い、Google DeepMind / University College London の David Silver 氏による強化学習のコース。&lt;a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html"&gt;こちらのページから、全ての講義スライドと講義ビデオが見られる&lt;/a&gt;。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;教科書&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;An Introduction to Reinforcement Learning&lt;ul&gt;
&lt;li&gt;直感的, このコースで参照&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Algorithms for Reinforcement Learning&lt;ul&gt;
&lt;li&gt;理論, 厳密&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;強化学習とは&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;様々な分野と関係&lt;/li&gt;
&lt;li&gt;工学、機械学習、神経科学（脳の報酬システムと関係）&lt;/li&gt;
&lt;li&gt;機械学習の３つの分類&lt;ul&gt;
&lt;li&gt;教師あり学習、教師なし学習、強化学習&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;他の機械学習アルゴリズムとの違い&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;教師の代わりに、報酬信号しかない&lt;/li&gt;
&lt;li&gt;報酬がすぐに得られるとは限らない&lt;/li&gt;
&lt;li&gt;時間の概念が重要。iid (独立同分布)データではない&lt;/li&gt;
&lt;li&gt;エージェントが環境に影響を及ぼす→データも変わる&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;強化学習の例&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ヘリコプターの曲芸を学習&lt;/li&gt;
&lt;li&gt;バックギャモンで世界チャンピオンに勝つ …&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async&gt;&lt;/script&gt;

&lt;p&gt;「無料でアクセスできる最高の強化学習のコース」と名高い、Google DeepMind / University College London の David Silver 氏による強化学習のコース。&lt;a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html"&gt;こちらのページから、全ての講義スライドと講義ビデオが見られる&lt;/a&gt;。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;教科書&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;An Introduction to Reinforcement Learning&lt;ul&gt;
&lt;li&gt;直感的, このコースで参照&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Algorithms for Reinforcement Learning&lt;ul&gt;
&lt;li&gt;理論, 厳密&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;強化学習とは&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;様々な分野と関係&lt;/li&gt;
&lt;li&gt;工学、機械学習、神経科学（脳の報酬システムと関係）&lt;/li&gt;
&lt;li&gt;機械学習の３つの分類&lt;ul&gt;
&lt;li&gt;教師あり学習、教師なし学習、強化学習&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;他の機械学習アルゴリズムとの違い&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;教師の代わりに、報酬信号しかない&lt;/li&gt;
&lt;li&gt;報酬がすぐに得られるとは限らない&lt;/li&gt;
&lt;li&gt;時間の概念が重要。iid (独立同分布)データではない&lt;/li&gt;
&lt;li&gt;エージェントが環境に影響を及ぼす→データも変わる&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;強化学習の例&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ヘリコプターの曲芸を学習&lt;/li&gt;
&lt;li&gt;バックギャモンで世界チャンピオンに勝つ&lt;/li&gt;
&lt;li&gt;投資ポートフォリオの管理&lt;/li&gt;
&lt;li&gt;発電所の制御&lt;/li&gt;
&lt;li&gt;人間型ロボットを歩かせる&lt;/li&gt;
&lt;li&gt;Atari の複数のゲームをプレイする&lt;/li&gt;
&lt;li&gt;Q：強化学習アルゴリズムは、人間の反応時間に比べて速く操作ができるので有利ではないか？ → A: 人間の反応時間に合わせてあるので、公平なはず&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;強化学習問題&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;報酬 \( R_t \) --&amp;gt; スカラー値のフィードバック信号。時刻 t においてどのぐらい「うまく行っているか」&lt;/li&gt;
&lt;li&gt;報酬の合計の期待値を最大化させるのが目的&lt;/li&gt;
&lt;li&gt;報酬に関する仮定：全てのゴールは、累積報酬の期待値を最大化させる問題に帰着できる&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;継続的な意思決定&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;目的：将来の報酬の合計を最大化させる行動を選択する&lt;/li&gt;
&lt;li&gt;貪欲的に行動するべきではない → 行動が長期にわかって効果を残す。報酬がすぐ得られるとは限らない。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;エージェントと環境&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;エージェントが環境を観察 \( O_t \)&lt;/li&gt;
&lt;li&gt;行動 \( A_t \)&lt;/li&gt;
&lt;li&gt;報酬 \( R_t \)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;履歴と状態&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;履歴 \( H_t = A_1, O_1, R_1, ..., A_t, O_t, R_t \)&lt;/li&gt;
&lt;li&gt;状態 \( S_t = f(H_t) \)&lt;/li&gt;
&lt;li&gt;環境状態 \( S^e_t \) → エージェントからは見えない&lt;/li&gt;
&lt;li&gt;エージェント状態 \( S^a_t \) &lt;/li&gt;
&lt;li&gt;マルコフ性: \( P[S_{t+1} | S_t] = P[S_{t+1} | S_1, ..., S_t] \)&lt;ul&gt;
&lt;li&gt;次の状態は、現在の状態だけに依存する&lt;/li&gt;
&lt;li&gt;現在の状態が分かれば、履歴は不要&lt;/li&gt;
&lt;li&gt;状態は、未来の十分統計量&lt;/li&gt;
&lt;li&gt;ヘリコプターの例：現在の位置、速度、角度、各速度 etc.  位置だけではマルコフ性が成立しない&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;完全に観察可能な環境&lt;ul&gt;
&lt;li&gt;\( O_t = S^a_t = S^e_t \) → マルコフ決定過程 (Markov Decision Process; MDP)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;部分的に観察可能な環境&lt;ul&gt;
&lt;li&gt;部分観測マルコフ決定過程 (Partially Observable MDP; POMDP)&lt;/li&gt;
&lt;li&gt;エージェント状態を、環境状態とは独立に構築する必要がある&lt;ul&gt;
&lt;li&gt;方法1: 状態に対する信念（確率分布）を維持する&lt;/li&gt;
&lt;li&gt;方法2: 前の状態と、現在の観察から、次の状態を予測する RNN を構築する&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;エージェント&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;方策: エージェントがどのように意思決定するか&lt;ul&gt;
&lt;li&gt;状態 s から行動 a への関数&lt;/li&gt;
&lt;li&gt;決定的な方策: \( a = \pi(s) \)&lt;/li&gt;
&lt;li&gt;確率的な方策: \( \pi(a | s) = P[A = a | S = s]\)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;価値関数: それぞれの状態/行動がどのぐらい良いか&lt;ul&gt;
&lt;li&gt;将来の報酬に対する予測&lt;/li&gt;
&lt;li&gt;方策に依存&lt;/li&gt;
&lt;li&gt;\( v_\pi(s) = E_\pi[R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + ... | S_t = s] \)&lt;/li&gt;
&lt;li&gt;時間による割引 \( \gamma \) → 遠い未来より近い未来の報酬を優先&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;モデル: エージェントによる環境の表現（「エージェントが環境がどういう仕組みで動いていると思っているか」）&lt;ul&gt;
&lt;li&gt;遷移モデル: 次の状態を予想する&lt;/li&gt;
&lt;li&gt;報酬モデル: 次の報酬を予想する&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;エージェントの分類&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;価値ベース: 価値関数を使う&lt;/li&gt;
&lt;li&gt;方策ベース: 方策を使う&lt;/li&gt;
&lt;li&gt;Actor Critic: 価値関数と方策の両方を使う&lt;/li&gt;
&lt;li&gt;モデル無し: 価値関数・方策のどちらかもしくは両方、モデル無し&lt;/li&gt;
&lt;li&gt;モデル有り: 価値関数・方策のどちらかもしくは両方、モデル有り&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;学習とプランニング&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;強化学習: 環境が未知の状態からスタート、エージェントが環境と相互作用し、方策を改善する&lt;/li&gt;
&lt;li&gt;プランニング: 環境のモデルが与えられる、相互作用せずにモデルを使って計算、方策を改善する&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;探索と搾取&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;探索: 報酬をあきらめてでも、環境に関する情報を得る&lt;/li&gt;
&lt;li&gt;搾取: 既に知っている情報を使い、報酬を最大化する&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;</content></entry></feed>