<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>エンジニア・研究者の英語学習 - Reinforcement Learning</title><link href="http://englishforhackers.com/" rel="alternate"></link><link href="http://englishforhackers.com/feeds/reinforcement-learning.atom.xml" rel="self"></link><id>http://englishforhackers.com/</id><updated>2018-09-03T00:00:00-04:00</updated><entry><title>動的計画法を使った計画 - Google DeepMind の David Silver 氏による強化学習コース 講義3</title><link href="http://englishforhackers.com/david-silver-reinforcement-learning-lecture3.html" rel="alternate"></link><published>2018-09-03T00:00:00-04:00</published><updated>2018-09-03T00:00:00-04:00</updated><author><name>Masato Hagiwara</name></author><id>tag:englishforhackers.com,2018-09-03:/david-silver-reinforcement-learning-lecture3.html</id><summary type="html">&lt;script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async&gt;&lt;/script&gt;

&lt;p&gt;「無料でアクセスできる最高の強化学習のコース」と名高い、Google DeepMind / University College London の David Silver 氏による強化学習のコース。&lt;a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html"&gt;こちらのページから、全ての講義スライドと講義ビデオが見られる&lt;/a&gt;。以下は、講義3 のメモです。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;はじめに&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;動的計画法&lt;ul&gt;
&lt;li&gt;「動的」: 逐次的、時間&lt;/li&gt;
&lt;li&gt;「計画」≒ 方策&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;動的計画法がいつ使えるか&lt;ul&gt;
&lt;li&gt;最適なサブ構造に分解し、そこから最適解が求められる場合&lt;ul&gt;
&lt;li&gt;例: グラフの最短経路問題&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;サブ問題がお互いに関係しており、何回も現れる場合 → キャッシュできる&lt;/li&gt;
&lt;li&gt;MDPはこの両方を満たす&lt;ul&gt;
&lt;li&gt;ベルマン方程式&lt;/li&gt;
&lt;li&gt;問題の再帰的な分解&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;例&lt;ul&gt;
&lt;li&gt;スケジュール&lt;/li&gt;
&lt;li&gt;文字列アルゴリズム&lt;/li&gt;
&lt;li&gt;グラフアルゴリズム&lt;/li&gt;
&lt;li&gt;グラフィカルアルゴリズム&lt;/li&gt;
&lt;li&gt;生物情報学&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;動的計画法を使った計画&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;MDP の情報が全て分かっている前提&lt;/li&gt;
&lt;li&gt;予測: MDP と方策 \( \pi \) が分かっている時に、価値関数 \( v_ …&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async&gt;&lt;/script&gt;

&lt;p&gt;「無料でアクセスできる最高の強化学習のコース」と名高い、Google DeepMind / University College London の David Silver 氏による強化学習のコース。&lt;a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html"&gt;こちらのページから、全ての講義スライドと講義ビデオが見られる&lt;/a&gt;。以下は、講義3 のメモです。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;はじめに&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;動的計画法&lt;ul&gt;
&lt;li&gt;「動的」: 逐次的、時間&lt;/li&gt;
&lt;li&gt;「計画」≒ 方策&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;動的計画法がいつ使えるか&lt;ul&gt;
&lt;li&gt;最適なサブ構造に分解し、そこから最適解が求められる場合&lt;ul&gt;
&lt;li&gt;例: グラフの最短経路問題&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;サブ問題がお互いに関係しており、何回も現れる場合 → キャッシュできる&lt;/li&gt;
&lt;li&gt;MDPはこの両方を満たす&lt;ul&gt;
&lt;li&gt;ベルマン方程式&lt;/li&gt;
&lt;li&gt;問題の再帰的な分解&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;例&lt;ul&gt;
&lt;li&gt;スケジュール&lt;/li&gt;
&lt;li&gt;文字列アルゴリズム&lt;/li&gt;
&lt;li&gt;グラフアルゴリズム&lt;/li&gt;
&lt;li&gt;グラフィカルアルゴリズム&lt;/li&gt;
&lt;li&gt;生物情報学&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;動的計画法を使った計画&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;MDP の情報が全て分かっている前提&lt;/li&gt;
&lt;li&gt;予測: MDP と方策 \( \pi \) が分かっている時に、価値関数 \( v_\pi \) を求める&lt;/li&gt;
&lt;li&gt;操作: MDP が分かっている時に、最適方策 \( \pi_* \) を求める&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;方策反復&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;MDP を解くための仕組みの一つ&lt;/li&gt;
&lt;li&gt;方策 \( \pi \) を評価する&lt;/li&gt;
&lt;li&gt;ベルマン方程式を逆向きに繰り返し適用&lt;/li&gt;
&lt;li&gt;任意の \( v_1 \) からスタート。ベルマン方程式を適用し、\( v_2 \) を得る。&lt;/li&gt;
&lt;li&gt;\( v_{k+1}(s) \) を計算するためには&lt;ul&gt;
&lt;li&gt;1ステップ先読みする。\( v_{k+1}(s) = \sum_{a \in A} \pi(a|s)( R^a_s + \gamma \sum_{s' \in S} P^a_{ss'} v_k(s') ) \)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;これを繰り返すと、\( v_* \) に収束する&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;方策をどう改善するか&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;方策 \( \pi \) が与えられた時、&lt;ul&gt;
&lt;li&gt;まず、方策 \( \pi \) を評価し、\( v_\pi(s) \) を得る&lt;/li&gt;
&lt;li&gt;\( v_\pi(s) \) に従い、貪欲に行動し、方策 \( \pi' \) を得る&lt;/li&gt;
&lt;li&gt;「格子世界」の例では、\( \pi' \) が最適方策 \( \pi_* \)&lt;/li&gt;
&lt;li&gt;一般的には、これを繰り返すと、最適方策に収束する&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;決定的な方策 \( \pi \) からスタート&lt;ul&gt;
&lt;li&gt;貪欲に行動することで、この方策を改善できる。 \( \pi'(s) = \arg\max_{a \in A} q_\pi(s, a) \)&lt;/li&gt;
&lt;li&gt;→ 価値関数も改善する&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;この繰り返し的な改善がストップする時 → ベルマン最適方程式を満たす → 方策は最適である \( v_\pi(s) = v_*(s) \)&lt;/li&gt;
&lt;li&gt;方策評価が収束するまで繰り返す必要があるか？ \( k \) 回繰り返せば十分。ただし \( k = 1 \) ではだめ。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;価値反復&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;MDP を解くためのもう一つの仕組み&lt;/li&gt;
&lt;li&gt;最適原則&lt;ul&gt;
&lt;li&gt;方策 \( \pi(a|s) \) は、以下の条件を満たす時、またその時に限って、最適価値関数 \( v_\pi(s) = v_*(s) \) を満たす。&lt;ul&gt;
&lt;li&gt;任意の状態 \( s' \) が \( s \) から到達可能&lt;/li&gt;
&lt;li&gt;状態 \( s' \) が、最適価値関数を満たす \( v_\pi(s') = v_*(s') \)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;もし、部分問題に対して最適解が分かっている時、\( v_*(s') \)&lt;ul&gt;
&lt;li&gt;１ステップ先読みする： \( v_*(s) \leftarrow \max_{a \in A} R^a_s + \gamma \sum_{s' \in S} P^a_{ss'}v_*(s') \)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;最適方策 \( \pi \) を探す&lt;ul&gt;
&lt;li&gt;\( v_1 \to v_2 \to ... \to v_* \)&lt;/li&gt;
&lt;li&gt;\( v_{k+1}(s) \) から \( v_k(s') \) を更新&lt;/li&gt;
&lt;li&gt;方策反復とは違い、方策を明示的に使わない&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;まとめ&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;予測 → ベルマン期待値方程式 - 反復方策評価&lt;/li&gt;
&lt;li&gt;操作 → ベルマン期待値方程式+貪欲的方策更新 - 方策反復&lt;/li&gt;
&lt;li&gt;操作 → ベルマン最適方程式 - 価値反復&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;拡張&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;非同期動的計画法&lt;ul&gt;
&lt;li&gt;他の状態の更新が終わるまで待たない。最適値に収束する&lt;/li&gt;
&lt;li&gt;In-place 動的計画法　&lt;ul&gt;
&lt;li&gt;価値関数の表を直接書き換える&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;優先度付き sweeping&lt;ul&gt;
&lt;li&gt;どの状態を次に更新するか優先度をつける&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;リアルタイム 動的計画法&lt;ul&gt;
&lt;li&gt;エージェントに関係のある状態だけ更新する&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ベルマン方程式は、状態数が多い時に効率が悪い&lt;ul&gt;
&lt;li&gt;サンプリング&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;</content></entry><entry><title>マルコフ決定過程 (MDP) - Google DeepMind の David Silver 氏による強化学習コース 講義2</title><link href="http://englishforhackers.com/david-silver-reinforcement-learning-lecture2.html" rel="alternate"></link><published>2018-09-02T00:00:00-04:00</published><updated>2018-09-02T00:00:00-04:00</updated><author><name>Masato Hagiwara</name></author><id>tag:englishforhackers.com,2018-09-02:/david-silver-reinforcement-learning-lecture2.html</id><summary type="html">&lt;script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async&gt;&lt;/script&gt;

&lt;p&gt;「無料でアクセスできる最高の強化学習のコース」と名高い、Google DeepMind / University College London の David Silver 氏による強化学習のコース。&lt;a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html"&gt;こちらのページから、全ての講義スライドと講義ビデオが見られる&lt;/a&gt;。以下は、講義2 のメモです。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;マルコフ決定過程 (MDP)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;環境が完全に観察可能&lt;/li&gt;
&lt;li&gt;状態が、過程を完全に規定する&lt;/li&gt;
&lt;li&gt;多くの強化学習問題が、MDP として定式化可能&lt;/li&gt;
&lt;li&gt;部分観測マルコフ決定過程 (POMDP) も、MDP に変換可能&lt;/li&gt;
&lt;li&gt;バンディットアルゴリズムも、状態が一つしかない MDP&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;マルコフ性&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;次に何が起こるかは、今の状態だけに依存&lt;/li&gt;
&lt;li&gt;Lecture 1 参照&lt;/li&gt;
&lt;li&gt;状態遷移確率 \( P_{ss'} = P[S_{t+1} = s' | S_t = s] \) 行列で表現可能&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;マルコフ過程 …&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async&gt;&lt;/script&gt;

&lt;p&gt;「無料でアクセスできる最高の強化学習のコース」と名高い、Google DeepMind / University College London の David Silver 氏による強化学習のコース。&lt;a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html"&gt;こちらのページから、全ての講義スライドと講義ビデオが見られる&lt;/a&gt;。以下は、講義2 のメモです。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;マルコフ決定過程 (MDP)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;環境が完全に観察可能&lt;/li&gt;
&lt;li&gt;状態が、過程を完全に規定する&lt;/li&gt;
&lt;li&gt;多くの強化学習問題が、MDP として定式化可能&lt;/li&gt;
&lt;li&gt;部分観測マルコフ決定過程 (POMDP) も、MDP に変換可能&lt;/li&gt;
&lt;li&gt;バンディットアルゴリズムも、状態が一つしかない MDP&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;マルコフ性&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;次に何が起こるかは、今の状態だけに依存&lt;/li&gt;
&lt;li&gt;Lecture 1 参照&lt;/li&gt;
&lt;li&gt;状態遷移確率 \( P_{ss'} = P[S_{t+1} = s' | S_t = s] \) 行列で表現可能&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;マルコフ過程&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;状態列 \( S_1, S_2, ... \) がマルコフ性を満たすとき → マルコフ過程&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;マルコフ報酬過程 (Markov Reward Process; MRP)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;R: 報酬関数 \( R_s = E[ R_{t+1} | S_t = s] \)&lt;/li&gt;
&lt;li&gt;利得 \( G_t = \sum_{k=0}^\infty \gamma^k R_{t+k+1} \)&lt;/li&gt;
&lt;li&gt;なぜ割引率 \( \gamma \) を使うか → 数学的に便利。報酬が発散するのを防ぐ。未来に行くほど不確定。直近の未来の報酬を優先。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;価値関数&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;状態 s に居るときの利得の期待値 \( v(s) = E[G_t | S_t = s] \)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ベルマン方程式&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;\( v(s) = E[R_{t+1} + \gamma v(S_{t+1}) | S_t = s] \)&lt;/li&gt;
&lt;li&gt;価値観数は、1) すぐ次の報酬、2) 次の状態の価値（＋割り引き）の２つに分解できる&lt;/li&gt;
&lt;li&gt;行列表現: \( v = R + \gamma Pv \)&lt;ul&gt;
&lt;li&gt;v: \( v = (v(1), ..., v(n))^T \)&lt;/li&gt;
&lt;li&gt;R: \( R = (R(1), ..., R(n))^T \)&lt;/li&gt;
&lt;li&gt;P: 状態 i から j への遷移確率行列&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;解析的に解ける: \( v = (I - \gamma P)^{-1}R \)&lt;ul&gt;
&lt;li&gt;小さい MDP にしか適用できない。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;マルコフ決定過程&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;マルコフ報酬過程 + 行動&lt;/li&gt;
&lt;li&gt;報酬 R: \( R^a_s = E[R_{t+1} | S_t = s, A_t = a] \)&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;方策&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;\( \pi(a | s) = P[A_t = a | S_t = s] \) → エージェントの振る舞いを完全に規定&lt;ul&gt;
&lt;li&gt;時間 \( t \) に依存しない&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;MDP と方策 \( \pi \) が与えられたとき、状態系列 \( S_1, S_2, ... \) はマルコフ過程 → マルコフ決定過程を「平ら」にする&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;価値関数&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;状態価値関数は、方策 \( \pi \) に依存： \( v_\pi(s) = E_\pi[G_t | S_t = s] \)&lt;/li&gt;
&lt;li&gt;行動価値関数: \( q(s, a) = E[G_t | S_t = s, A_t = a] \)&lt;/li&gt;
&lt;li&gt;ベルマン方程式を使って、直近の報酬と次の状態の価値に分解できる&lt;ul&gt;
&lt;li&gt;状態価値関数: \( v_\pi(s) = \sum_{a \in A} \pi(a|s) q_\pi(a, s) \)&lt;/li&gt;
&lt;li&gt;行動価値関数: \( q(s, a) = R^a_s + \gamma \sum_{s' \in S}P^a_{ss'} v_\pi(s') \)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ベルマン方程式の再帰適用: \( v_\pi(s) = \sum_{a \in A} \pi(a|s)( R^a_s + \gamma \sum_{s' \in S} P^a_{ss'} v_\pi(s') ) \)&lt;/li&gt;
&lt;li&gt;\( q_\pi \) にも同じことができる&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;最適状態価値関数&lt;ul&gt;
&lt;li&gt;全ての方策の中で、価値関数が最大となるもの: \( v_*(s) = \max_\pi v_\pi(s) \)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;最適行動価値関数&lt;ul&gt;
&lt;li&gt;\( q_{*}(s, a) = \max_\pi q_\pi(s, a) \) → これがあれば、MDP は「解けた」（各状態において、どう行動すべきかが分かる）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;最適方策&lt;ul&gt;
&lt;li&gt;ある方策が他の方策より良いとは？ \( \pi \ge \pi' if v_\pi(s) \ge v_\pi'(s), \forall s \)&lt;/li&gt;
&lt;li&gt;定理: 他のあらゆる方策よりも良い最適方策 \( \pi_* \) が存在する。複数存在する場合もある&lt;/li&gt;
&lt;li&gt;\( q_*(s, a) \) を最大化する行動を取ることで、最適方策が得られる&lt;/li&gt;
&lt;li&gt;\( v_* \) と \( q_* \) についても、上記のベルマン方程式が適用できる&lt;ul&gt;
&lt;li&gt;ただし、\( \sum_{a \in A} \) は \( \max_a \) に置き換わる&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;非線形 (max が入る) →　閉じた形での解は存在しない&lt;/li&gt;
&lt;li&gt;繰り返し&lt;ul&gt;
&lt;li&gt;価値反復 (Value Iteration)&lt;/li&gt;
&lt;li&gt;方策反復 (Policy Iteration)&lt;/li&gt;
&lt;li&gt;Q 学習&lt;/li&gt;
&lt;li&gt;Sarsa &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;MDP の拡張&lt;/li&gt;
&lt;li&gt;無限 / 連続 MDP&lt;/li&gt;
&lt;li&gt;部分観測マルコフ決定過程 (POMDP)&lt;/li&gt;
&lt;li&gt;割り引きの無い, 平均報酬 MDP&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;</content></entry><entry><title>強化学習入門 - Google DeepMind の David Silver 氏による強化学習コース 講義1</title><link href="http://englishforhackers.com/david-silver-reinforcement-learning-lecture1.html" rel="alternate"></link><published>2018-09-01T00:00:00-04:00</published><updated>2018-09-01T00:00:00-04:00</updated><author><name>Masato Hagiwara</name></author><id>tag:englishforhackers.com,2018-09-01:/david-silver-reinforcement-learning-lecture1.html</id><summary type="html">&lt;script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async&gt;&lt;/script&gt;

&lt;p&gt;「無料でアクセスできる最高の強化学習のコース」と名高い、Google DeepMind / University College London の David Silver 氏による強化学習のコース。&lt;a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html"&gt;こちらのページから、全ての講義スライドと講義ビデオが見られる&lt;/a&gt;。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;教科書&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;An Introduction to Reinforcement Learning&lt;ul&gt;
&lt;li&gt;直感的, このコースで参照&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Algorithms for Reinforcement Learning&lt;ul&gt;
&lt;li&gt;理論, 厳密&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;強化学習とは&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;様々な分野と関係&lt;/li&gt;
&lt;li&gt;工学、機械学習、神経科学（脳の報酬システムと関係）&lt;/li&gt;
&lt;li&gt;機械学習の３つの分類&lt;ul&gt;
&lt;li&gt;教師あり学習、教師なし学習、強化学習&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;他の機械学習アルゴリズムとの違い&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;教師の代わりに、報酬信号しかない&lt;/li&gt;
&lt;li&gt;報酬がすぐに得られるとは限らない&lt;/li&gt;
&lt;li&gt;時間の概念が重要。iid (独立同分布)データではない&lt;/li&gt;
&lt;li&gt;エージェントが環境に影響を及ぼす→データも変わる&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;強化学習の例&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ヘリコプターの曲芸を学習&lt;/li&gt;
&lt;li&gt;バックギャモンで世界チャンピオンに勝つ …&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async&gt;&lt;/script&gt;

&lt;p&gt;「無料でアクセスできる最高の強化学習のコース」と名高い、Google DeepMind / University College London の David Silver 氏による強化学習のコース。&lt;a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html"&gt;こちらのページから、全ての講義スライドと講義ビデオが見られる&lt;/a&gt;。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;教科書&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;An Introduction to Reinforcement Learning&lt;ul&gt;
&lt;li&gt;直感的, このコースで参照&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Algorithms for Reinforcement Learning&lt;ul&gt;
&lt;li&gt;理論, 厳密&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;強化学習とは&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;様々な分野と関係&lt;/li&gt;
&lt;li&gt;工学、機械学習、神経科学（脳の報酬システムと関係）&lt;/li&gt;
&lt;li&gt;機械学習の３つの分類&lt;ul&gt;
&lt;li&gt;教師あり学習、教師なし学習、強化学習&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;他の機械学習アルゴリズムとの違い&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;教師の代わりに、報酬信号しかない&lt;/li&gt;
&lt;li&gt;報酬がすぐに得られるとは限らない&lt;/li&gt;
&lt;li&gt;時間の概念が重要。iid (独立同分布)データではない&lt;/li&gt;
&lt;li&gt;エージェントが環境に影響を及ぼす→データも変わる&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;強化学習の例&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ヘリコプターの曲芸を学習&lt;/li&gt;
&lt;li&gt;バックギャモンで世界チャンピオンに勝つ&lt;/li&gt;
&lt;li&gt;投資ポートフォリオの管理&lt;/li&gt;
&lt;li&gt;発電所の制御&lt;/li&gt;
&lt;li&gt;人間型ロボットを歩かせる&lt;/li&gt;
&lt;li&gt;Atari の複数のゲームをプレイする&lt;/li&gt;
&lt;li&gt;Q：強化学習アルゴリズムは、人間の反応時間に比べて速く操作ができるので有利ではないか？ → A: 人間の反応時間に合わせてあるので、公平なはず&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;強化学習問題&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;報酬 \( R_t \) --&amp;gt; スカラー値のフィードバック信号。時刻 t においてどのぐらい「うまく行っているか」&lt;/li&gt;
&lt;li&gt;報酬の合計の期待値を最大化させるのが目的&lt;/li&gt;
&lt;li&gt;報酬に関する仮定：全てのゴールは、累積報酬の期待値を最大化させる問題に帰着できる&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;継続的な意思決定&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;目的：将来の報酬の合計を最大化させる行動を選択する&lt;/li&gt;
&lt;li&gt;貪欲的に行動するべきではない → 行動が長期にわかって効果を残す。報酬がすぐ得られるとは限らない。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;エージェントと環境&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;エージェントが環境を観察 \( O_t \)&lt;/li&gt;
&lt;li&gt;行動 \( A_t \)&lt;/li&gt;
&lt;li&gt;報酬 \( R_t \)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;履歴と状態&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;履歴 \( H_t = A_1, O_1, R_1, ..., A_t, O_t, R_t \)&lt;/li&gt;
&lt;li&gt;状態 \( S_t = f(H_t) \)&lt;/li&gt;
&lt;li&gt;環境状態 \( S^e_t \) → エージェントからは見えない&lt;/li&gt;
&lt;li&gt;エージェント状態 \( S^a_t \) &lt;/li&gt;
&lt;li&gt;マルコフ性: \( P[S_{t+1} | S_t] = P[S_{t+1} | S_1, ..., S_t] \)&lt;ul&gt;
&lt;li&gt;次の状態は、現在の状態だけに依存する&lt;/li&gt;
&lt;li&gt;現在の状態が分かれば、履歴は不要&lt;/li&gt;
&lt;li&gt;状態は、未来の十分統計量&lt;/li&gt;
&lt;li&gt;ヘリコプターの例：現在の位置、速度、角度、各速度 etc.  位置だけではマルコフ性が成立しない&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;完全に観察可能な環境&lt;ul&gt;
&lt;li&gt;\( O_t = S^a_t = S^e_t \) → マルコフ決定過程 (Markov Decision Process; MDP)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;部分的に観察可能な環境&lt;ul&gt;
&lt;li&gt;部分観測マルコフ決定過程 (Partially Observable MDP; POMDP)&lt;/li&gt;
&lt;li&gt;エージェント状態を、環境状態とは独立に構築する必要がある&lt;ul&gt;
&lt;li&gt;方法1: 状態に対する信念（確率分布）を維持する&lt;/li&gt;
&lt;li&gt;方法2: 前の状態と、現在の観察から、次の状態を予測する RNN を構築する&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;エージェント&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;方策: エージェントがどのように意思決定するか&lt;ul&gt;
&lt;li&gt;状態 s から行動 a への関数&lt;/li&gt;
&lt;li&gt;決定的な方策: \( a = \pi(s) \)&lt;/li&gt;
&lt;li&gt;確率的な方策: \( \pi(a | s) = P[A = a | S = s]\)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;価値関数: それぞれの状態/行動がどのぐらい良いか&lt;ul&gt;
&lt;li&gt;将来の報酬に対する予測&lt;/li&gt;
&lt;li&gt;方策に依存&lt;/li&gt;
&lt;li&gt;\( v_\pi(s) = E_\pi[R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + ... | S_t = s] \)&lt;/li&gt;
&lt;li&gt;時間による割引 \( \gamma \) → 遠い未来より近い未来の報酬を優先&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;モデル: エージェントによる環境の表現（「エージェントが環境がどういう仕組みで動いていると思っているか」）&lt;ul&gt;
&lt;li&gt;遷移モデル: 次の状態を予想する&lt;/li&gt;
&lt;li&gt;報酬モデル: 次の報酬を予想する&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;エージェントの分類&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;価値ベース: 価値関数を使う&lt;/li&gt;
&lt;li&gt;方策ベース: 方策を使う&lt;/li&gt;
&lt;li&gt;Actor Critic: 価値関数と方策の両方を使う&lt;/li&gt;
&lt;li&gt;モデル無し: 価値関数・方策のどちらかもしくは両方、モデル無し&lt;/li&gt;
&lt;li&gt;モデル有り: 価値関数・方策のどちらかもしくは両方、モデル有り&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;学習とプランニング&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;強化学習: 環境が未知の状態からスタート、エージェントが環境と相互作用し、方策を改善する&lt;/li&gt;
&lt;li&gt;プランニング: 環境のモデルが与えられる、相互作用せずにモデルを使って計算、方策を改善する&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;探索と搾取&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;探索: 報酬をあきらめてでも、環境に関する情報を得る&lt;/li&gt;
&lt;li&gt;搾取: 既に知っている情報を使い、報酬を最大化する&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;</content></entry></feed>