
<!DOCTYPE html>
<html lang="ja">
<head>
  <link href='//fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,700,400italic' rel='stylesheet' type='text/css'>

    <link rel="stylesheet/less" type="text/css" href="http://englishforhackers.com/theme/stylesheet/style.less">
    <script src="//cdnjs.cloudflare.com/ajax/libs/less.js/2.5.1/less.min.js" type="text/javascript"></script>

  <link rel="stylesheet" type="text/css" href="http://englishforhackers.com/theme/pygments/monokai.min.css">
  <link rel="stylesheet" type="text/css" href="http://englishforhackers.com/theme/font-awesome/css/font-awesome.min.css">

    <link href="http://englishforhackers.com/static/custom.css" rel="stylesheet">

    <link href="http://englishforhackers.com/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="エンジニア・研究者の英語学習 Atom">


    <link rel="shortcut icon" href="/images/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/images/favicon.ico" type="image/x-icon">

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="robots" content="index, follow" />

    <!-- Chrome, Firefox OS and Opera -->
    <meta name="theme-color" content="#333333">
    <!-- Windows Phone -->
    <meta name="msapplication-navbutton-color" content="#333333">
    <!-- iOS Safari -->
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

<meta name="author" content="Masato Hagiwara" />
<meta name="description" content="「無料でアクセスできる最高の強化学習のコース」と名高い、Google DeepMind / University College London の David Silver 氏による強化学習のコース。こちらのページから、全ての講義スライドと講義ビデオが見られる。以下は、講義5 のメモです。 はじめに 非常に多くの問題が、MDP としてモデル化できる On-policy (方策オン型) 「行動しながら学ぶ」 学習している方策と、サンプルを生成する方策が同じ Off-policy (方策オフ型) 「他の人の行動から学ぶ」 学習している方策と、サンプルを生成する方策が違う 方策オン型 MC 制御 復習: 方策反復：1) 方策評価 \( v_\pi \) の推定 と、2) 方策改善 (貪欲的方策改善) を繰り返す ここに、MC 法による方策評価を組み込むことはできるか？ 問題点：\( V …" />
<meta name="keywords" content="">
<meta property="og:site_name" content="エンジニア・研究者の英語学習"/>
<meta property="og:title" content="モデルフリー制御 - Google DeepMind の David Silver 氏による強化学習コース 講義5"/>
<meta property="og:description" content="「無料でアクセスできる最高の強化学習のコース」と名高い、Google DeepMind / University College London の David Silver 氏による強化学習のコース。こちらのページから、全ての講義スライドと講義ビデオが見られる。以下は、講義5 のメモです。 はじめに 非常に多くの問題が、MDP としてモデル化できる On-policy (方策オン型) 「行動しながら学ぶ」 学習している方策と、サンプルを生成する方策が同じ Off-policy (方策オフ型) 「他の人の行動から学ぶ」 学習している方策と、サンプルを生成する方策が違う 方策オン型 MC 制御 復習: 方策反復：1) 方策評価 \( v_\pi \) の推定 と、2) 方策改善 (貪欲的方策改善) を繰り返す ここに、MC 法による方策評価を組み込むことはできるか？ 問題点：\( V …"/>
<meta property="og:locale" content="ja_JP"/>
<meta property="og:url" content="http://englishforhackers.com/david-silver-reinforcement-learning-lecture5.html"/>
<meta property="og:type" content="article"/>
<meta property="article:published_time" content="2018-09-05 00:00:00-04:00"/>
<meta property="article:modified_time" content=""/>
<meta property="article:author" content="http://englishforhackers.com/author/masato-hagiwara.html">
<meta property="article:section" content="Reinforcement Learning"/>
<meta property="og:image" content="http://masatohagiwara.net/face.jpg">

  <title>モデルフリー制御 - Google DeepMind の David Silver 氏による強化学習コース 講義5 &ndash; エンジニア・研究者の英語学習</title>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
    (adsbygoogle = window.adsbygoogle || []).push({
      google_ad_client: "ca-pub-7401771876348738",
      enable_page_level_ads: true
    });
  </script>
</head>
<body>
  <aside>
    <div>
      <a href="http://englishforhackers.com">
        <img src="http://masatohagiwara.net/face.jpg" alt="エンジニア・研究者の<br/>英語学習" title="エンジニア・研究者の<br/>英語学習">
      </a>
      <h1><a href="http://englishforhackers.com">エンジニア・研究者の<br/>英語学習</a></h1>

<p>ソフトウェア・エンジニアや研究者のための英語学習情報。講演/トーク・技術記事・論文などの紹介</p>
      <nav>
        <ul class="list">

          <li><a href="http://masatohagiwara.net/" target="_blank">個人ページ</a></li>
          <li><a href="http://www.duolingo.com/" target="_blank">Duolingo</a></li>
          <li><a href="https://docs.google.com/forms/d/e/1FAIpQLSffbTFvdUXJhN4jTOmylcIRvlntKmaYVkIbYPrbBCPm0iC9Sw/viewform?usp=sf_link" target="_blank">お問い合わせ</a></li>
        </ul>
      </nav>

      <ul class="social">
        <li><a class="sc-github" href="https://github.com/mhagiwara" target="_blank"><i class="fa fa-github"></i></a></li>
        <li><a class="sc-twitter" href="https://twitter.com/mhagiwara" target="_blank"><i class="fa fa-twitter"></i></a></li>
        <li><a class="sc-rss" href="feeds/all.atom.xml" target="_blank"><i class="fa fa-rss"></i></a></li>
      </ul>
    </div>


  </aside>
  <main>

    <nav>
      <a href="http://englishforhackers.com">  Home
</a>

      <a href="/archives.html">Archives</a>
      <a href="/categories.html">Categories</a>
      <a href="/tags.html">Tags</a>

      <a href="http://englishforhackers.com/feeds/all.atom.xml">  Atom
</a>

    </nav>

<article class="single">
  <header>
    <h1 id="david-silver-reinforcement-learning-lecture5">モデルフリー制御 - Google DeepMind の David Silver 氏による強化学習コース 講義5</h1>
    <p>
        Posted on 2018-09-05(水) in <a href="http://englishforhackers.com/category/reinforcement-learning.html">Reinforcement Learning</a>


    </p>
  </header>


  <div>
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script>

<p>「無料でアクセスできる最高の強化学習のコース」と名高い、Google DeepMind / University College London の David Silver 氏による強化学習のコース。<a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html">こちらのページから、全ての講義スライドと講義ビデオが見られる</a>。以下は、講義5 のメモです。</p>
<ul>
<li>
<p>はじめに</p>
<ul>
<li>非常に多くの問題が、MDP としてモデル化できる</li>
<li>On-policy (方策オン型)<ul>
<li>「行動しながら学ぶ」</li>
<li>学習している方策と、サンプルを生成する方策が同じ</li>
</ul>
</li>
<li>Off-policy (方策オフ型)<ul>
<li>「他の人の行動から学ぶ」</li>
<li>学習している方策と、サンプルを生成する方策が違う</li>
</ul>
</li>
</ul>
</li>
<li>
<p>方策オン型　MC 制御</p>
<ul>
<li>復習: 方策反復：1) 方策評価 \( v_\pi \) の推定 と、2) 方策改善 (貪欲的方策改善) を繰り返す</li>
<li>ここに、MC 法による方策評価を組み込むことはできるか？<ul>
<li>問題点：\( V(s) \) に従って貪欲に行動しようとしても、MDP の完全な情報が必要 → 解法: 代わりに \( Q(s, a) \) を使う</li>
</ul>
</li>
<li>\( Q = q_\pi \) を MC で評価する<ul>
<li>問題点：探索問題。貪欲的に行動すると、必要な状態に到達することができない</li>
</ul>
</li>
<li>探索<ul>
<li>ε-貪欲探索<ul>
<li>確率εでランダムな行動を（一様分布に従って）取る</li>
<li>ε-貪欲探索に従う新しい方策 \( \pi' \) は、前の方策 \( \pi \) よりも良い</li>
</ul>
</li>
<li>MC で方策を評価し、ε-貪欲探索をすると？<ul>
<li>最適方策 \( \pi_* \) に到達する。どのぐらいかかるか分からない</li>
</ul>
</li>
<li>モンテカルロ制御<ul>
<li>エピソードの完了 → Q を更新</li>
</ul>
</li>
</ul>
</li>
<li>GLIE (Greedy in the Limit with Infinite Exploration) <ul>
<li>全ての状態-行動ペアを、無限回探索する　</li>
<li>方策が貪欲方策に収束する</li>
<li>GLIE モンテカルロ制御<ul>
<li>各状態-行動ペアについて、\( G_t \) の平均を \( Q(S_t, A_t) \) として保持</li>
<li>新しいQ値に従い、ε-貪欲方策を更新</li>
<li>最適な行動価値関数 \( q_*(s, a) \) に収束</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>方策オン型 TD 学習</p>
<ul>
<li>アイデア：MC の代わりに TD を制御ループの時に使う<ul>
<li>TD を \( Q(S, A) \) の推定に使う</li>
<li>Sarsa: なぜ Sarsa? (S, A) → R → S' → A'</li>
<li>更新式: \( Q(S, A) \leftarrow Q(S, A) + \alpha( R + \gamma Q(S', A') - Q(S, A) ) \)</li>
<li>方策改善には、ε貪欲探索を使う</li>
</ul>
</li>
<li>Sarsa は、\( Q(s, a) \to q_*(s, a) \) に収束する (条件付きだが、ほとんどの場合成り立つ)</li>
<li>nステップ Sarsa<ul>
<li>nステップ先までの報酬を考慮。例: \( q_t^{(2)} = R_{t+1} + \gamma R_{t+2} + \gamma^2 Q(S_{t+2}) \)</li>
<li>\( Q(S_t, A) \leftarrow Q(S_t, A) + \alpha (q_t^{(n)} - Q(S_t, A)) \)</li>
</ul>
</li>
<li>Sarsa(λ) の前向き観点<ul>
<li>\( q^\lambda_t = (1 - \lambda) \sum_{n=1}^\infty \lambda^{n-1}q_t^{(n)} \)</li>
<li>\( Q(S_t, A) \leftarrow Q(S_t, A) + \alpha (q^\lambda_t - Q(S_t, A)) \)</li>
<li>問題点：時間軸上で先読みしている。エピソードの最後まで待ちたくない</li>
</ul>
</li>
<li>Sarsa(λ) の後ろ向き観点<ul>
<li>Eligibility Trace \( E_t(s, a) \) を定義 (TD(λ) と違い、状態と行動のペアに対して定義)</li>
<li>\( \delta_t = R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \) </li>
<li>\( Q(s, a) \leftarrow Q(s, a) + \alpha \delta_t E_t(s, a) \)</li>
</ul>
</li>
</ul>
</li>
<li>
<p>方策オフ型</p>
<ul>
<li>別の方策 \( \mu \) に従いながら、対象の方策 \( \pi \) を評価</li>
<li>なぜこれが重要か<ul>
<li>人間や他のエージェントから学ぶ</li>
<li>古い方策によって作られた経験から学習</li>
<li>探索的な方策から、最適な方策を学習</li>
<li>一つの方策から、複数の方策を学習</li>
</ul>
</li>
<li>重要サンプリング<ul>
<li>異なる分布の期待値を予測する</li>
</ul>
</li>
<li>方策オフ型のモンテカルロ法に重要サンプリングを適用する<ul>
<li>非常に大きい分散。ほとんど使えない。</li>
<li>TD 学習を使うことが必須</li>
</ul>
</li>
<li>Q-Learning<ul>
<li>振る舞いを規定する方策 \( \mu \) によって取られた（実際の）行動 \( A_{t+1} \)</li>
<li>学習中の方策によって取られた（仮想的な）行動 \( A' \)</li>
<li>\( Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha (R_{t+1} + \gamma Q(S_{t+1}, A') - Q(S^t, A)) \)</li>
<li>方策オフ型 Q-Learning → 学習したい方策が貪欲的な場合 (SarsaMax)<ul>
<li>最適価値関数に収束</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
  </div>
  <div class="tag-cloud">
    <p>
    </p>
  </div>


    <div class="addthis_relatedposts_inline">


<div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_shortname = 'englishforhackers';
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>
      Please enable JavaScript to view comments.

</noscript>
</article>

    <footer>
<p>
  &copy; Masato Hagiwara 2017 - This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>
</p>
<p>  Powered by <a href="http://getpelican.com" target="_blank">Pelican</a> - <a href="https://github.com/alexandrevicenzi/flex" target="_blank">Flex</a> theme by <a href="http://alexandrevicenzi.com" target="_blank">Alexandre Vicenzi</a>
</p><p>
  <a rel="license"
     href="http://creativecommons.org/licenses/by-nc-sa/4.0/"
     target="_blank">
    <img alt="Creative Commons License"
         title="Creative Commons License"
         style="border-width:0"
         src="https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png"
         width="80"
         height="15"/>
  </a>
</p>    </footer>
  </main>

<!-- Google Analytics -->
<script type="text/javascript">
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-175204-13', 'auto');
  ga('send', 'pageview');
</script>
<!-- End Google Analytics -->



<script type="application/ld+json">
{
  "@context" : "http://schema.org",
  "@type" : "Blog",
  "name": " エンジニア・研究者の英語学習 ",
  "url" : "http://englishforhackers.com",
  "image": "http://masatohagiwara.net/face.jpg",
  "description": "ソフトウェア・エンジニアや研究者のための英語学習情報。講演/トーク・技術記事・論文などの紹介"
}
</script>

</body>
</html>