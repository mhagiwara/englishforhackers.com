Title: Ilya Sutskever 氏によるメタ学習と自己学習の最前線 (MIT 講義まとめ)
Date: 2018-05-25 00:00
Category: Deep Learning
slug: ilya-sutskever-meta-learning-self-play

OpenAI の共同創立者兼研究ディレクターである Ilya Sutskever 氏。過去 5 年以内の論文の総引用数が 46,000 を超えるという、名実共に現在の人工知能・深層学習分野の第一人者でしょう。

MIT の「汎用人工知能(Artificial General Intelligence)」という講義シリーズの中でのSutskever 氏の講演が上がっていましたので、今回は抄訳とともにご紹介します。
メタ学習や自己学習など、最先端の話題もさることながら、進化論への言及など、非常に示唆深い、分かりやすい講演でおすすめです。

[講演ビデオはこちら](https://www.youtube.com/watch?v=9EN_HoEk3KY)

### はじめに

- そもそも深層学習はなぜうまくいくか
    - 定理：与えられたデータに対して、そのデータを生成できる最小のプログラムが、最も良い予測ができる
    - ただし、データが与えられたときに、そのデータを生成できる最小のプログラムを見つけることは困難である
    - 最小のプログラムの代わりに、最小の「回路」なら、逆伝播を使って見つけることができる
    - AI の基礎となる事実
    - ニューラルネットの訓練 → ニューラル「式」を解いている
    - N個の Nビットの数字を、2つしか隠れ層を持たないニューラルネットでソートできる
- 強化学習
    - エージェントを環境内で評価する枠組み
    - 完璧ではないが「そこそこ良い」結果を出すアルゴリズムが存在する
    - 問題点：報酬が環境によって与えられる点。現実では、エージェント(人間)が観察から報酬を「理解」する
        - 本当の「報酬」は、生存か死亡か。それ以外は全てそれに帰結される
    - アルゴリズム
        - ロバストでシンプルだが、効率が良くない
        - まとめ「ランダムに新しいことを試す。予測と現実を比較し、現実が予測を上回ったなら、これらの行動を強化するようにパラーメタを変える」
    - 2つの手法
        - Policy gradient
            - 報酬の合計の期待値 → 微分 → 奇跡的に、上の「まとめ」と同じになる
            - 方策オン型 (on-policy) 
            - 自分自身の行動からしか学習できない
        - Q-learning
            - 方策オフ型 (off-policy)
            - 自分自身ではなく、他のエージェントからのデータでも学習できる
    - 強化学習のポテンシャル
        - サンプル効率性が良い
        - 現在のアルゴリズムの効率は良くないが、徐々に改善している
		
### メタ学習

- 学習方法を学習する
- システムを複数のタスクで訓練する
- インスタンスの代わりに、「タスク＋テストケース」を与える
- 例
    - [Mishra et al. 2017] 手書き文字を分類する
    - [Zoph and Le, 2017] ニューラル構造サーチ
- 例 [Peng et al. 2017]
    - 方策をシミュレーションで学習し、物理的なロボットに移すことができるか
    - 問題：シミュレーターと現実が違う（特に摩擦などを再現するのが難しい）
    - シミュレーター環境の物理条件にランダム性を加え、それに適応する方策を学習する
- 例 [Frans et al. 2017]
    - 方策の階層
    - メタ学習を利用し、タスクを最速で達成するための基礎行動を学習
- 現在の機械学習
    - 訓練時とテスト時の設定が同じ
    - 現実はそうではない（学校で学んだことが仕事で役立つとは限らない）

- Hindsight Experience Replay (事後経験再現) [Andrychowicz et al. 2017]
	- 強化学習の問題：報酬が無いと学習できない
	- 「行動を経て、ゴールを達成できなかった」という経験、すなわち失敗から何かを学習できないか
	- 例: A に到達したかったのに、B に到達してしまった
		- 「A に到達できなかった。何も学べなかった」ではなく「A ではなく、B に到達する方法を学んだ」
	- A に到達するとき、A に到達する方法の方策オン型学習をしているのと同時に、B に到達する方法の方策オフ型学習をしている

### 自己学習

- TD-Gammon [Tesauro, 1992]
    - 2つのニューラルネットにバックギャモンの対戦をさせる
    - 世界チャンピオンに勝つ戦略を生み出す
- AlphaGo Zero
    - 囲碁の世界チャンピオンに外部データ無しで勝つ
- DOTA 2
    - 1対1で世界チャンピオンに勝つ
- 自己学習の魅力
    - エージェント自身が環境を作る
    - 敵対関係にあることで、お互いに学習する
- 相撲 [Bansal et al., 2017]
    - お互いを外に出すだけ
    - 立つ、バランスを取るなど、何も分からない状態からスタート
    - 学習するには、自分と同じぐらい賢い相手が必要。いつも勝っていたら学習しない
    - 転移学習も起きる。相撲で学習したエージェントにランダムに力を加えてもバランスが取れる
- Dota 2 Bot
    - はじめは弱かったが、数ヶ月で世界チャンピオンに匹敵するほどに
    - 「自己学習は、計算をデータに変換する」
    - 今後、コンピュータの能力が高まるにしたがって、さらに重要度を増す
- 未来
    - 自己学習の結果が、外のタスクにも有用になるためにはどうしたらよいか
    - 人間の脳は、過去200万年に渡って、急激にその容量が増大した
        - Sutskever 氏の考え：「部族の中での立ち位置」が生存に重要であることに気づいた
        - 他の「大きい脳を持つ」人間とうまくやっていくには、もっと大きな脳を持つことが生存に有利
        - 制約の無い自己学習から、交渉、言語、政治などが生まれる可能性も。
    - もし、このような環境から汎用知能が生まれ、かつ、Dota 2 Bot で見たような急激な能力の向上が引き継がれるなら、この汎用知能の能力も急速に向上するだろう
- アラインメント [Christiano et al., 2017]
    - 目標をどうエージェントに伝えるか
    - 行動を２つ人間に見せて、どちらか良いかを選ぶだけ
    - 500回ぐらい選ぶと、シミュレーションした「足」が宙返りできるようになる
    - Atari のゲームも学習できる
