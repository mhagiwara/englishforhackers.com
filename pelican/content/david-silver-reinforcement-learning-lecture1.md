Title: 強化学習入門 - Google DeepMind の David Silver 氏による強化学習コース 講義1
Date: 2018-09-01 00:00
Category: Reinforcement Learning
slug: david-silver-reinforcement-learning-lecture1

<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script>

「無料でアクセスできる最高の強化学習のコース」と名高い、Google DeepMind / University College London の David Silver 氏による強化学習のコース。[こちらのページから、全ての講義スライドと講義ビデオが見られる](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html)。

- 教科書
	- An Introduction to Reinforcement Learning
		- 直感的, このコースで参照
	- Algorithms for Reinforcement Learning
		- 理論, 厳密

- 強化学習とは
	- 様々な分野と関係
	- 工学、機械学習、神経科学（脳の報酬システムと関係）
	- 機械学習の３つの分類
		- 教師あり学習、教師なし学習、強化学習

- 他の機械学習アルゴリズムとの違い
	- 教師の代わりに、報酬信号しかない
	- 報酬がすぐに得られるとは限らない
	- 時間の概念が重要。iid (独立同分布)データではない
	- エージェントが環境に影響を及ぼす→データも変わる

- 強化学習の例
	- ヘリコプターの曲芸を学習
	- バックギャモンで世界チャンピオンに勝つ
	- 投資ポートフォリオの管理
	- 発電所の制御
	- 人間型ロボットを歩かせる
	- Atari の複数のゲームをプレイする
	- Q：強化学習アルゴリズムは、人間の反応時間に比べて速く操作ができるので有利ではないか？ → A: 人間の反応時間に合わせてあるので、公平なはず

- 強化学習問題
	- 報酬 \\( R_t \\) --> スカラー値のフィードバック信号。時刻 t においてどのぐらい「うまく行っているか」
	- 報酬の合計の期待値を最大化させるのが目的
	- 報酬に関する仮定：全てのゴールは、累積報酬の期待値を最大化させる問題に帰着できる

- 継続的な意思決定
	- 目的：将来の報酬の合計を最大化させる行動を選択する
	- 貪欲的に行動するべきではない → 行動が長期にわかって効果を残す。報酬がすぐ得られるとは限らない。

- エージェントと環境
	- エージェントが環境を観察 \\( O_t \\)
	- 行動 \\( A_t \\)
	- 報酬 \\( R_t \\)

- 履歴と状態
	- 履歴 \\( H_t = A_1, O_1, R_1, ..., A_t, O_t, R_t \\)
	- 状態 \\( S_t = f(H_t) \\)
	- 環境状態 \\( S^e_t \\) → エージェントからは見えない
	- エージェント状態 \\( S^a_t \\) 
	- マルコフ性: \\( P[S_{t+1} | S_t] = P[S_{t+1} | S_1, ..., S_t] \\)
		- 次の状態は、現在の状態だけに依存する
		- 現在の状態が分かれば、履歴は不要
		- 状態は、未来の十分統計量
		- ヘリコプターの例：現在の位置、速度、角度、各速度 etc.  位置だけではマルコフ性が成立しない
	- 完全に観察可能な環境
		- \\( O_t = S^a_t = S^e_t \\) → マルコフ決定過程 (Markov Decision Process; MDP)
	- 部分的に観察可能な環境
		- 部分観測マルコフ決定過程 (Partially Observable MDP; POMDP)
		- エージェント状態を、環境状態とは独立に構築する必要がある
			- 方法1: 状態に対する信念（確率分布）を維持する
			- 方法2: 前の状態と、現在の観察から、次の状態を予測する RNN を構築する

- エージェント
	- 方策: エージェントがどのように意思決定するか
		- 状態 s から行動 a への関数
		- 決定的な方策: \\( a = \pi(s) \\)
		- 確率的な方策: \\( \pi(a | s) = P[A = a | S = s]\\)
	- 価値関数: それぞれの状態/行動がどのぐらい良いか
		- 将来の報酬に対する予測
		- 方策に依存
		- \\( v_\pi(s) = E_\pi[R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + ... | S_t = s] \\)
		- 時間による割引 \\( \gamma \\) → 遠い未来より近い未来の報酬を優先
	- モデル: エージェントによる環境の表現（「エージェントが環境がどういう仕組みで動いていると思っているか」）
		- 遷移モデル: 次の状態を予想する
		- 報酬モデル: 次の報酬を予想する

- エージェントの分類
	- 価値ベース: 価値関数を使う
	- 方策ベース: 方策を使う
	- Actor Critic: 価値関数と方策の両方を使う
	- モデル無し: 価値関数・方策のどちらかもしくは両方、モデル無し
	- モデル有り: 価値関数・方策のどちらかもしくは両方、モデル有り

- 学習とプランニング
	- 強化学習: 環境が未知の状態からスタート、エージェントが環境と相互作用し、方策を改善する
	- プランニング: 環境のモデルが与えられる、相互作用せずにモデルを使って計算、方策を改善する

- 探索と搾取
	- 探索: 報酬をあきらめてでも、環境に関する情報を得る
	- 搾取: 既に知っている情報を使い、報酬を最大化する
