<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>エンジニア・研究者の英語学習 - Reinforcement Learning</title><link href="http://englishforhackers.com/" rel="alternate"></link><link href="http://englishforhackers.com/feeds/reinforcement-learning.atom.xml" rel="self"></link><id>http://englishforhackers.com/</id><updated>2018-09-01T00:00:00-04:00</updated><entry><title>強化学習入門 - Google DeepMind の David Silver 氏による強化学習コース 講義1</title><link href="http://englishforhackers.com/david-silver-reinforcement-learning-lecture1.html" rel="alternate"></link><published>2018-09-01T00:00:00-04:00</published><updated>2018-09-01T00:00:00-04:00</updated><author><name>Masato Hagiwara</name></author><id>tag:englishforhackers.com,2018-09-01:/david-silver-reinforcement-learning-lecture1.html</id><summary type="html">&lt;script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async&gt;&lt;/script&gt;

&lt;p&gt;「無料でアクセスできる最高の強化学習のコース」と名高い、Google DeepMind / University College London の David Silver 氏による強化学習のコース。&lt;a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html"&gt;こちらのページから、全ての講義スライドと講義ビデオが見られる&lt;/a&gt;。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;教科書&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;An Introduction to Reinforcement Learning&lt;ul&gt;
&lt;li&gt;直感的, このコースで参照&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Algorithms for Reinforcement Learning&lt;ul&gt;
&lt;li&gt;理論, 厳密&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;強化学習とは&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;様々な分野と関係&lt;/li&gt;
&lt;li&gt;工学、機械学習、神経科学（脳の報酬システムと関係）&lt;/li&gt;
&lt;li&gt;機械学習の３つの分類&lt;ul&gt;
&lt;li&gt;教師あり学習、教師なし学習、強化学習&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;他の機械学習アルゴリズムとの違い&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;教師の代わりに、報酬信号しかない&lt;/li&gt;
&lt;li&gt;報酬がすぐに得られるとは限らない&lt;/li&gt;
&lt;li&gt;時間の概念が重要。iid (独立同分布)データではない&lt;/li&gt;
&lt;li&gt;エージェントが環境に影響を及ぼす→データも変わる&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;強化学習の例&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ヘリコプターの曲芸を学習&lt;/li&gt;
&lt;li&gt;バックギャモンで世界チャンピオンに勝つ …&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async&gt;&lt;/script&gt;

&lt;p&gt;「無料でアクセスできる最高の強化学習のコース」と名高い、Google DeepMind / University College London の David Silver 氏による強化学習のコース。&lt;a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html"&gt;こちらのページから、全ての講義スライドと講義ビデオが見られる&lt;/a&gt;。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;教科書&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;An Introduction to Reinforcement Learning&lt;ul&gt;
&lt;li&gt;直感的, このコースで参照&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Algorithms for Reinforcement Learning&lt;ul&gt;
&lt;li&gt;理論, 厳密&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;強化学習とは&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;様々な分野と関係&lt;/li&gt;
&lt;li&gt;工学、機械学習、神経科学（脳の報酬システムと関係）&lt;/li&gt;
&lt;li&gt;機械学習の３つの分類&lt;ul&gt;
&lt;li&gt;教師あり学習、教師なし学習、強化学習&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;他の機械学習アルゴリズムとの違い&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;教師の代わりに、報酬信号しかない&lt;/li&gt;
&lt;li&gt;報酬がすぐに得られるとは限らない&lt;/li&gt;
&lt;li&gt;時間の概念が重要。iid (独立同分布)データではない&lt;/li&gt;
&lt;li&gt;エージェントが環境に影響を及ぼす→データも変わる&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;強化学習の例&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ヘリコプターの曲芸を学習&lt;/li&gt;
&lt;li&gt;バックギャモンで世界チャンピオンに勝つ&lt;/li&gt;
&lt;li&gt;投資ポートフォリオの管理&lt;/li&gt;
&lt;li&gt;発電所の制御&lt;/li&gt;
&lt;li&gt;人間型ロボットを歩かせる&lt;/li&gt;
&lt;li&gt;Atari の複数のゲームをプレイする&lt;/li&gt;
&lt;li&gt;Q：強化学習アルゴリズムは、人間の反応時間に比べて速く操作ができるので有利ではないか？ → A: 人間の反応時間に合わせてあるので、公平なはず&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;強化学習問題&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;報酬 \( R_t \) --&amp;gt; スカラー値のフィードバック信号。時刻 t においてどのぐらい「うまく行っているか」&lt;/li&gt;
&lt;li&gt;報酬の合計の期待値を最大化させるのが目的&lt;/li&gt;
&lt;li&gt;報酬に関する仮定：全てのゴールは、累積報酬の期待値を最大化させる問題に帰着できる&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;継続的な意思決定&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;目的：将来の報酬の合計を最大化させる行動を選択する&lt;/li&gt;
&lt;li&gt;貪欲的に行動するべきではない → 行動が長期にわかって効果を残す。報酬がすぐ得られるとは限らない。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;エージェントと環境&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;エージェントが環境を観察 \( O_t \)&lt;/li&gt;
&lt;li&gt;行動 \( A_t \)&lt;/li&gt;
&lt;li&gt;報酬 \( R_t \)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;履歴と状態&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;履歴 \( H_t = A_1, O_1, R_1, ..., A_t, O_t, R_t \)&lt;/li&gt;
&lt;li&gt;状態 \( S_t = f(H_t) \)&lt;/li&gt;
&lt;li&gt;環境状態 \( S^e_t \) → エージェントからは見えない&lt;/li&gt;
&lt;li&gt;エージェント状態 \( S^a_t \) &lt;/li&gt;
&lt;li&gt;マルコフ性: \( P[S_{t+1} | S_t] = P[S_{t+1} | S_1, ..., S_t] \)&lt;ul&gt;
&lt;li&gt;次の状態は、現在の状態だけに依存する&lt;/li&gt;
&lt;li&gt;現在の状態が分かれば、履歴は不要&lt;/li&gt;
&lt;li&gt;状態は、未来の十分統計量&lt;/li&gt;
&lt;li&gt;ヘリコプターの例：現在の位置、速度、角度、各速度 etc.  位置だけではマルコフ性が成立しない&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;完全に観察可能な環境&lt;ul&gt;
&lt;li&gt;\( O_t = S^a_t = S^e_t \) → マルコフ決定過程 (Markov Decision Process; MDP)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;部分的に観察可能な環境&lt;ul&gt;
&lt;li&gt;部分観測マルコフ決定過程 (Partially Observable MDP; POMDP)&lt;/li&gt;
&lt;li&gt;エージェント状態を、環境状態とは独立に構築する必要がある&lt;ul&gt;
&lt;li&gt;方法1: 状態に対する信念（確率分布）を維持する&lt;/li&gt;
&lt;li&gt;方法2: 前の状態と、現在の観察から、次の状態を予測する RNN を構築する&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;エージェント&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;方策: エージェントがどのように意思決定するか&lt;ul&gt;
&lt;li&gt;状態 s から行動 a への関数&lt;/li&gt;
&lt;li&gt;決定的な方策: \( a = \pi(s) \)&lt;/li&gt;
&lt;li&gt;確率的な方策: \( \pi(a | s) = P[A = a | S = s]\)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;価値関数: それぞれの状態/行動がどのぐらい良いか&lt;ul&gt;
&lt;li&gt;将来の報酬に対する予測&lt;/li&gt;
&lt;li&gt;方策に依存&lt;/li&gt;
&lt;li&gt;\( v_\pi(s) = E_\pi[R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + ... | S_t = s] \)&lt;/li&gt;
&lt;li&gt;時間による割引 \( \gamma \) → 遠い未来より近い未来の報酬を優先&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;モデル: エージェントによる環境の表現（「エージェントが環境がどういう仕組みで動いていると思っているか」）&lt;ul&gt;
&lt;li&gt;遷移モデル: 次の状態を予想する&lt;/li&gt;
&lt;li&gt;報酬モデル: 次の報酬を予想する&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;エージェントの分類&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;価値ベース: 価値関数を使う&lt;/li&gt;
&lt;li&gt;方策ベース: 方策を使う&lt;/li&gt;
&lt;li&gt;Actor Critic: 価値関数と方策の両方を使う&lt;/li&gt;
&lt;li&gt;モデル無し: 価値関数・方策のどちらかもしくは両方、モデル無し&lt;/li&gt;
&lt;li&gt;モデル有り: 価値関数・方策のどちらかもしくは両方、モデル有り&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;学習とプランニング&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;強化学習: 環境が未知の状態からスタート、エージェントが環境と相互作用し、方策を改善する&lt;/li&gt;
&lt;li&gt;プランニング: 環境のモデルが与えられる、相互作用せずにモデルを使って計算、方策を改善する&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;探索と搾取&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;探索: 報酬をあきらめてでも、環境に関する情報を得る&lt;/li&gt;
&lt;li&gt;搾取: 既に知っている情報を使い、報酬を最大化する&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;</content></entry></feed>