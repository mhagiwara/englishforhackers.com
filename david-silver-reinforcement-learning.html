
<!DOCTYPE html>
<html lang="ja">
<head>
  <link href='//fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,700,400italic' rel='stylesheet' type='text/css'>

    <link rel="stylesheet/less" type="text/css" href="http://englishforhackers.com/theme/stylesheet/style.less">
    <script src="//cdnjs.cloudflare.com/ajax/libs/less.js/2.5.1/less.min.js" type="text/javascript"></script>

  <link rel="stylesheet" type="text/css" href="http://englishforhackers.com/theme/pygments/monokai.min.css">
  <link rel="stylesheet" type="text/css" href="http://englishforhackers.com/theme/font-awesome/css/font-awesome.min.css">

    <link href="http://englishforhackers.com/static/custom.css" rel="stylesheet">

    <link href="http://englishforhackers.com/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="エンジニアの英語学習法 Atom">


    <link rel="shortcut icon" href="/images/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/images/favicon.ico" type="image/x-icon">

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="robots" content="index, follow" />

    <!-- Chrome, Firefox OS and Opera -->
    <meta name="theme-color" content="#333333">
    <!-- Windows Phone -->
    <meta name="msapplication-navbutton-color" content="#333333">
    <!-- iOS Safari -->
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

<meta name="author" content="Masato Hagiwara" />
<meta name="description" content="「無料でアクセスできる最高の強化学習のコース」と名高い、Google DeepMind / University College London の David Silver 氏による強化学習のコース。こちらのページから、全ての講義スライドと講義ビデオが見られる。 講義1: 強化学習入門 教科書 An Introduction to Reinforcement Learning 直感的, このコースで参照 Algorithms for Reinforcement Learning 理論, 厳密 強化学習とは 様々な分野と関係 工学、機械学習、神経科学（脳の報酬システムと関係） 機械学習の３つの分類 教師あり学習、教師なし学習、強化学習 他の機械学習アルゴリズムとの違い 教師の代わりに、報酬信号しかない 報酬がすぐに得られるとは限らない 時間の概念が重要。iid (独立同分布)データではない エージェントが環境に影響を及ぼす→データも変わる 強化学習の例 …" />
<meta name="keywords" content="">
<meta property="og:site_name" content="エンジニアの英語学習法"/>
<meta property="og:title" content="強化学習入門 - Google DeepMind の David Silver 氏による強化学習コース"/>
<meta property="og:description" content="「無料でアクセスできる最高の強化学習のコース」と名高い、Google DeepMind / University College London の David Silver 氏による強化学習のコース。こちらのページから、全ての講義スライドと講義ビデオが見られる。 講義1: 強化学習入門 教科書 An Introduction to Reinforcement Learning 直感的, このコースで参照 Algorithms for Reinforcement Learning 理論, 厳密 強化学習とは 様々な分野と関係 工学、機械学習、神経科学（脳の報酬システムと関係） 機械学習の３つの分類 教師あり学習、教師なし学習、強化学習 他の機械学習アルゴリズムとの違い 教師の代わりに、報酬信号しかない 報酬がすぐに得られるとは限らない 時間の概念が重要。iid (独立同分布)データではない エージェントが環境に影響を及ぼす→データも変わる 強化学習の例 …"/>
<meta property="og:locale" content="ja_JP"/>
<meta property="og:url" content="http://englishforhackers.com/david-silver-reinforcement-learning.html"/>
<meta property="og:type" content="article"/>
<meta property="article:published_time" content="2018-09-01 00:00:00-04:00"/>
<meta property="article:modified_time" content=""/>
<meta property="article:author" content="http://englishforhackers.com/author/masato-hagiwara.html">
<meta property="article:section" content="Reinforcement Learning"/>
<meta property="og:image" content="http://masatohagiwara.net/face.jpg">

  <title>強化学習入門 - Google DeepMind の David Silver 氏による強化学習コース &ndash; エンジニアの英語学習法</title>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
    (adsbygoogle = window.adsbygoogle || []).push({
      google_ad_client: "ca-pub-7401771876348738",
      enable_page_level_ads: true
    });
  </script>
</head>
<body>
  <aside>
    <div>
      <a href="http://englishforhackers.com">
        <img src="http://masatohagiwara.net/face.jpg" alt="エンジニアの<br/>英語学習法" title="エンジニアの<br/>英語学習法">
      </a>
      <h1><a href="http://englishforhackers.com">エンジニアの<br/>英語学習法</a></h1>

<p>英語を身につけるためには、興味のある教材を見つけるのが一番。ソフトウェア・エンジニアや研究者のための英語学習情報をお届けします。英語の講演やトーク、講義、記事などの紹介が中心です。</p>
      <nav>
        <ul class="list">

          <li><a href="http://masatohagiwara.net/" target="_blank">個人ページ</a></li>
          <li><a href="http://www.duolingo.com/" target="_blank">Duolingo</a></li>
          <li><a href="https://docs.google.com/forms/d/e/1FAIpQLSffbTFvdUXJhN4jTOmylcIRvlntKmaYVkIbYPrbBCPm0iC9Sw/viewform?usp=sf_link" target="_blank">お問い合わせ</a></li>
        </ul>
      </nav>

      <ul class="social">
        <li><a class="sc-github" href="https://github.com/mhagiwara" target="_blank"><i class="fa fa-github"></i></a></li>
        <li><a class="sc-twitter" href="https://twitter.com/mhagiwara" target="_blank"><i class="fa fa-twitter"></i></a></li>
        <li><a class="sc-rss" href="feeds/all.atom.xml" target="_blank"><i class="fa fa-rss"></i></a></li>
      </ul>
    </div>


  </aside>
  <main>

    <nav>
      <a href="http://englishforhackers.com">  Home
</a>

      <a href="/archives.html">Archives</a>
      <a href="/categories.html">Categories</a>
      <a href="/tags.html">Tags</a>

      <a href="http://englishforhackers.com/feeds/all.atom.xml">  Atom
</a>

    </nav>

<article class="single">
  <header>
    <h1 id="david-silver-reinforcement-learning">強化学習入門 - Google DeepMind の David Silver 氏による強化学習コース</h1>
    <p>
        Posted on 2018-09-01(土) in <a href="http://englishforhackers.com/category/reinforcement-learning.html">Reinforcement Learning</a>


    </p>
  </header>


  <div>
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script>

<p>「無料でアクセスできる最高の強化学習のコース」と名高い、Google DeepMind / University College London の David Silver 氏による強化学習のコース。<a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html">こちらのページから、全ての講義スライドと講義ビデオが見られる</a>。</p>
<h2>講義1: 強化学習入門</h2>
<ul>
<li>
<p>教科書</p>
<ul>
<li>An Introduction to Reinforcement Learning<ul>
<li>直感的, このコースで参照</li>
</ul>
</li>
<li>Algorithms for Reinforcement Learning<ul>
<li>理論, 厳密</li>
</ul>
</li>
</ul>
</li>
<li>
<p>強化学習とは</p>
<ul>
<li>様々な分野と関係</li>
<li>工学、機械学習、神経科学（脳の報酬システムと関係）</li>
<li>機械学習の３つの分類<ul>
<li>教師あり学習、教師なし学習、強化学習</li>
</ul>
</li>
</ul>
</li>
<li>
<p>他の機械学習アルゴリズムとの違い</p>
<ul>
<li>教師の代わりに、報酬信号しかない</li>
<li>報酬がすぐに得られるとは限らない</li>
<li>時間の概念が重要。iid (独立同分布)データではない</li>
<li>エージェントが環境に影響を及ぼす→データも変わる</li>
</ul>
</li>
<li>
<p>強化学習の例</p>
<ul>
<li>ヘリコプターの曲芸を学習</li>
<li>バックギャモンで世界チャンピオンに勝つ</li>
<li>投資ポートフォリオの管理</li>
<li>発電所の制御</li>
<li>人間型ロボットを歩かせる</li>
<li>Atari の複数のゲームをプレイする</li>
<li>Q：強化学習アルゴリズムは、人間の反応時間に比べて速く操作ができるので有利ではないか？ → A: 人間の反応時間に合わせてあるので、公平なはず</li>
</ul>
</li>
<li>
<p>強化学習問題</p>
<ul>
<li>報酬 \( R_t \) --&gt; スカラー値のフィードバック信号。時刻 t においてどのぐらい「うまく行っているか」</li>
<li>報酬の合計の期待値を最大化させるのが目的</li>
<li>報酬に関する仮定：全てのゴールは、累積報酬の期待値を最大化させる問題に帰着できる</li>
</ul>
</li>
<li>
<p>継続的な意思決定</p>
<ul>
<li>目的：将来の報酬の合計を最大化させる行動を選択する</li>
<li>貪欲的に行動するべきではない → 行動が長期にわかって効果を残す。報酬がすぐ得られるとは限らない。</li>
</ul>
</li>
<li>
<p>エージェントと環境</p>
<ul>
<li>エージェントが環境を観察 \( O_t \)</li>
<li>行動 \( A_t \)</li>
<li>報酬 \( R_t \)</li>
</ul>
</li>
<li>
<p>履歴と状態</p>
<ul>
<li>履歴 \( H_t = A_1, O_1, R_1, ..., A_t, O_t, R_t \)</li>
<li>状態 \( S_t = f(H_t) \)</li>
<li>環境状態 \( S^e_t \) → エージェントからは見えない</li>
<li>エージェント状態 \( S^a_t \) </li>
<li>マルコフ性: \( P[S_{t+1} | S_t] = P[S_{t+1} | S_1, ..., S_t] \)<ul>
<li>次の状態は、現在の状態だけに依存する</li>
<li>現在の状態が分かれば、履歴は不要</li>
<li>状態は、未来の十分統計量</li>
<li>ヘリコプターの例：現在の位置、速度、角度、各速度 etc.  位置だけではマルコフ性が成立しない</li>
</ul>
</li>
<li>完全に観察可能な環境<ul>
<li>\( O_t = S^a_t = S^e_t \) → マルコフ決定過程 (Markov Decision Process; MDP)</li>
</ul>
</li>
<li>部分的に観察可能な環境<ul>
<li>部分観測マルコフ決定過程 (Partially Observable MDP; POMDP)</li>
<li>エージェント状態を、環境状態とは独立に構築する必要がある<ul>
<li>方法1: 状態に対する信念（確率分布）を維持する</li>
<li>方法2: 前の状態と、現在の観察から、次の状態を予測する RNN を構築する</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>エージェント</p>
<ul>
<li>方策: エージェントがどのように意思決定するか<ul>
<li>状態 s から行動 a への関数</li>
<li>決定的な方策: \( a = \pi(s) \)</li>
<li>確率的な方策: \( \pi(a | s) = P[A = a | S = s]\)</li>
</ul>
</li>
<li>価値関数: それぞれの状態/行動がどのぐらい良いか<ul>
<li>将来の報酬に対する予測</li>
<li>方策に依存</li>
<li>\( v_\pi(s) = E_\pi[R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + ... | S_t = s] \)</li>
<li>時間による割引 \( \gamma \) → 遠い未来より近い未来の報酬を優先</li>
</ul>
</li>
<li>モデル: エージェントによる環境の表現（「エージェントが環境がどういう仕組みで動いていると思っているか」）<ul>
<li>遷移モデル: 次の状態を予想する</li>
<li>報酬モデル: 次の報酬を予想する</li>
</ul>
</li>
</ul>
</li>
<li>
<p>エージェントの分類</p>
<ul>
<li>価値ベース: 価値関数を使う</li>
<li>方策ベース: 方策を使う</li>
<li>Actor Critic: 価値関数と方策の両方を使う</li>
<li>モデル無し: 価値関数・方策のどちらかもしくは両方、モデル無し</li>
<li>モデル有り: 価値関数・方策のどちらかもしくは両方、モデル有り</li>
</ul>
</li>
<li>
<p>学習とプランニング</p>
<ul>
<li>強化学習: 環境が未知の状態からスタート、エージェントが環境と相互作用し、方策を改善する</li>
<li>プランニング: 環境のモデルが与えられる、相互作用せずにモデルを使って計算、方策を改善する</li>
</ul>
</li>
<li>
<p>探索と搾取</p>
<ul>
<li>探索: 報酬をあきらめてでも、環境に関する情報を得る</li>
<li>搾取: 既に知っている情報を使い、報酬を最大化する</li>
</ul>
</li>
</ul>
<h2>講義2: マルコフ決定過程</h2>
<ul>
<li>
<p>マルコフ決定過程 (MDP)</p>
<ul>
<li>環境が完全に観察可能</li>
<li>状態が、過程を完全に規定する</li>
<li>多くの強化学習問題が、MDP として定式化可能</li>
<li>部分観測マルコフ決定過程 (POMDP) も、MDP に変換可能</li>
<li>バンディットアルゴリズムも、状態が一つしかない MDP</li>
</ul>
</li>
<li>
<p>マルコフ性</p>
<ul>
<li>次に何が起こるかは、今の状態だけに依存</li>
<li>Lecture 1 参照</li>
<li>状態遷移確率 \( P_{ss'} = P[S_{t+1} = s' | S_t = s] \) 行列で表現可能</li>
</ul>
</li>
<li>
<p>マルコフ過程</p>
<ul>
<li>状態列 \( S_1, S_2, ... \) がマルコフ性を満たすとき → マルコフ過程</li>
</ul>
</li>
<li>
<p>マルコフ報酬過程 (Markov Reward Process; MRP)</p>
<ul>
<li>R: 報酬関数 \( R_s = E[ R_{t+1} | S_t = s] \)</li>
<li>利得 \( G_t = \sum_{k=0}^\infty \gamma^k R_{t+k+1} \)</li>
<li>なぜ割引率 \( \gamma \) を使うか → 数学的に便利。報酬が発散するのを防ぐ。未来に行くほど不確定。直近の未来の報酬を優先。</li>
</ul>
</li>
<li>
<p>価値関数</p>
<ul>
<li>状態 s に居るときの利得の期待値 \( v(s) = E[G_t | S_t = s] \)</li>
</ul>
</li>
<li>
<p>ベルマン方程式</p>
<ul>
<li>\( v(s) = E[R_{t+1} + \gamma v(S_{t+1}) | S_t = s] \)</li>
<li>価値観数は、1) すぐ次の報酬、2) 次の状態の価値（＋割り引き）の２つに分解できる</li>
<li>行列表現: \( v = R + \gamma Pv \)<ul>
<li>v: \( v = (v(1), ..., v(n))^T \)</li>
<li>R: \( R = (R(1), ..., R(n))^T \)</li>
<li>P: 状態 i から j への遷移確率行列</li>
</ul>
</li>
<li>解析的に解ける: \( v = (I - \gamma P)^{-1}R \)<ul>
<li>小さい MDP にしか適用できない。</li>
</ul>
</li>
</ul>
</li>
<li>
<p>マルコフ決定過程</p>
<ul>
<li>マルコフ報酬過程 + 行動</li>
<li>報酬 R: \( R^a_s = E[R_{t+1} | S_t = s, A_t = a] \)</li>
<li>
<p>方策</p>
<ul>
<li>\( \pi(a | s) = P[A_t = a | S_t = s] \) → エージェントの振る舞いを完全に規定<ul>
<li>時間 \( t \) に依存しない</li>
</ul>
</li>
<li>MDP と方策 \( \pi \) が与えられたとき、状態系列 \( S_1, S_2, ... \) はマルコフ過程 → マルコフ決定過程を「平ら」にする</li>
</ul>
</li>
<li>
<p>価値関数</p>
<ul>
<li>状態価値関数は、方策 \( \pi \) に依存： \( v_\pi(s) = E_\pi[G_t | S_t = s] \)</li>
<li>行動価値関数: \( q(s, a) = E[G_t | S_t = s, A_t = a] \)</li>
<li>ベルマン方程式を使って、直近の報酬と次の状態の価値に分解できる<ul>
<li>状態価値関数: \( v_\pi(s) = \sum_{a \in A} \pi(a|s) q_\pi(a, s) \)</li>
<li>行動価値関数: \( q(s, a) = R^a_s + \gamma \sum_{s' \in S}P^a_{ss'} v_\pi(s') \)</li>
</ul>
</li>
<li>ベルマン方程式の再帰適用: \( v_\pi(s) = \sum_{a \in A} \pi(a|s)( R^a_s + \gamma \sum_{s' \in S} P^a_{ss'} v_\pi(s') ) \)</li>
<li>\( q_\pi \) にも同じことができる</li>
</ul>
</li>
<li>最適状態価値関数<ul>
<li>全ての方策の中で、価値関数が最大となるもの: \( v_*(s) = \max_\pi v_\pi(s) \)</li>
</ul>
</li>
<li>最適行動価値関数<ul>
<li>\( q_{*}(s, a) = \max_\pi q_\pi(s, a) \) → これがあれば、MDP は「解けた」（各状態において、どう行動すべきかが分かる）</li>
</ul>
</li>
<li>最適方策<ul>
<li>ある方策が他の方策より良いとは？ \( \pi \ge \pi' if v_\pi(s) \ge v_\pi'(s), \forall s \)</li>
<li>定理: 他のあらゆる方策よりも良い最適方策 \( \pi_* \) が存在する。複数存在する場合もある</li>
<li>\( q_*(s, a) \) を最大化する行動を取ることで、最適方策が得られる</li>
<li>\( v_* \) と \( q_* \) についても、上記のベルマン方程式が適用できる<ul>
<li>ただし、\( \sum_{a \in A} \) は \( \max_a \) に置き換わる</li>
</ul>
</li>
<li>非線形 (max が入る) →　閉じた形での解は存在しない</li>
<li>繰り返し<ul>
<li>価値反復 (Value Iteration)</li>
<li>方策反復 (Policy Iteration)</li>
<li>Q 学習</li>
<li>Sarsa </li>
</ul>
</li>
</ul>
</li>
<li>MDP の拡張</li>
<li>無限 / 連続 MDP</li>
<li>部分観測マルコフ決定過程 (POMDP)</li>
<li>割り引きの無い, 平均報酬 MDP</li>
</ul>
</li>
</ul>
<h2>講義3: 動的計画法による計画</h2>
<ul>
<li>
<p>はじめに</p>
<ul>
<li>動的計画法<ul>
<li>「動的」: 逐次的、時間</li>
<li>「計画」≒ 方策</li>
</ul>
</li>
<li>動的計画法がいつ使えるか<ul>
<li>最適なサブ構造に分解し、そこから最適解が求められる場合<ul>
<li>例: グラフの最短経路問題</li>
</ul>
</li>
<li>サブ問題がお互いに関係しており、何回も現れる場合 → キャッシュできる</li>
<li>MDPはこの両方を満たす<ul>
<li>ベルマン方程式</li>
<li>問題の再帰的な分解</li>
</ul>
</li>
</ul>
</li>
<li>例<ul>
<li>スケジュール</li>
<li>文字列アルゴリズム</li>
<li>グラフアルゴリズム</li>
<li>グラフィカルアルゴリズム</li>
<li>生物情報学</li>
</ul>
</li>
</ul>
</li>
<li>
<p>動的計画法を使った計画</p>
<ul>
<li>MDP の情報が全て分かっている前提</li>
<li>予測: MDP と方策 \( \pi \) が分かっている時に、価値関数 \( v_\pi \) を求める</li>
<li>操作: MDP が分かっている時に、最適方策 \( \pi_* \) を求める</li>
</ul>
</li>
<li>
<p>方策反復</p>
<ul>
<li>MDP を解くための仕組みの一つ</li>
<li>方策 \( \pi \) を評価する</li>
<li>ベルマン方程式を逆向きに繰り返し適用</li>
<li>任意の \( v_1 \) からスタート。ベルマン方程式を適用し、\( v_2 \) を得る。</li>
<li>\( v_{k+1}(s) \) を計算するためには<ul>
<li>1ステップ先読みする。\( v_{k+1}(s) = \sum_{a \in A} \pi(a|s)( R^a_s + \gamma \sum_{s' \in S} P^a_{ss'} v_k(s') ) \)</li>
</ul>
</li>
<li>これを繰り返すと、\( v_* \) に収束する</li>
</ul>
</li>
<li>
<p>方策をどう改善するか</p>
<ul>
<li>方策 \( \pi \) が与えられた時、<ul>
<li>まず、方策 \( \pi \) を評価し、\( v_\pi(s) \) を得る</li>
<li>\( v_\pi(s) \) に従い、貪欲に行動し、方策 \( \pi' \) を得る</li>
<li>「格子世界」の例では、\( \pi' \) が最適方策 \( \pi_* \)</li>
<li>一般的には、これを繰り返すと、最適方策に収束する</li>
</ul>
</li>
<li>決定的な方策 \( \pi \) からスタート<ul>
<li>貪欲に行動することで、この方策を改善できる。 \( \pi'(s) = \arg\max_{a \in A} q_\pi(s, a) \)</li>
<li>→ 価値関数も改善する</li>
</ul>
</li>
<li>この繰り返し的な改善がストップする時 → ベルマン最適方程式を満たす → 方策は最適である \( v_\pi(s) = v_*(s) \)</li>
<li>方策評価が収束するまで繰り返す必要があるか？ \( k \) 回繰り返せば十分。ただし \( k = 1 \) ではだめ。</li>
</ul>
</li>
<li>
<p>価値反復</p>
<ul>
<li>MDP を解くためのもう一つの仕組み</li>
<li>最適原則<ul>
<li>方策 \( \pi(a|s) \) は、以下の条件を満たす時、またその時に限って、最適価値関数 \( v_\pi(s) = v_*(s) \) を満たす。<ul>
<li>任意の状態 \( s' \) が \( s \) から到達可能</li>
<li>状態 \( s' \) が、最適価値関数を満たす \( v_\pi(s') = v_*(s') \)</li>
</ul>
</li>
</ul>
</li>
<li>もし、部分問題に対して最適解が分かっている時、\( v_*(s') \)<ul>
<li>１ステップ先読みする： \( v_*(s) \leftarrow \max_{a \in A} R^a_s + \gamma \sum_{s' \in S} P^a_{ss'}v_*(s') \)</li>
</ul>
</li>
<li>最適方策 \( \pi \) を探す<ul>
<li>\( v_1 \to v_2 \to ... \to v_* \)</li>
<li>\( v_{k+1}(s) \) から \( v_k(s') \) を更新</li>
<li>方策反復とは違い、方策を明示的に使わない</li>
</ul>
</li>
</ul>
</li>
<li>
<p>まとめ</p>
<ul>
<li>予測 → ベルマン期待値方程式 - 反復方策評価</li>
<li>操作 → ベルマン期待値方程式+貪欲的方策更新 - 方策反復</li>
<li>操作 → ベルマン最適方程式 - 価値反復</li>
</ul>
</li>
<li>
<p>拡張</p>
<ul>
<li>非同期動的計画法<ul>
<li>他の状態の更新が終わるまで待たない。最適値に収束する</li>
<li>In-place 動的計画法　<ul>
<li>価値関数の表を直接書き換える</li>
</ul>
</li>
<li>優先度付き sweeping<ul>
<li>どの状態を次に更新するか優先度をつける</li>
</ul>
</li>
<li>リアルタイム 動的計画法<ul>
<li>エージェントに関係のある状態だけ更新する</li>
</ul>
</li>
</ul>
</li>
<li>ベルマン方程式は、状態数が多い時に効率が悪い<ul>
<li>サンプリング</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2>講義4: モデルフリー予測</h2>
<p>モデルフリー予測 = 未知の MDP の価値関数を推定する</p>
<ul>
<li>
<p>モンテカルロ (MC) 学習</p>
<ul>
<li>経験のエピソードから直接学習する</li>
<li>エピソードが終了する必要あり</li>
<li>方策 \( \pi \) の下で、経験のエピソード \( S_1, A_1, R_2, ..., S_k \sim \pi \) から \( v_\pi \) を学習</li>
<li>復習： 利得 \( G_t = R_{t+1} + \gamma R_{t+2} + ... + \gamma^{T-1}R_T \)</li>
<li>復習： 価値関数 \( v_\pi(s) = E_\pi[G_t | S_t = s] \)</li>
<li>モンテカルロ方策評価：各エピソードについて、状態 \( s \) を最初に訪問した時に<ul>
<li>カウンターと利得の合計を更新。</li>
<li>\( V(s) \) → 利得の合計 / 訪問回数</li>
<li>十分多くの \( N(s) \) を観察すると、\( V(s) \) は \( v_\pi(s) \) に収束</li>
</ul>
</li>
<li>各訪問ごとに更新する場合<ul>
<li>訪問回数を、各訪問ごとに更新</li>
<li>これでも、\( V(s) \to v_\pi(s) \)</li>
</ul>
</li>
<li>「差分」を使った系列の平均計算<ul>
<li>系列 \( x_1, x_2, ... \) を全て観測し終わらなくても、それまでの平均 \( \mu_k \) を計算することができる</li>
<li>\( \mu_k = \mu_{k-1} + \frac{1}{k} (x_k - \mu_{k-1}) \) </li>
<li>直感的な説明： 新しい値を観測した時、それまでの推定値と大きくことなる場合は、推定値を大きく更新する。</li>
<li>\( \mu_{k-1} \) →.それまでの推定値。\( x_k - \mu_{k-1} \) → 新しい値を観測した時の「驚き」。\( \frac{1}{k} \) → 学習率</li>
</ul>
</li>
<li>差分モンテカルロ更新<ul>
<li>エピソード毎に更新（全てのエピソードの「和」を保持しない）</li>
<li>\( N(S_t) \leftarrow N(S_t) + 1 \) </li>
<li>\( V(S_t) \leftarrow V(S_t) + \frac{1}{N(S_t)}(G_t - V(S_t)) \)</li>
<li>注：エピソードが終わるまで待つ必要あり</li>
</ul>
</li>
</ul>
</li>
<li>
<p>時間差分 (Temporal Difference) 学習</p>
<ul>
<li>経験のエピソードから直接学習する (モンテカルロ法と同じ)</li>
<li>エピソードが終了してなくても良い → ブートストラップ法 (モンテカルロ法との差異)</li>
<li>MC と TD の違い<ul>
<li>MC: 実際の利得 \( G_t \) を使って更新: \( V(S_t) \leftarrow V(S_t) + \alpha(G_t - V(S_t)) \)</li>
<li>TD: 利得の予測値を使って更新: \( V(S_t) \leftarrow V(S_t) + \alpha(R_{t+1} + \gamma V(S_{t+1}) - V(S_t)) \)<ul>
<li>\( R_{t+1} + \gamma V(S_{t+1}) \) は TD ターゲットと呼ばれる</li>
<li>\( \delta_t = R_{t+1} + \gamma V(S_{t+1}) - V(S_t) \) は TD 誤差と呼ばれる</li>
</ul>
</li>
</ul>
</li>
<li>TD の長所・欠点<ul>
<li>最終結果を見る前に学習できる。最終結果の無いエピソードでも学習できる</li>
<li>偏り (Bias) と分散　(Variance) のトレードオフ<ul>
<li>真の TD ターゲット \( R_{t+1} + \gamma v_\pi(S_{t+1}) \) は \( v_\pi(S_t) \)の 不偏推定量</li>
<li>TD ターケット \( R_{t+1} + \gamma V(S_{t+1}) \) は、偏りのある推定量　</li>
<li>ただし、TD ターゲットは、利得よりも分散が小さい</li>
</ul>
</li>
<li>MC は分散大、偏りゼロ。TD は分散小、偏りあり。</li>
<li>TD(0) は \( v_\pi(s) \) に収束。初期値に敏感</li>
<li>MC は、観察された利得との平均二乗誤差を最小化する解に収束する</li>
<li>TD は、データに対して尤度最大の MDPを学習し、それを解く</li>
</ul>
</li>
<li>ブートストラップとサンプリング<ul>
<li>MC → ブートストラップ無し</li>
<li>DP → ブートストラップ有り</li>
<li>TD → ブートストラップ有り</li>
<li>MC → サンプリング有り</li>
<li>DP → サンプリング無し</li>
<li>TD → サンプリング無し</li>
</ul>
</li>
</ul>
</li>
<li>
<p>TD(λ)</p>
<ul>
<li>TD ターゲットを計算するときに、nステップ先読みする<ul>
<li>例：\( G_t^{(2)}  = R_{t+1} + \gamma R_{t+2} + \gamma^2 V(S_{t+2}) \)</li>
<li>n を大きくすると、MC 法に収束する</li>
</ul>
</li>
<li>nステップ先読みを平均する<ul>
<li>例：\( \frac{1}{2} G_t^{(2)} + \frac{1}{2} G_t^{(4)} \)</li>
</ul>
</li>
<li>γ利得 → 全てのnステップ利得の幾何平均<ul>
<li>\( G_t^\lambda = (1-\lambda) \sum_{n=1}^\infty \lambda^{n-1} G_t^n \)</li>
<li>なぜ幾何平均? 前の値を保持しなくて良いので、計算コストが低い。TD(0) と同じコストで計算できる。</li>
</ul>
</li>
<li>TD(λ) の「前向き」観点<ul>
<li>\( V(S_t) \leftarrow V(S_t) + \alpha (G^\lambda_t - V(S_t)) \)</li>
</ul>
</li>
<li>MC と同じように、エピソードが収束するまで待つ必要がある</li>
<li>TD(λ) の「後ろ向き」観点<ul>
<li>Eligibility Trace<ul>
<li>最近性と、頻度を両方考慮する量</li>
<li>\( E_0(s) = 0, E_t(s) = \gamma E_{t-1}(s) + {\mathbf 1}(S_t = s) \)</li>
</ul>
</li>
<li>\( V(s) \leftarrow V(s) + \alpha \delta_t E_t(s) \)</li>
<li>\( \lambda = 0 \) の時 → TD(0) と等価</li>
<li>\( \lambda = 1 \) の時 → 更新の合計は　MC と同じ</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2>講義5: モデルフリー制御</h2>
<ul>
<li>
<p>はじめに</p>
<ul>
<li>非常に多くの問題が、MDP としてモデル化できる</li>
<li>On-policy (方策オン型)<ul>
<li>「行動しながら学ぶ」</li>
<li>学習している方策と、サンプルを生成する方策が同じ</li>
</ul>
</li>
<li>Off-policy (方策オフ型)<ul>
<li>「他の人の行動から学ぶ」</li>
<li>学習している方策と、サンプルを生成する方策が違う</li>
</ul>
</li>
</ul>
</li>
<li>
<p>方策オン型　MC 制御</p>
<ul>
<li>復習: 方策反復：1) 方策評価 \( v_\pi \) の推定 と、2) 方策改善 (貪欲的方策改善) を繰り返す</li>
<li>ここに、MC 法による方策評価を組み込むことはできるか？<ul>
<li>問題点：\( V(s) \) に従って貪欲に行動しようとしても、MDP の完全な情報が必要 → 解法: 代わりに \( Q(s, a) \) を使う</li>
</ul>
</li>
<li>\( Q = q_\pi \) を MC で評価する<ul>
<li>問題点：探索問題。貪欲的に行動すると、必要な状態に到達することができない</li>
</ul>
</li>
<li>探索<ul>
<li>ε-貪欲探索<ul>
<li>確率εでランダムな行動を（一様分布に従って）取る</li>
<li>ε-貪欲探索に従う新しい方策 \( \pi' \) は、前の方策 \( \pi \) よりも良い</li>
</ul>
</li>
<li>MC で方策を評価し、ε-貪欲探索をすると？<ul>
<li>最適方策 \( \pi_* \) に到達する。どのぐらいかかるか分からない</li>
</ul>
</li>
<li>モンテカルロ制御<ul>
<li>エピソードの完了 → Q を更新</li>
</ul>
</li>
</ul>
</li>
<li>GLIE (Greedy in the Limit with Infinite Exploration) <ul>
<li>全ての状態-行動ペアを、無限回探索する　</li>
<li>方策が貪欲方策に収束する</li>
<li>GLIE モンテカルロ制御<ul>
<li>各状態-行動ペアについて、\( G_t \) の平均を \( Q(S_t, A_t) \) として保持</li>
<li>新しいQ値に従い、ε-貪欲方策を更新</li>
<li>最適な行動価値関数 \( q_*(s, a) \) に収束</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>方策オン型 TD 学習</p>
<ul>
<li>アイデア：MC の代わりに TD を制御ループの時に使う<ul>
<li>TD を \( Q(S, A) \) の推定に使う</li>
<li>Sarsa: なぜ Sarsa? (S, A) → R → S' → A'</li>
<li>更新式: \( Q(S, A) \leftarrow Q(S, A) + \alpha( R + \gamma Q(S', A') - Q(S, A) ) \)</li>
<li>方策改善には、ε貪欲探索を使う</li>
</ul>
</li>
<li>Sarsa は、\( Q(s, a) \to q_*(s, a) \) に収束する (条件付きだが、ほとんどの場合成り立つ)</li>
<li>nステップ Sarsa<ul>
<li>nステップ先までの報酬を考慮。例: \( q_t^{(2)} = R_{t+1} + \gamma R_{t+2} + \gamma^2 Q(S_{t+2}) \)</li>
<li>\( Q(S_t, A) \leftarrow Q(S_t, A) + \alpha (q_t^{(n)} - Q(S_t, A)) \)</li>
</ul>
</li>
<li>Sarsa(λ) の前向き観点<ul>
<li>\( q^\lambda_t = (1 - \lambda) \sum_{n=1}^\infty \lambda^{n-1}q_t^{(n)} \)</li>
<li>\( Q(S_t, A) \leftarrow Q(S_t, A) + \alpha (q^\lambda_t - Q(S_t, A)) \)</li>
<li>問題点：時間軸上で先読みしている。エピソードの最後まで待ちたくない</li>
</ul>
</li>
<li>Sarsa(λ) の後ろ向き観点<ul>
<li>Eligibility Trace \( E_t(s, a) \) を定義 (TD(λ) と違い、状態と行動のペアに対して定義)</li>
<li>\( \delta_t = R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \) </li>
<li>\( Q(s, a) \leftarrow Q(s, a) + \alpha \delta_t E_t(s, a) \)</li>
</ul>
</li>
</ul>
</li>
<li>
<p>方策オフ型</p>
<ul>
<li>別の方策 \( \mu \) に従いながら、対象の方策 \( \pi \) を評価</li>
<li>なぜこれが重要か<ul>
<li>人間や他のエージェントから学ぶ</li>
<li>古い方策によって作られた経験から学習</li>
<li>探索的な方策から、最適な方策を学習</li>
<li>一つの方策から、複数の方策を学習</li>
</ul>
</li>
<li>重要サンプリング<ul>
<li>異なる分布の期待値を予測する</li>
</ul>
</li>
<li>方策オフ型のモンテカルロ法に重要サンプリングを適用する<ul>
<li>非常に大きい分散。ほとんど使えない。</li>
<li>TD 学習を使うことが必須</li>
</ul>
</li>
<li>Q-Learning<ul>
<li>振る舞いを規定する方策 \( \mu \) によって取られた（実際の）行動 \( A_{t+1} \)</li>
<li>学習中の方策によって取られた（仮想的な）行動 \( A' \)</li>
<li>\( Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha (R_{t+1} + \gamma Q(S_{t+1}, A') - Q(S^t, A)) \)</li>
<li>方策オフ型 Q-Learning → 学習したい方策が貪欲的な場合 (SarsaMax)<ul>
<li>最適価値関数に収束</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2>講義6: 価値関数の近似</h2>
<ul>
<li>
<p>はじめに</p>
<ul>
<li>大規模な強化学習<ul>
<li>バックギャモン: \( 10^{20} \) 個の状態</li>
<li>囲碁: \( 10^{170} \) 個の状態</li>
<li>ヘリコプター: 連続的な状態 → もはや参照テーブルを作ることができない</li>
</ul>
</li>
<li>価値 \( V(s) \) もしくは \( Q(s, a) \)<ul>
<li>テーブルが巨大になってメモリに載らない、もしくは載ったとしてもスパースすぎて学習が遅い</li>
</ul>
</li>
<li>関数近似<ul>
<li>\( v_\pi(s) \) を \( \widehat{v} (s, {\bf w})\)で近似</li>
<li>\( q_\pi(s, a) \) を \( \widehat{q} (s, a, {\bf w}) \) で近似</li>
</ul>
</li>
<li>３つのタイプの関数近似<ul>
<li>s → 関数 → \( \widehat{v} (s, {\bf w})\)</li>
<li>s, a → 関数 → \( \widehat{q} (s, a, {\bf w}) \)</li>
<li>s → 関数 → \( \widehat{q} (s, a_1, {\bf w}), ..., \widehat{q} (s, a_m, {\bf w}) \)</li>
</ul>
</li>
<li>関数の近似法<ul>
<li>特徴量の線形和 → 微分可能</li>
<li>ニューラル・ネットワーク → 微分可能</li>
<li>決定木</li>
<li>近傍法</li>
<li>フーリエ・ウェーブレット基底</li>
</ul>
</li>
<li>非定常、iid でない入力</li>
</ul>
</li>
<li>
<p>逐次的手法</p>
<ul>
<li>勾配降下法<ul>
<li>\( J(w) \) パラメータ \( w \) の微分可能な関数</li>
<li>J の勾配 \( \nabla_w J(w) = (\frac{\partial J(w)}{\partial w_1}, .. )^T \)</li>
<li>\( \Delta w \) を、\( \nabla_w J(w) \) に比例して更新</li>
</ul>
</li>
<li>確率的勾配降下法による価値関数の近似<ul>
<li>価値関数の真の値（オラクル）が分かったとしたら、目的関数 (平均二乗誤差)は、<ul>
<li>\( J(w) = E_\pi [ (v_\pi(S) - \widehat{v}(S, w))^2 ] \)</li>
</ul>
</li>
<li>期待値ではなく、インスタンス1つづつ更新する<ul>
<li>\( \Delta W = \alpha(v_\pi(S) - \widehat{v}(S, w)) \nabla_w \widehat{v}(S, w) \)</li>
</ul>
</li>
<li>特殊な場合：関数が特徴量の線形和の場合<ul>
<li>目的関数は、パラメータ w の二次関数</li>
<li>確率的勾配降下法によって、大域解が求まる</li>
<li>テーブル参照は、線形価値関数近似の特殊な場合</li>
</ul>
</li>
</ul>
</li>
<li>逐次的予測アルゴリズム<ul>
<li>オラクル \( v_\pi(s) \) の代わりに、ターゲット(予測値)を使う → ターゲットを使って教師あり学習をするのと同様<ul>
<li>MC: \( G_t \)</li>
<li>TD(0): \( R_{t+1} + \gamma \widehat{v}(S_{t+1}, w) \)</li>
<li>TD(λ): \( G^\lambda_t \)</li>
</ul>
</li>
<li>MC<ul>
<li>\( (S_1, G_1), (S_2, G_2), ... \) を「訓練データ」として使うのと同等</li>
<li>非線形価値関数を使っている時でも、(局所)最小解に収束する</li>
</ul>
</li>
<li>TD(0)<ul>
<li>\( R_{t+1} + \gamma \widehat{v}(S_{t+1}, w) \) は、偏りのあるサンプル</li>
<li>線形TD(0) は、大域解(の近く)に収束する</li>
</ul>
</li>
<li>TD(λ)<ul>
<li>\( (S_1, G^\lambda_1), (S_2, G^\lambda_2), ... \) を訓練データとして使うのと同等</li>
<li>前向き観点: \( G^\lambda_t \) を使う</li>
<li>後向き観点<ul>
<li>Eligibility Trace: \( \gamma \lambda E_{t-1} + x(S_t) \) → 各特徴量について計算</li>
<li>\( \Delta_w = \alpha \delta_t E_t \)</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>価値関数近似による制御<ul>
<li>価値関数近似 \( \widehat{q}(s, a, w) \) ＋ ε-貪欲方策</li>
<li>例: ニューラル・ネットワークにより \( q \) を近似: \( \widehat{q}(S, A, w) = q_\pi(S, A) \)</li>
<li>上と同じ議論：真の値（オラクル）を仮定し、それとの平均二乗誤差</li>
<li>素性は、状態 S と行動 A の関数になる</li>
<li>MC, TD(0), TD(λ) を、上と同様に定義できる</li>
</ul>
</li>
<li>ブートストラップ法を使うべきか？どう λ を設定するか？<ul>
<li>多くの問題において、\( \lambda = 0.9 \) あたりで性能が最高になる</li>
</ul>
</li>
<li>収束<ul>
<li>評価<ul>
<li>TD は、方策オフ型の場合、モデルが線形でも非線形でも、収束しない（発散・振動）する場合がある</li>
<li>Gradient TD は、この問題を解決</li>
</ul>
</li>
<li>制御<ul>
<li>最適値に近づくにつれて、「ぶれ」（＝最適値に近づいたり遠ざかったりする）の問題が起こる</li>
<li>非線形では収束が保証されない</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>バッチ手法</p>
<ul>
<li>勾配降下法は、サンプル効率的ではない</li>
<li>最小二乗予測<ul>
<li>経験 \( D = { (s_1, v_1^\pi), ..., (s_T, V_T^\pi) } \)</li>
<li>\( LS(w) = E_D[(v^\pi - \widehat{v}(s, w)^2] \)</li>
</ul>
</li>
<li>経験再生を使った確率的勾配降下法<ul>
<li>経験 D から \( (s, v^\pi) \) をサンプリング、確率的勾配降下法を適用</li>
<li>\( LS(w) \) に収束</li>
</ul>
</li>
<li>経験再生を使った DQN (Deep Q-Networks)<ul>
<li>行動は、ε貪欲方策に従う</li>
<li>遷移 \( s_t, a_t, r_{t+1}, s_{t+1} \) を、全てメモリー D に保存しておく</li>
<li>D から、ミニバッチ (64個のインスタンス) を取り出す</li>
<li>Q-Learning ターゲットをこのミニバッチを使って計算</li>
<li>ターゲットとQ-network の最小二乗誤差を最小化</li>
<li>安定化のためのテクニック<ul>
<li>経験再生を使うと、インスタンス間の相関が減る</li>
<li>2つのネットワークを使う。片方をフリーズさせ、最新の予測を使うかわりにそのネットワークを使う</li>
</ul>
</li>
</ul>
</li>
<li>線形最小二乗誤差予測<ul>
<li>線形モデルを使えば、閉じた形で解が求まる → 効率的</li>
<li>MC, TD, TD(λ) と組み合わせて、LSMC, LSTD, LSTD(λ) を得る</li>
<li>TD に比べて、収束性が向上</li>
</ul>
</li>
<li>線形最小二乗誤差制御 (LSPI)<ul>
<li>収束性が向上</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2>講義7: 方策勾配法</h2>
<ul>
<li>
<p>はじめに</p>
<ul>
<li>これまでは、価値関数から (例えば、ε貪欲法を使って) 方策を直接生成した</li>
<li>価値関数をモデル化する代わりに、方策を直接モデル化する<ul>
<li>\( \pi_\theta(s, a) = P[a | s, \theta] \)</li>
</ul>
</li>
<li>長所<ul>
<li>良い収束性</li>
<li>高次元もしくは行動が連続空間の場合</li>
<li>確率的な方策を学べる</li>
</ul>
</li>
<li>確率的な方策が良い場合<ul>
<li>じゃんけん<ul>
<li>もし、方策が決定的なら、相手にそのことを利用されてしまう</li>
<li>最適な方策は、確率的にランダムな手を出すこと</li>
</ul>
</li>
<li>Aliasing (偽信号 -&gt; 2つ以上の状態がお互いに見分けられない場合) が起こる場合<ul>
<li>確率的に行動するのが最適</li>
<li>素性のせいで、環境の表現が制限される場合も、これに相当</li>
</ul>
</li>
</ul>
</li>
<li>方策の目的関数<ul>
<li>1) 開始状態の値を使う 2) 状態の平均値を使う 3) 1ステップ毎の平均報酬</li>
<li>どれを使っても同じ手法 (方策勾配) になる</li>
</ul>
</li>
<li>方策最適化<ul>
<li>\( J(\theta) \) を最小化する \( \theta \) を見つける</li>
<li>様々な手法が使える<ul>
<li>勾配を使わない手法</li>
<li>勾配を使う手法 (例: 勾配降下法) </li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Finite Difference 方策勾配法</p>
<ul>
<li>勾配の方向に登り、極大解を探す \( \Delta \theta = \alpha_\theta J(\theta) \)</li>
<li>Finite Differences<ul>
<li>各次元について、少し ε だけ値を変えて、\( J(\theta) \) がどう変化するか見る → 勾配の近似</li>
<li>高次元の場合、非効率的</li>
</ul>
</li>
</ul>
</li>
<li>
<p>モンテカルロ方策勾配法</p>
<ul>
<li>尤度比<ul>
<li>方策勾配を解析的に計算する</li>
<li>\( \pi_\theta \) が微分可能で、勾配 \( \nabla_\theta \pi_\theta(s, a) \) が分かっているとすると</li>
<li>\( \nabla_\theta \pi_\theta(s, a) = \pi_\theta(s, a) \nabla_\theta \log \pi_\theta(s, a) \)</li>
<li>スコア関数 \( \nabla_\theta \log \pi_\theta(s, a) \) </li>
<li>これに従うと、尤度最大化 (MLE)</li>
<li>Softmax 方策<ul>
<li>\( \pi_\theta(s, a) \propto \exp{ \phi(s, a)^T \theta )} \)</li>
</ul>
</li>
<li>ガウシアン方策<ul>
<li>平均を、特徴量の線形和で表現 \( \mu(s) = \phi(s)^T \theta \)</li>
</ul>
</li>
</ul>
</li>
<li>One-Step MDP<ul>
<li>尤度比トリックを使う:  \( \nabla_\theta J(\theta) = E_{\pi_\theta}[\nabla_\theta \log \pi_\theta(s, a)r] \) </li>
</ul>
</li>
<li>方策勾配定理<ul>
<li>どの方策目的関数に対しても、\( \nabla_\theta J(\theta) = E_{\pi_\theta}[\nabla_\theta \log \pi_\theta(s, a) Q^{\theta_\pi}(s, a)] \)</li>
</ul>
</li>
<li>Monte-Carlo Policy Gradient (REINFORCE)<ul>
<li>パラメータを勾配降下(上昇)法で更新</li>
<li>\( Q^{\pi_\theta} \) の不偏サンプルとして、\( v_t \) (t から最後までの報酬和) を使う</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Actor-Critic 方策勾配法</p>
<ul>
<li>モンテカルロ方策は、分散がまだ大きい</li>
<li>Critic を使って、行動価値関数を近似</li>
<li>Critic: パラメータ w を使う、Actor: パラメータ θ を使う</li>
<li>Critic: \( Q_w(s, a) \) → 前回の講義と同様に推定</li>
<li>分散を減らすトリック：ベースラインを使う<ul>
<li>期待値を変えずに、ベースラインを減らせる</li>
<li>方策勾配から \( B(s) \) を引く</li>
<li>状態価値関数 \( V^{\pi_\theta} \) をベースラインとして使うと良い</li>
<li>Advantage Function \( A^{\pi_\theta}(s, a) = Q^{\pi_\theta}(s, a) - V^{\pi_\theta}(s) \)<ul>
<li>→ 方策勾配に組み込む</li>
</ul>
</li>
</ul>
</li>
<li>どうやって Advantage Function を推定するか<ul>
<li>方法1. ２つの異なるパラメータを使う</li>
<li>方法2. TD 誤差を使う (期待値が Advantage Function と同じになる)<ul>
<li>\( V(s) \) だけを推定すれば良い</li>
</ul>
</li>
</ul>
</li>
<li>Critic の変種 (異なる時間スケール、ターゲット)<ul>
<li>MC, TD(0), 前向き観点 TD(λ), 後ろ向き観点 TD(λ)</li>
</ul>
</li>
<li>Actor の変種 (異なる時間スケール、ターゲット)<ul>
<li>MC → 利得 \( v_t \) </li>
<li>TD → TD誤差 \( r + \gamma V_v(s_{t+1}) \)</li>
<li>Eligibility Trace</li>
</ul>
</li>
<li>全く偏りの無い方策勾配を求めることも可能 → Compatible function approximator</li>
</ul>
</li>
</ul>
  </div>
  <div class="tag-cloud">
    <p>
    </p>
  </div>


    <div class="addthis_relatedposts_inline">


<div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_shortname = 'englishforhackers';
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>
      Please enable JavaScript to view comments.

</noscript>
</article>

    <footer>
<p>
  &copy; Masato Hagiwara 2017 - This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>
</p>
<p>  Powered by <a href="http://getpelican.com" target="_blank">Pelican</a> - <a href="https://github.com/alexandrevicenzi/flex" target="_blank">Flex</a> theme by <a href="http://alexandrevicenzi.com" target="_blank">Alexandre Vicenzi</a>
</p><p>
  <a rel="license"
     href="http://creativecommons.org/licenses/by-nc-sa/4.0/"
     target="_blank">
    <img alt="Creative Commons License"
         title="Creative Commons License"
         style="border-width:0"
         src="https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png"
         width="80"
         height="15"/>
  </a>
</p>    </footer>
  </main>

<!-- Google Analytics -->
<script type="text/javascript">
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-175204-13', 'auto');
  ga('send', 'pageview');
</script>
<!-- End Google Analytics -->



<script type="application/ld+json">
{
  "@context" : "http://schema.org",
  "@type" : "Blog",
  "name": " エンジニアの英語学習法 ",
  "url" : "http://englishforhackers.com",
  "image": "http://masatohagiwara.net/face.jpg",
  "description": "英語を身につけるためには、興味のある教材を見つけるのが一番。ソフトウェア・エンジニアや研究者のための英語学習情報をお届けします。英語の講演やトーク、講義、記事などの紹介が中心です。"
}
</script>

</body>
</html>