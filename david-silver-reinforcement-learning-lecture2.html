
<!DOCTYPE html>
<html lang="ja">
<head>
  <link href='//fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,700,400italic' rel='stylesheet' type='text/css'>

    <link rel="stylesheet/less" type="text/css" href="http://englishforhackers.com/theme/stylesheet/style.less">
    <script src="//cdnjs.cloudflare.com/ajax/libs/less.js/2.5.1/less.min.js" type="text/javascript"></script>

  <link rel="stylesheet" type="text/css" href="http://englishforhackers.com/theme/pygments/monokai.min.css">
  <link rel="stylesheet" type="text/css" href="http://englishforhackers.com/theme/font-awesome/css/font-awesome.min.css">

    <link href="http://englishforhackers.com/static/custom.css" rel="stylesheet">

    <link href="http://englishforhackers.com/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="エンジニア・研究者の英語学習 Atom">


    <link rel="shortcut icon" href="/images/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/images/favicon.ico" type="image/x-icon">

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="robots" content="index, follow" />

    <!-- Chrome, Firefox OS and Opera -->
    <meta name="theme-color" content="#333333">
    <!-- Windows Phone -->
    <meta name="msapplication-navbutton-color" content="#333333">
    <!-- iOS Safari -->
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

<meta name="author" content="Masato Hagiwara" />
<meta name="description" content="「無料でアクセスできる最高の強化学習のコース」と名高い、Google DeepMind / University College London の David Silver 氏による強化学習のコース。こちらのページから、全ての講義スライドと講義ビデオが見られる。以下は、講義2 のメモです。 マルコフ決定過程 (MDP) 環境が完全に観察可能 状態が、過程を完全に規定する 多くの強化学習問題が、MDP として定式化可能 部分観測マルコフ決定過程 (POMDP) も、MDP に変換可能 バンディットアルゴリズムも、状態が一つしかない MDP マルコフ性 次に何が起こるかは、今の状態だけに依存 Lecture 1 参照 状態遷移確率 \( P_{ss'} = P[S_{t+1} = s' | S_t = s] \) 行列で表現可能 マルコフ過程 …" />
<meta name="keywords" content="">
<meta property="og:site_name" content="エンジニア・研究者の英語学習"/>
<meta property="og:title" content="マルコフ決定過程 (MDP) - Google DeepMind の David Silver 氏による強化学習コース 講義2"/>
<meta property="og:description" content="「無料でアクセスできる最高の強化学習のコース」と名高い、Google DeepMind / University College London の David Silver 氏による強化学習のコース。こちらのページから、全ての講義スライドと講義ビデオが見られる。以下は、講義2 のメモです。 マルコフ決定過程 (MDP) 環境が完全に観察可能 状態が、過程を完全に規定する 多くの強化学習問題が、MDP として定式化可能 部分観測マルコフ決定過程 (POMDP) も、MDP に変換可能 バンディットアルゴリズムも、状態が一つしかない MDP マルコフ性 次に何が起こるかは、今の状態だけに依存 Lecture 1 参照 状態遷移確率 \( P_{ss'} = P[S_{t+1} = s' | S_t = s] \) 行列で表現可能 マルコフ過程 …"/>
<meta property="og:locale" content="ja_JP"/>
<meta property="og:url" content="http://englishforhackers.com/david-silver-reinforcement-learning-lecture2.html"/>
<meta property="og:type" content="article"/>
<meta property="article:published_time" content="2018-09-02 00:00:00-04:00"/>
<meta property="article:modified_time" content=""/>
<meta property="article:author" content="http://englishforhackers.com/author/masato-hagiwara.html">
<meta property="article:section" content="Reinforcement Learning"/>
<meta property="og:image" content="http://masatohagiwara.net/face.jpg">

  <title>マルコフ決定過程 (MDP) - Google DeepMind の David Silver 氏による強化学習コース 講義2 &ndash; エンジニア・研究者の英語学習</title>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
    (adsbygoogle = window.adsbygoogle || []).push({
      google_ad_client: "ca-pub-7401771876348738",
      enable_page_level_ads: true
    });
  </script>
</head>
<body>
  <aside>
    <div>
      <a href="http://englishforhackers.com">
        <img src="http://masatohagiwara.net/face.jpg" alt="エンジニア・研究者の<br/>英語学習" title="エンジニア・研究者の<br/>英語学習">
      </a>
      <h1><a href="http://englishforhackers.com">エンジニア・研究者の<br/>英語学習</a></h1>

<p>ソフトウェア・エンジニアや研究者のための英語学習情報。講演/トーク・技術記事・論文などの紹介</p>
      <nav>
        <ul class="list">

          <li><a href="http://masatohagiwara.net/" target="_blank">個人ページ</a></li>
          <li><a href="http://www.duolingo.com/" target="_blank">Duolingo</a></li>
          <li><a href="https://docs.google.com/forms/d/e/1FAIpQLSffbTFvdUXJhN4jTOmylcIRvlntKmaYVkIbYPrbBCPm0iC9Sw/viewform?usp=sf_link" target="_blank">お問い合わせ</a></li>
        </ul>
      </nav>

      <ul class="social">
        <li><a class="sc-github" href="https://github.com/mhagiwara" target="_blank"><i class="fa fa-github"></i></a></li>
        <li><a class="sc-twitter" href="https://twitter.com/mhagiwara" target="_blank"><i class="fa fa-twitter"></i></a></li>
        <li><a class="sc-rss" href="feeds/all.atom.xml" target="_blank"><i class="fa fa-rss"></i></a></li>
      </ul>
    </div>


  </aside>
  <main>

    <nav>
      <a href="http://englishforhackers.com">  Home
</a>

      <a href="/archives.html">Archives</a>
      <a href="/categories.html">Categories</a>
      <a href="/tags.html">Tags</a>

      <a href="http://englishforhackers.com/feeds/all.atom.xml">  Atom
</a>

    </nav>

<article class="single">
  <header>
    <h1 id="david-silver-reinforcement-learning-lecture2">マルコフ決定過程 (MDP) - Google DeepMind の David Silver 氏による強化学習コース 講義2</h1>
    <p>
        Posted on 2018-09-02(日) in <a href="http://englishforhackers.com/category/reinforcement-learning.html">Reinforcement Learning</a>


    </p>
  </header>


  <div>
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script>

<p>「無料でアクセスできる最高の強化学習のコース」と名高い、Google DeepMind / University College London の David Silver 氏による強化学習のコース。<a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html">こちらのページから、全ての講義スライドと講義ビデオが見られる</a>。以下は、講義2 のメモです。</p>
<ul>
<li>
<p>マルコフ決定過程 (MDP)</p>
<ul>
<li>環境が完全に観察可能</li>
<li>状態が、過程を完全に規定する</li>
<li>多くの強化学習問題が、MDP として定式化可能</li>
<li>部分観測マルコフ決定過程 (POMDP) も、MDP に変換可能</li>
<li>バンディットアルゴリズムも、状態が一つしかない MDP</li>
</ul>
</li>
<li>
<p>マルコフ性</p>
<ul>
<li>次に何が起こるかは、今の状態だけに依存</li>
<li>Lecture 1 参照</li>
<li>状態遷移確率 \( P_{ss'} = P[S_{t+1} = s' | S_t = s] \) 行列で表現可能</li>
</ul>
</li>
<li>
<p>マルコフ過程</p>
<ul>
<li>状態列 \( S_1, S_2, ... \) がマルコフ性を満たすとき → マルコフ過程</li>
</ul>
</li>
<li>
<p>マルコフ報酬過程 (Markov Reward Process; MRP)</p>
<ul>
<li>R: 報酬関数 \( R_s = E[ R_{t+1} | S_t = s] \)</li>
<li>利得 \( G_t = \sum_{k=0}^\infty \gamma^k R_{t+k+1} \)</li>
<li>なぜ割引率 \( \gamma \) を使うか → 数学的に便利。報酬が発散するのを防ぐ。未来に行くほど不確定。直近の未来の報酬を優先。</li>
</ul>
</li>
<li>
<p>価値関数</p>
<ul>
<li>状態 s に居るときの利得の期待値 \( v(s) = E[G_t | S_t = s] \)</li>
</ul>
</li>
<li>
<p>ベルマン方程式</p>
<ul>
<li>\( v(s) = E[R_{t+1} + \gamma v(S_{t+1}) | S_t = s] \)</li>
<li>価値観数は、1) すぐ次の報酬、2) 次の状態の価値（＋割り引き）の２つに分解できる</li>
<li>行列表現: \( v = R + \gamma Pv \)<ul>
<li>v: \( v = (v(1), ..., v(n))^T \)</li>
<li>R: \( R = (R(1), ..., R(n))^T \)</li>
<li>P: 状態 i から j への遷移確率行列</li>
</ul>
</li>
<li>解析的に解ける: \( v = (I - \gamma P)^{-1}R \)<ul>
<li>小さい MDP にしか適用できない。</li>
</ul>
</li>
</ul>
</li>
<li>
<p>マルコフ決定過程</p>
<ul>
<li>マルコフ報酬過程 + 行動</li>
<li>報酬 R: \( R^a_s = E[R_{t+1} | S_t = s, A_t = a] \)</li>
<li>
<p>方策</p>
<ul>
<li>\( \pi(a | s) = P[A_t = a | S_t = s] \) → エージェントの振る舞いを完全に規定<ul>
<li>時間 \( t \) に依存しない</li>
</ul>
</li>
<li>MDP と方策 \( \pi \) が与えられたとき、状態系列 \( S_1, S_2, ... \) はマルコフ過程 → マルコフ決定過程を「平ら」にする</li>
</ul>
</li>
<li>
<p>価値関数</p>
<ul>
<li>状態価値関数は、方策 \( \pi \) に依存： \( v_\pi(s) = E_\pi[G_t | S_t = s] \)</li>
<li>行動価値関数: \( q(s, a) = E[G_t | S_t = s, A_t = a] \)</li>
<li>ベルマン方程式を使って、直近の報酬と次の状態の価値に分解できる<ul>
<li>状態価値関数: \( v_\pi(s) = \sum_{a \in A} \pi(a|s) q_\pi(a, s) \)</li>
<li>行動価値関数: \( q(s, a) = R^a_s + \gamma \sum_{s' \in S}P^a_{ss'} v_\pi(s') \)</li>
</ul>
</li>
<li>ベルマン方程式の再帰適用: \( v_\pi(s) = \sum_{a \in A} \pi(a|s)( R^a_s + \gamma \sum_{s' \in S} P^a_{ss'} v_\pi(s') ) \)</li>
<li>\( q_\pi \) にも同じことができる</li>
</ul>
</li>
<li>最適状態価値関数<ul>
<li>全ての方策の中で、価値関数が最大となるもの: \( v_*(s) = \max_\pi v_\pi(s) \)</li>
</ul>
</li>
<li>最適行動価値関数<ul>
<li>\( q_{*}(s, a) = \max_\pi q_\pi(s, a) \) → これがあれば、MDP は「解けた」（各状態において、どう行動すべきかが分かる）</li>
</ul>
</li>
<li>最適方策<ul>
<li>ある方策が他の方策より良いとは？ \( \pi \ge \pi' if v_\pi(s) \ge v_\pi'(s), \forall s \)</li>
<li>定理: 他のあらゆる方策よりも良い最適方策 \( \pi_* \) が存在する。複数存在する場合もある</li>
<li>\( q_*(s, a) \) を最大化する行動を取ることで、最適方策が得られる</li>
<li>\( v_* \) と \( q_* \) についても、上記のベルマン方程式が適用できる<ul>
<li>ただし、\( \sum_{a \in A} \) は \( \max_a \) に置き換わる</li>
</ul>
</li>
<li>非線形 (max が入る) →　閉じた形での解は存在しない</li>
<li>繰り返し<ul>
<li>価値反復 (Value Iteration)</li>
<li>方策反復 (Policy Iteration)</li>
<li>Q 学習</li>
<li>Sarsa </li>
</ul>
</li>
</ul>
</li>
<li>MDP の拡張</li>
<li>無限 / 連続 MDP</li>
<li>部分観測マルコフ決定過程 (POMDP)</li>
<li>割り引きの無い, 平均報酬 MDP</li>
</ul>
</li>
</ul>
  </div>
  <div class="tag-cloud">
    <p>
    </p>
  </div>


    <div class="addthis_relatedposts_inline">


<div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_shortname = 'englishforhackers';
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>
      Please enable JavaScript to view comments.

</noscript>
</article>

    <footer>
<p>
  &copy; Masato Hagiwara 2017 - This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>
</p>
<p>  Powered by <a href="http://getpelican.com" target="_blank">Pelican</a> - <a href="https://github.com/alexandrevicenzi/flex" target="_blank">Flex</a> theme by <a href="http://alexandrevicenzi.com" target="_blank">Alexandre Vicenzi</a>
</p><p>
  <a rel="license"
     href="http://creativecommons.org/licenses/by-nc-sa/4.0/"
     target="_blank">
    <img alt="Creative Commons License"
         title="Creative Commons License"
         style="border-width:0"
         src="https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png"
         width="80"
         height="15"/>
  </a>
</p>    </footer>
  </main>

<!-- Google Analytics -->
<script type="text/javascript">
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-175204-13', 'auto');
  ga('send', 'pageview');
</script>
<!-- End Google Analytics -->



<script type="application/ld+json">
{
  "@context" : "http://schema.org",
  "@type" : "Blog",
  "name": " エンジニア・研究者の英語学習 ",
  "url" : "http://englishforhackers.com",
  "image": "http://masatohagiwara.net/face.jpg",
  "description": "ソフトウェア・エンジニアや研究者のための英語学習情報。講演/トーク・技術記事・論文などの紹介"
}
</script>

</body>
</html>